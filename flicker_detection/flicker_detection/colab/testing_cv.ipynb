{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "testing-cv.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hc07180011/testing-cv/blob/main/flicker_detection/flicker_detection/colab/testing_cv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlB-t1FOLjqY",
        "outputId": "b20c35e6-38f9-4b16-b77f-b6f079b7548a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnuSnIi1Rt7r",
        "outputId": "ab4969be-82c4-4a7d-8541-7d28fc789572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'testing-cv'...\n",
            "remote: Enumerating objects: 1523, done.\u001b[K\n",
            "remote: Counting objects: 100% (1187/1187), done.\u001b[K\n",
            "remote: Compressing objects: 100% (896/896), done.\u001b[K\n",
            "remote: Total 1523 (delta 705), reused 627 (delta 271), pack-reused 336\u001b[K\n",
            "Receiving objects: 100% (1523/1523), 113.94 MiB | 17.10 MiB/s, done.\n",
            "Resolving deltas: 100% (864/864), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hc07180011/testing-cv.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd testing-cv/flicker_detection/flicker_detection/\n",
        "%pip install -r requirements.txt\n",
        "%pip install -r requirements_dev.txt\n",
        "%pip install tensorflow-addons\n",
        "%pip install coloredlogs rich\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RujnFEboSD1K",
        "outputId": "5fd8f959-682f-4461-95fa-16ceef369dc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/testing-cv/flicker_detection/flicker_detection\n",
            "Requirement already satisfied: absl-py==1.0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.6.3)\n",
            "Requirement already satisfied: cachetools==4.2.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (4.2.4)\n",
            "Requirement already satisfied: certifi==2021.10.8 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (2021.10.8)\n",
            "Collecting charset-normalizer==2.0.10\n",
            "  Downloading charset_normalizer-2.0.10-py3-none-any.whl (39 kB)\n",
            "Collecting colorama==0.4.4\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting coloredlogs==15.0.1\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting commonmark==0.9.1\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler==0.11.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (0.11.0)\n",
            "Requirement already satisfied: flatbuffers==2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (2.0)\n",
            "Collecting fonttools==4.28.5\n",
            "  Downloading fonttools-4.28.5-py3-none-any.whl (890 kB)\n",
            "\u001b[K     |████████████████████████████████| 890 kB 11.3 MB/s \n",
            "\u001b[?25hCollecting gast==0.4.0\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting google-auth==2.3.3\n",
            "  Downloading google_auth-2.3.3-py2.py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 49.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib==0.4.6 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (0.4.6)\n",
            "Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (0.2.0)\n",
            "Collecting grpcio==1.43.0\n",
            "  Downloading grpcio-1.43.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 41.7 MB/s \n",
            "\u001b[?25hCollecting h5py==3.6.0\n",
            "  Downloading h5py-3.6.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 37.2 MB/s \n",
            "\u001b[?25hCollecting humanfriendly==10.0\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.0 MB/s \n",
            "\u001b[?25hCollecting idna==3.3\n",
            "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 7.9 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata==4.10.1\n",
            "  Downloading importlib_metadata-4.10.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: joblib==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 21)) (1.1.0)\n",
            "Collecting keras==2.7.0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 42.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Keras-Preprocessing==1.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 23)) (1.1.2)\n",
            "Collecting kiwisolver==1.3.2\n",
            "  Downloading kiwisolver-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 38.2 MB/s \n",
            "\u001b[?25hCollecting libclang==12.0.0\n",
            "  Downloading libclang-12.0.0-2-py2.py3-none-manylinux1_x86_64.whl (13.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.3 MB 24.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Markdown==3.3.6 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 26)) (3.3.6)\n",
            "Collecting matplotlib==3.5.1\n",
            "  Downloading matplotlib-3.5.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 32.1 MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement numpy==1.22.1 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0rc1, 1.13.0rc2, 1.13.0, 1.13.1, 1.13.3, 1.14.0rc1, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0rc1, 1.15.0rc2, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0rc1, 1.16.0rc2, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0rc1, 1.17.0rc2, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0rc1, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0rc1, 1.19.0rc2, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0rc1, 1.20.0rc2, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0rc1, 1.21.0rc2, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for numpy==1.22.1\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py==1.0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 2)) (1.6.3)\n",
            "Requirement already satisfied: cached-property==1.5.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 3)) (1.5.2)\n",
            "Collecting cachetools==5.0.0\n",
            "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: certifi==2021.10.8 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 5)) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer==2.0.12 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 6)) (2.0.12)\n",
            "Collecting commonmark==0.9.1\n",
            "  Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "Requirement already satisfied: cycler==0.11.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 8)) (0.11.0)\n",
            "Requirement already satisfied: flatbuffers==2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 9)) (2.0)\n",
            "Collecting fonttools==4.31.2\n",
            "  Downloading fonttools-4.31.2-py3-none-any.whl (899 kB)\n",
            "\u001b[K     |████████████████████████████████| 899 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast==0.5.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 11)) (0.5.3)\n",
            "Collecting google-auth==2.6.2\n",
            "  Downloading google_auth-2.6.2-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 48.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib==0.4.6 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 13)) (0.4.6)\n",
            "Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 14)) (0.2.0)\n",
            "Requirement already satisfied: grpcio==1.44.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 15)) (1.44.0)\n",
            "Collecting h5py==3.6.0\n",
            "  Using cached h5py-3.6.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
            "Collecting idna==3.3\n",
            "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
            "Collecting imbalanced-learn==0.9.0\n",
            "  Downloading imbalanced_learn-0.9.0-py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 46.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: imblearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 19)) (0.0)\n",
            "Requirement already satisfied: importlib-metadata==4.11.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 20)) (4.11.3)\n",
            "Requirement already satisfied: joblib==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 21)) (1.1.0)\n",
            "Requirement already satisfied: keras==2.8.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 22)) (2.8.0)\n",
            "Requirement already satisfied: Keras-Preprocessing==1.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 23)) (1.1.2)\n",
            "Collecting kiwisolver==1.4.2\n",
            "  Downloading kiwisolver-1.4.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 24.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: libclang==13.0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 25)) (13.0.0)\n",
            "Requirement already satisfied: Markdown==3.3.6 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 26)) (3.3.6)\n",
            "Collecting matplotlib==3.5.1\n",
            "  Using cached matplotlib-3.5.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "Requirement already satisfied: numpy==1.21.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 28)) (1.21.5)\n",
            "Requirement already satisfied: oauthlib==3.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 29)) (3.2.0)\n",
            "Collecting opencv-python==4.5.5.64\n",
            "  Downloading opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 60.5 MB 69 kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 31)) (3.3.0)\n",
            "Requirement already satisfied: packaging==21.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 32)) (21.3)\n",
            "Collecting Pillow==9.1.0\n",
            "  Downloading Pillow-9.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 34.5 MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pkg_resources==0.0.0 (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for pkg_resources==0.0.0\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.16.1\n",
            "Collecting coloredlogs\n",
            "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Collecting rich\n",
            "  Downloading rich-12.1.0-py3-none-any.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 5.6 MB/s \n",
            "\u001b[?25hCollecting humanfriendly>=9.1\n",
            "  Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich) (2.6.1)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from rich) (3.10.0.2)\n",
            "Installing collected packages: humanfriendly, commonmark, rich, coloredlogs\n",
            "Successfully installed coloredlogs-15.0.1 commonmark-0.9.1 humanfriendly-10.0 rich-12.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tIweLngAboSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## logging.py"
      ],
      "metadata": {
        "id": "o1VIfZ1LHCfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import logging\n",
        "\n",
        "from rich.logging import RichHandler\n",
        "\n",
        "\n",
        "def init_logger() -> None:\n",
        "    logger = logging.getLogger(\"rich\")\n",
        "\n",
        "    FORMAT = \"%(name)s[%(process)d] \" + \\\n",
        "        \"%(processName)s(%(threadName)s) \" + \\\n",
        "        \"%(module)s:%(lineno)d  %(message)s\"\n",
        "\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "    formatter = logging.Formatter(\n",
        "        FORMAT,\n",
        "        datefmt=\"%Y%m%d %H:%M:%S\"\n",
        "    )\n",
        "    logging.basicConfig(\n",
        "        level=\"NOTSET\", format=FORMAT, handlers=[RichHandler()]\n",
        "    )\n",
        "\n",
        "    ch = logging.StreamHandler()\n",
        "    ch.setLevel(logging.DEBUG)\n",
        "    ch.setFormatter(formatter)\n",
        "\n",
        "    # sys.stdin.reconfigure(encoding=\"utf-8\")\n",
        "    # sys.stdout.reconfigure(encoding=\"utf-8\")\n",
        "\n",
        "    logger.addHandler(ch)\n",
        "\n",
        "    logging.info(\"Initializing ok.\")"
      ],
      "metadata": {
        "id": "yyAonoQP7Zj-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## facenet.py"
      ],
      "metadata": {
        "id": "eS5Hgl-tG8Zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.applications import resnet, mobilenet\n",
        "from tensorflow_addons.layers import AdaptiveMaxPooling3D\n",
        "\n",
        "\n",
        "class Facenet:\n",
        "    \"\"\"\n",
        "    adaptive pooling sample:\n",
        "    https://ideone.com/cJoN3x\n",
        "    \"\"\"\n",
        "    def __init__(self) -> None:\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.__target_shape = (200, 200)\n",
        "\n",
        "        np.random.seed(0)\n",
        "\n",
        "        base_cnn = mobilenet.MobileNet(\n",
        "            weights=\"imagenet\",\n",
        "            input_shape=self.__target_shape + (3,),\n",
        "            include_top=False\n",
        "        )\n",
        "\n",
        "        adaptive_1 = AdaptiveMaxPooling3D(\n",
        "            output_size=(6, 6, 1024))(base_cnn.output)\n",
        "\n",
        "        output = layers.Dense(256)(adaptive_1)\n",
        "\n",
        "        adaptive_m = AdaptiveMaxPooling3D(\n",
        "            output_size=(6, 6, 256))(output)\n",
        "\n",
        "        self.__embedding = Model(base_cnn.input, adaptive_m,name='Ebedding')\n",
        "        with open('basecnn_summary.txt', 'w') as fh:\n",
        "            self.__embedding.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
        "\n",
        "        for layer in base_cnn.layers[:-23]:\n",
        "            layer.trainable = False\n",
        "\n",
        "        anchor_input = layers.Input(\n",
        "            name=\"anchor\", shape=self.__target_shape + (3,)\n",
        "        )\n",
        "\n",
        "        adapt_anchor = AdaptiveMaxPooling3D(\n",
        "            output_size=(200, 200, 3))(anchor_input)\n",
        "        adapted_anchor = layers.Input(\n",
        "            name=\"adapted_anchor\", shape=adapt_anchor.shape, tensor=adapt_anchor)\n",
        "\n",
        "        positive_input = layers.Input(\n",
        "            name=\"positive\", shape=self.__target_shape + (3,)\n",
        "        )\n",
        "\n",
        "        adapt_positive = AdaptiveMaxPooling3D(\n",
        "            output_size=(200, 200, 3))(positive_input)\n",
        "        adapted_positive = layers.Input(\n",
        "            name=\"adapted_positive\", shape=adapt_positive.shape, tensor=adapt_positive)\n",
        "\n",
        "        negative_input = layers.Input(\n",
        "            name=\"negative\", shape=self.__target_shape + (3,)\n",
        "        )\n",
        "\n",
        "        adapt_negative = AdaptiveMaxPooling3D(\n",
        "            output_size=(200, 200, 3))(negative_input)\n",
        "        adapted_negative = layers.Input(\n",
        "            name=\"adapted_negative\", shape=adapt_negative.shape, tensor=adapt_negative)\n",
        "\n",
        "        distances = DistanceLayer()(\n",
        "            self.__embedding(resnet.preprocess_input(anchor_input)),\n",
        "            self.__embedding(resnet.preprocess_input(positive_input)),\n",
        "            self.__embedding(resnet.preprocess_input(negative_input)),\n",
        "        )\n",
        "\n",
        "        siamese_network = Model(\n",
        "            inputs=[\n",
        "                adapted_anchor,\n",
        "                adapted_positive,\n",
        "                adapted_negative,\n",
        "                anchor_input,\n",
        "                positive_input,\n",
        "                negative_input,\n",
        "            ],\n",
        "            outputs=distances\n",
        "        )\n",
        "\n",
        "        with open('resnet_preprocess_summary.txt', 'w') as fh:\n",
        "            siamese_network.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
        "\n",
        "        adaptive_0 = AdaptiveMaxPooling3D(\n",
        "            output_size=(1024, 6, 6))(siamese_network.output)\n",
        "\n",
        "        adaptive_siamese_network = Model(siamese_network.input, adaptive_0)\n",
        "\n",
        "        self.__siamese_model = SiameseModel(adaptive_siamese_network)\n",
        "        self.__siamese_model.built = True\n",
        "\n",
        "        with open('adaptive_siamese_summary.txt', 'w') as fh:\n",
        "            self.__siamese_model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
        "        \n",
        "        model_base_dir = os.path.join(\"/content/drive/MyDrive/google_cv/flicker_detection_model_architecture/\")\n",
        "\n",
        "        model_settings = json.load(\n",
        "            open(os.path.join(model_base_dir, \"model.json\"), \"r\")\n",
        "        )\n",
        "        model_path = os.path.join(model_base_dir, model_settings[\"name\"])\n",
        "        if os.path.exists(model_path):\n",
        "            self.__siamese_model.load_weights(model_path)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def get_embedding(self, images: np.ndarray, batched=True) -> np.ndarray:\n",
        "        assert (not batched) or len(\n",
        "            images.shape) == 4, \"images should be an array of image with shape (width, height, 3)\"\n",
        "        if not batched:\n",
        "            images = np.array([images, ])\n",
        "        resized_images = np.array([cv2.resize(image, dsize=self.__target_shape,\n",
        "                                              interpolation=cv2.INTER_CUBIC) for image in images])\n",
        "        image_tensor = tf.convert_to_tensor(resized_images, np.float32)\n",
        "        return self.__embedding(resnet.preprocess_input(image_tensor)).numpy()\n",
        "\n",
        "\n",
        "class DistanceLayer(layers.Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, anchor, positive, negative):\n",
        "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
        "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
        "        return (ap_distance, an_distance)\n",
        "\n",
        "\n",
        "class SiameseModel(Model):\n",
        "\n",
        "    def __init__(self, siamese_network, margin=0.5):\n",
        "        super(SiameseModel, self).__init__()\n",
        "        self.siamese_network = siamese_network\n",
        "        self.margin = margin\n",
        "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.siamese_network(inputs)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self._compute_loss(data)\n",
        "\n",
        "        gradients = tape.gradient(\n",
        "            loss, self.siamese_network.trainable_weights)\n",
        "\n",
        "        self.optimizer.apply_gradients(\n",
        "            zip(gradients, self.siamese_network.trainable_weights)\n",
        "        )\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        loss = self._compute_loss(data)\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}\n",
        "\n",
        "    def _compute_loss(self, data):\n",
        "        ap_distance, an_distance = self.siamese_network(data)\n",
        "        loss = ap_distance - an_distance\n",
        "        loss = tf.maximum(loss + self.margin, 0.0)\n",
        "        return loss\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker]"
      ],
      "metadata": {
        "id": "ceaWGfQSG6N8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## keras.py"
      ],
      "metadata": {
        "id": "8NYdNlmfHkWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, precision_recall_curve, roc_curve, auc\n",
        "\n",
        "\n",
        "logging.getLogger(\"matplotlib\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)\n",
        "\n",
        "\n",
        "class MyMetrics:\n",
        "    \"\"\"\n",
        "    keras metrics api:\n",
        "    https://keras.io/api/metrics/\n",
        "    custom sensitivity specificity:\n",
        "    https://stackoverflow.com/questions/55640149/error-in-keras-when-i-want-to-calculate-the-sensitivity-and-specificity\n",
        "    custom auc:\n",
        "    https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def precision(self, y_true, y_pred):\n",
        "        true_positives = tf.keras.backend.sum(\n",
        "            tf.keras.backend.round(\n",
        "                tf.keras.backend.clip(y_true * y_pred, 0, 1)\n",
        "            )\n",
        "        )\n",
        "        predicted_positives = tf.keras.backend.sum(\n",
        "            tf.keras.backend.round(\n",
        "                tf.keras.backend.clip(y_pred, 0, 1)\n",
        "            )\n",
        "        )\n",
        "        precision = true_positives / \\\n",
        "            (predicted_positives + tf.keras.backend.epsilon())\n",
        "        return precision\n",
        "\n",
        "    def recall(self, y_true, y_pred):\n",
        "        true_positives = tf.keras.backend.sum(\n",
        "            tf.keras.backend.round(\n",
        "                tf.keras.backend.clip(y_true * y_pred, 0, 1)\n",
        "            )\n",
        "        )\n",
        "        possible_positives = tf.keras.backend.sum(\n",
        "            tf.keras.backend.round(\n",
        "                tf.keras.backend.clip(y_true, 0, 1)\n",
        "            )\n",
        "        )\n",
        "        recall = true_positives / \\\n",
        "            (possible_positives + tf.keras.backend.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def f1(self, y_true, y_pred):\n",
        "        precision = self.precision(y_true, y_pred)\n",
        "        recall = self.recall(y_true, y_pred)\n",
        "        return 2 * ((precision * recall) /\n",
        "                    (precision + recall + tf.keras.backend.epsilon()))\n",
        "\n",
        "    def auc(self, y_true, y_pred):\n",
        "        return tf.py_function(auc, (y_true, y_pred), tf.double)\n",
        "\n",
        "    def specificity(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        param:\n",
        "        y_pred - Predicted labels\n",
        "        y_true - True labels \n",
        "        Returns:\n",
        "        Specificity score\n",
        "        \"\"\"\n",
        "        neg_y_true = 1 - y_true\n",
        "        neg_y_pred = 1 - y_pred\n",
        "        fp = tf.keras.backend.sum(neg_y_true * y_pred)\n",
        "        tn = tf.keras.backend.sum(neg_y_true * neg_y_pred)\n",
        "        specificity = tn / (tn + fp + tf.keras.backend.epsilon())\n",
        "        return specificity\n",
        "\n",
        "\n",
        "_my_metrics = MyMetrics()\n",
        "\n",
        "\n",
        "class Model:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: tf.keras.models.Sequential,\n",
        "        loss: str,\n",
        "        optimizer: tf.keras.optimizers,\n",
        "        metrics: list = list((\n",
        "            _my_metrics.f1,\n",
        "            # _my_metrics.auc,\n",
        "            tf.keras.metrics.AUC(),\n",
        "            tf.keras.metrics.Precision(),\n",
        "            tf.keras.metrics.Recall(),\n",
        "            _my_metrics.specificity,\n",
        "            tf.keras.metrics.SpecificityAtSensitivity(0.5),\n",
        "            tf.keras.metrics.SensitivityAtSpecificity(0.5),\n",
        "            'accuracy')),\n",
        "        summary=True\n",
        "    ) -> None:\n",
        "        self.model = model\n",
        "        self.model.compile(\n",
        "            loss=loss,\n",
        "            optimizer=optimizer,\n",
        "            metrics=metrics\n",
        "        )\n",
        "        if summary:\n",
        "            print(self.model.summary())\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        X_train: np.array,\n",
        "        y_train: np.array,\n",
        "        epochs: int,\n",
        "        validation_split: float,\n",
        "        batch_size: int,\n",
        "        model_path: str = \"model.h5\",\n",
        "        monitor: str = \"val_f1\",\n",
        "        mode: str = \"max\"\n",
        "    ) -> None:\n",
        "        self.history = self.model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=epochs,\n",
        "            validation_split=validation_split,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=[\n",
        "                tf.keras.callbacks.ModelCheckpoint(\n",
        "                    model_path,\n",
        "                    save_best_only=True,\n",
        "                    monitor=monitor,\n",
        "                    mode=mode\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def plot_history(self, key: str, title=None) -> None:\n",
        "        plt.figure(figsize=(16, 4), dpi=200)\n",
        "        plt.plot(self.history.history[\"{}\".format(key)])\n",
        "        plt.plot(self.history.history[\"val_{}\".format(key)])\n",
        "        plt.legend([\"{}\".format(key), \"val_{}\".format(key)])\n",
        "        plt.xlabel(\"# Epochs\")\n",
        "        plt.ylabel(\"{}\".format(key))\n",
        "        if title:\n",
        "            plt.title(\"{}\".format(title))\n",
        "        plt.savefig(\"{}.png\".format(key))\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "class InferenceModel:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path: str,\n",
        "        custom_objects: dict = dict({\n",
        "            'f1': _my_metrics.f1,\n",
        "            # 'auc': _my_metrics.auc,\n",
        "            'auc': tf.keras.metrics.AUC(),\n",
        "            'precision': tf.keras.metrics.Precision(),\n",
        "            'recall': tf.keras.metrics.Recall(),\n",
        "            'specificity': _my_metrics.specificity,\n",
        "            'spec_at_sen': tf.keras.metrics.SpecificityAtSensitivity(0.5),\n",
        "            'sen_at_spec': tf.keras.metrics.SensitivityAtSpecificity(0.5), })\n",
        "    ) -> None:\n",
        "        self.model = tf.keras.models.load_model(\n",
        "            model_path,\n",
        "            custom_objects=custom_objects\n",
        "        )\n",
        "\n",
        "    def predict(self, X_test: np.array) -> np.array:\n",
        "        y_pred = self.model.predict(X_test)\n",
        "        return y_pred.flatten()\n",
        "\n",
        "    \"\"\"\n",
        "    TO DO if needed plot \n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    specificity = tn / (tn+fp)\n",
        "    \"\"\"\n",
        "\n",
        "    def evaluate(self, y_true: np.array, y_pred: np.array) -> None:\n",
        "        threshold_range = np.arange(0.1, 1.0, 0.001)\n",
        "\n",
        "        f1_scores = list()\n",
        "        for lambda_ in threshold_range:\n",
        "            f1_scores.append(f1_score(y_true, (y_pred > lambda_).astype(int)))\n",
        "\n",
        "        logging.info(\"Max f1: {:.4f}, at thres = {:.4f}\".format(\n",
        "            np.max(f1_scores), threshold_range[np.argmax(f1_scores)]\n",
        "        ))\n",
        "\n",
        "        fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "        plt.plot([0, 1], [0, 1], linestyle=\"dashed\")\n",
        "        plt.plot(fpr, tpr, marker=\"o\")\n",
        "        plt.plot([0, 0, 1], [0, 1, 1], linestyle=\"dashed\", c=\"red\")\n",
        "        plt.legend([\n",
        "            \"No Skill\",\n",
        "            \"ROC curve (area = {:.2f})\".format(auc(fpr, tpr)),\n",
        "            \"Perfect\"\n",
        "        ])\n",
        "        plt.xlabel(\"False Positive Rate\")\n",
        "        plt.ylabel(\"True Positive Rate\")\n",
        "        plt.title(\"ROC Curve\")\n",
        "        plt.savefig(\"roc_curve.png\")\n",
        "        plt.close()\n",
        "\n",
        "        precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
        "        plt.plot([0, 1], [0, 0], linestyle=\"dashed\")\n",
        "        plt.plot(recall, precision, marker=\"o\")\n",
        "        plt.legend([\n",
        "            \"No Skill\",\n",
        "            \"Model\"\n",
        "        ])\n",
        "        plt.xlabel(\"Recall\")\n",
        "        plt.ylabel(\"Precision\")\n",
        "        plt.title(\"Precision-recall Curve\")\n",
        "        plt.savefig(\"pc_curve.png\")\n",
        "\n",
        "        print(confusion_matrix(\n",
        "            y_true,\n",
        "            (y_pred > threshold_range[np.argmax(f1_scores)]).astype(int)\n",
        "        ))\n"
      ],
      "metadata": {
        "id": "Sf8-Q3mf64tH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train.py"
      ],
      "metadata": {
        "id": "xqQfgylOIFzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "import torch\n",
        "import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Flatten, Bidirectional\n",
        "from keras.layers.convolutional import Conv1D\n",
        "\n",
        "\n",
        "data_base_dir = \"/content/drive/MyDrive/google_cv\"\n",
        "os.makedirs(data_base_dir, exist_ok=True)\n",
        "cache_base_dir = \".cache\"\n",
        "os.makedirs(cache_base_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "def _embed(\n",
        "    video_data_dir: str,\n",
        "    output_dir: str\n",
        ") -> None:\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    facenet = Facenet()\n",
        "    for path in tqdm.tqdm(os.listdir(video_data_dir)):\n",
        "        if os.path.exists(os.path.join(output_dir, \"{}.npy\".format(path))):\n",
        "            continue\n",
        "\n",
        "        vidcap = cv2.VideoCapture(os.path.join(video_data_dir, path))\n",
        "        success, image = vidcap.read()\n",
        "\n",
        "        embeddings = ()\n",
        "        while success:\n",
        "            embeddings = embeddings + tuple(facenet.get_embedding(cv2.resize(\n",
        "                image, (200, 200)), batched=False)[0].flatten())\n",
        "            success, image = vidcap.read()\n",
        "\n",
        "        embeddings = np.array(embeddings)\n",
        "\n",
        "        np.save(os.path.join(output_dir, path), embeddings)\n",
        "\n",
        "\n",
        "def _get_chunk_array(input_arr: np.array, chunk_size: int) -> np.array:\n",
        "    if input_arr.size == 0:\n",
        "        return np.zeros(1859, dtype=np.uint8).tolist()\n",
        "    usable_vec = input_arr[:(\n",
        "        np.floor(len(input_arr)/chunk_size)*chunk_size).astype(int)]\n",
        "\n",
        "    i_pad = np.concatenate((usable_vec, np.array(\n",
        "        [input_arr[-1]]*(chunk_size-len(usable_vec) % chunk_size))))\n",
        "    asymmetric_chunks = np.split(\n",
        "        i_pad,\n",
        "        list(range(\n",
        "            chunk_size,\n",
        "            input_arr.shape[0] + 1,\n",
        "            chunk_size\n",
        "        ))\n",
        "    )\n",
        "    return tuple(asymmetric_chunks)\n",
        "\n",
        "\n",
        "def _preprocess(\n",
        "    label_path: str,\n",
        "    mapping_path: str,\n",
        "    data_dir: str,\n",
        "    cache_path: str\n",
        ") -> Tuple[np.array]:\n",
        "    \"\"\"\n",
        "    can consider reducing precision of np.float32 to np.float16 to reduce memory consumption\n",
        "\n",
        "    abstract:\n",
        "    https://towardsdatascience.com/overcoming-data-preprocessing-bottlenecks-with-tensorflow-data-service-nvidia-dali-and-other-d6321917f851\n",
        "    cuda solution:\n",
        "    https://stackoverflow.com/questions/60996756/how-do-i-assign-a-numpy-int64-to-a-torch-cuda-floattensor\n",
        "    static memory allocation solution:\n",
        "    https://pytorch.org/docs/stable/generated/torch.zeros.html\n",
        "    \"\"\"\n",
        "    if os.path.exists(\"{}.npz\".format(cache_path)):\n",
        "        __cache__ = np.load(\"{}.npz\".format(cache_path), allow_pickle=True)\n",
        "        return tuple((__cache__[k] for k in __cache__))\n",
        "\n",
        "    pass_videos = list([\n",
        "        \"0096.mp4\", \"0097.mp4\", \"0098.mp4\",\n",
        "        \"0125.mp4\", \"0126.mp4\", \"0127.mp4\",\n",
        "        \"0145.mp4\", \"0146.mp4\", \"0147.mp4\",\n",
        "        \"0178.mp4\", \"0179.mp4\", \"0180.mp4\"\n",
        "    ])\n",
        "    raw_labels = json.load(open(label_path, \"r\"))\n",
        "    encoding_filename_mapping = json.load(open(mapping_path, \"r\"))\n",
        "\n",
        "    embedding_path_list = sorted([\n",
        "        x for x in os.listdir(data_dir)\n",
        "        if x.split(\".npy\")[0] not in pass_videos\n",
        "        and encoding_filename_mapping[x.replace(\".npy\", \"\")] in raw_labels\n",
        "    ])\n",
        "\n",
        "    embedding_list_train, embedding_list_test, _, _ = train_test_split(\n",
        "        embedding_path_list,\n",
        "        list(range(len(embedding_path_list))),\n",
        "        test_size=0.1,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    chunk_size = 30\n",
        "\n",
        "    video_embeddings_list_train = ()\n",
        "    video_labels_list_train = ()\n",
        "    logging.debug(\n",
        "        \"taking training chunks, length = {}\".format(len(embedding_list_train))\n",
        "    )\n",
        "    for path in tqdm.tqdm(embedding_list_train):\n",
        "        real_filename = encoding_filename_mapping[path.replace(\".npy\", \"\")]\n",
        "\n",
        "        buf_embedding = np.load(os.path.join(data_dir, path))\n",
        "        if buf_embedding.shape[0] == 0:\n",
        "            continue\n",
        "\n",
        "        video_embeddings_list_train = video_embeddings_list_train + \\\n",
        "            (*_get_chunk_array(buf_embedding, chunk_size),)\n",
        "\n",
        "        flicker_idxs = np.array(raw_labels[real_filename]) - 1\n",
        "        buf_label = np.zeros(buf_embedding.shape[0]).astype(\n",
        "            np.uint8) if buf_embedding.shape[0] > 0 else np.zeros(1859, dtype=int).tolist()\n",
        "        buf_label[flicker_idxs] = 1\n",
        "        video_labels_list_train = video_labels_list_train + tuple(\n",
        "            1 if sum(x) else 0\n",
        "            for x in _get_chunk_array(buf_label, chunk_size)\n",
        "        )\n",
        "\n",
        "    video_embeddings_list_test = ()\n",
        "    video_labels_list_test = ()\n",
        "    logging.debug(\n",
        "        \"taking testing chunks, length = {}\".format(len(embedding_list_test))\n",
        "    )\n",
        "    for path in tqdm.tqdm(embedding_list_test):\n",
        "        real_filename = encoding_filename_mapping[path.replace(\".npy\", \"\")]\n",
        "\n",
        "        buf_embedding = np.load(os.path.join(data_dir, path))\n",
        "        if buf_embedding.shape[0] == 0:\n",
        "            continue\n",
        "\n",
        "        video_embeddings_list_test = video_embeddings_list_test + \\\n",
        "            (*_get_chunk_array(buf_embedding, chunk_size),)\n",
        "\n",
        "        flicker_idxs = np.array(raw_labels[real_filename]) - 1\n",
        "        buf_label = np.zeros(buf_embedding.shape[0]).astype(np.uint8)\n",
        "        buf_label[flicker_idxs] = 1\n",
        "        video_labels_list_test = video_labels_list_test + tuple(\n",
        "            1 if sum(x) else 0\n",
        "            for x in _get_chunk_array(buf_label, chunk_size)\n",
        "        )\n",
        "    X_train = np.array(video_embeddings_list_train)\n",
        "    X_test = np.array(video_embeddings_list_test)\n",
        "    y_train = np.array(video_labels_list_train)\n",
        "    y_test = np.array(video_labels_list_test)\n",
        "\n",
        "    logging.debug(\"ok. got training: {}/{}, testing: {}/{}\".format(\n",
        "        X_train.shape, y_train.shape,\n",
        "        X_test.shape, y_test.shape\n",
        "    ))\n",
        "\n",
        "    np.savez(cache_path, X_train, X_test, y_train, y_test)\n",
        "\n",
        "    return (X_train, X_test, y_train, y_test)\n",
        "\n",
        "\n",
        "def _oversampling(\n",
        "    X_train: np.array,\n",
        "    y_train: np.array,\n",
        "    method=\"SMOTE\"\n",
        ") -> Tuple[np.array]:\n",
        "    \"\"\"\n",
        "    batched alternative:\n",
        "    https://imbalanced-learn.org/stable/references/generated/imblearn.keras.BalancedBatchGenerator.html\n",
        "    \"\"\"\n",
        "    train_path = os.path.join(data_base_dir,\"flicker_detection_model_architecture/X_train.npy\")\n",
        "    test_path = os.path.join(data_base_dir,\"flicker_detection_model_architecture/y_train.npy\")\n",
        "    if os.path.exists(train_path) and os.path.exists(test_path):\n",
        "        X_train , y_train = np.load(train_path), np.load(test_path)\n",
        "        logging.info(\"{}{}\".format(X_train.shape, y_train.shape))\n",
        "        return X_train, y_train\n",
        "\n",
        "    sm = SMOTE(random_state=42)\n",
        "    original_X_shape = X_train.shape\n",
        "    X_train, y_train = sm.fit_resample(\n",
        "        np.reshape(X_train, (-1, np.prod(original_X_shape[1:]))),\n",
        "        y_train\n",
        "    )\n",
        "    X_train = np.reshape(X_train, (-1,) + original_X_shape[1:])\n",
        "    np.save(\"X_train.npy\", X_train)\n",
        "    np.save(\"y_train.npy\", y_train)\n",
        "    return (X_train, y_train)\n",
        "\n",
        "\n",
        "def _train(X_train: np.array, y_train: np.array) -> object:\n",
        "    buf = Sequential()\n",
        "    buf.add(Bidirectional(LSTM(units=256, activation='sigmoid'),\n",
        "                          input_shape=(X_train.shape[1:])))\n",
        "    buf.add(Dense(units=128, activation=\"sigmoid\"))\n",
        "    buf.add(Flatten())\n",
        "    buf.add(Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "    model = Model(\n",
        "        model=buf,\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    )\n",
        "    model.train(X_train, y_train, 1000, 0.1, 1024)\n",
        "    for k in list((\"loss\", \"accuracy\", \"f1\", \"auc\", \"specificity\")):\n",
        "        model.plot_history(k, title=\"{} - LSTM, Chunk, Oversampling\".format(k))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def _test(model_path: str, X_test: np.array, y_test: np.array) -> None:\n",
        "    model = InferenceModel(model_path)\n",
        "    y_pred = model.predict(X_test)\n",
        "    model.evaluate(y_test, y_pred)\n",
        "\n"
      ],
      "metadata": {
        "id": "WRYNAZg5IAA-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline()-> Tuple[np.array]:\n",
        "    # import gc\n",
        "    init_logger()\n",
        "\n",
        "    logging.info(\"[Embedding] Start ...\")\n",
        "    _embed(\n",
        "        os.path.join(data_base_dir, \"flicker-detection\"),\n",
        "        os.path.join(data_base_dir, \"embedding\")\n",
        "    )\n",
        "    logging.info(\"[Embedding] done.\")\n",
        "\n",
        "    # gc.collect()\n",
        "    \n",
        "    logging.info(\"[Preprocessing] Start ...\")\n",
        "    X_train, X_test, y_train, y_test = _preprocess(\n",
        "        os.path.join(\"/content/drive/MyDrive/google_cv/flicker_detection_model_architecture/\", \"label.json\"),\n",
        "        os.path.join(\"/content/drive/MyDrive/google_cv/flicker_detection_model_architecture/\", \"mapping.json\"),\n",
        "        os.path.join(data_base_dir, \"embedding\"),\n",
        "        os.path.join(cache_base_dir, \"train_test\")\n",
        "    )\n",
        "    logging.info(\"[Preprocessing] done.\")\n",
        "\n",
        "    logging.info(\"[Oversampling] Start ...\")\n",
        "    X_train, y_train = _oversampling(\n",
        "        X_train,\n",
        "        y_train\n",
        "    )\n",
        "    logging.info(\"[Oversampling] done.\")\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "X_train, X_test, y_train, y_test = pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "nT-rZW7jGtxy",
        "outputId": "6cc6bbbc-15eb-4421-df36-40b6715d37a7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[04/03/22 14:27:59]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m root\u001b[1m[\u001b[0m\u001b[1;36m1376\u001b[0m\u001b[1m]\u001b[0m                     \u001b]8;id=554619;file://<ipython-input-1-4b676fa22426>\u001b\\\u001b[2m<ipython-input-1-4b676fa22426>\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=374469;file://<ipython-input-1-4b676fa22426>#30\u001b\\\u001b[2m30\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;35mMainProcess\u001b[0m\u001b[1m(\u001b[0mMainThread\u001b[1m)\u001b[0m \u001b[1m<\u001b[0m\u001b[1;95mipyth\u001b[0m \u001b[2m                                 \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;95mon-input-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;95m-4b676fa22426\u001b[0m\u001b[1m>\u001b[0m:\u001b[1;36m30\u001b[0m    \u001b[2m                                 \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         Initializing ok.               \u001b[2m                                 \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/03/22 14:27:59] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> root<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1376</span><span style=\"font-weight: bold\">]</span>                     <a href=\"file://<ipython-input-1-4b676fa22426>\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-1-4b676fa22426&gt;</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://<ipython-input-1-4b676fa22426>#30\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">30</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MainProcess</span><span style=\"font-weight: bold\">(</span>MainThread<span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">ipyth</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                 </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">on-input-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">-4b676fa22426</span><span style=\"font-weight: bold\">&gt;</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                 </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         Initializing ok.               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                 </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m root\u001b[1m[\u001b[0m\u001b[1;36m1376\u001b[0m\u001b[1m]\u001b[0m                     \u001b]8;id=990030;file://<ipython-input-10-863391bb9b42>\u001b\\\u001b[2m<ipython-input-10-863391bb9b42>\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=205639;file://<ipython-input-10-863391bb9b42>#5\u001b\\\u001b[2m5\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;35mMainProcess\u001b[0m\u001b[1m(\u001b[0mMainThread\u001b[1m)\u001b[0m \u001b[1m<\u001b[0m\u001b[1;95mipyth\u001b[0m \u001b[2m                                 \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;95mon-input-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;95m-863391bb9b42\u001b[0m\u001b[1m>\u001b[0m:\u001b[1;36m5\u001b[0m    \u001b[2m                                 \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1m[\u001b[0mEmbedding\u001b[1m]\u001b[0m Start \u001b[33m...\u001b[0m          \u001b[2m                                 \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> root<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1376</span><span style=\"font-weight: bold\">]</span>                     <a href=\"file://<ipython-input-10-863391bb9b42>\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-10-863391bb9b42&gt;</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://<ipython-input-10-863391bb9b42>#5\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MainProcess</span><span style=\"font-weight: bold\">(</span>MainThread<span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">ipyth</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                 </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">on-input-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">-863391bb9b42</span><span style=\"font-weight: bold\">&gt;</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                 </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"font-weight: bold\">[</span>Embedding<span style=\"font-weight: bold\">]</span> Start <span style=\"color: #808000; text-decoration-color: #808000\">...</span>          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                 </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m tensorflow\u001b[1m[\u001b[0m\u001b[1;36m1376\u001b[0m\u001b[1m]\u001b[0m \u001b[1;35mMainProcess\u001b[0m\u001b[1m(\u001b[0mMainThread\u001b[1m)\u001b[0m        \u001b]8;id=982271;file:///usr/local/lib/python3.7/dist-packages/keras/applications/mobilenet.py\u001b\\\u001b[2mmobilenet.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=693167;file:///usr/local/lib/python3.7/dist-packages/keras/applications/mobilenet.py#226\u001b\\\u001b[2m226\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         mobilenet:\u001b[1;36m226\u001b[0m  `input_shape` is undefined or    \u001b[2m                \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         non-square, or `rows` is not in \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m160\u001b[0m, \u001b[1;36m192\u001b[0m, \u001b[2m                \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;36m224\u001b[0m\u001b[1m]\u001b[0m. Weights for input shape \u001b[1m(\u001b[0m\u001b[1;36m224\u001b[0m, \u001b[1;36m224\u001b[0m\u001b[1m)\u001b[0m will   \u001b[2m                \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         be loaded as the default.                       \u001b[2m                \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> tensorflow<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1376</span><span style=\"font-weight: bold\">]</span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MainProcess</span><span style=\"font-weight: bold\">(</span>MainThread<span style=\"font-weight: bold\">)</span>        <a href=\"file:///usr/local/lib/python3.7/dist-packages/keras/applications/mobilenet.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">mobilenet.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.7/dist-packages/keras/applications/mobilenet.py#226\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">226</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         mobilenet:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">226</span>  `input_shape` is undefined or    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         non-square, or `rows` is not in <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">160</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">192</span>, <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">224</span><span style=\"font-weight: bold\">]</span>. Weights for input shape <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">224</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">224</span><span style=\"font-weight: bold\">)</span> will   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         be loaded as the default.                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 145/145 [00:00<00:00, 1188.60it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[04/03/22 14:28:02]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m root\u001b[1m[\u001b[0m\u001b[1;36m1376\u001b[0m\u001b[1m]\u001b[0m                    \u001b]8;id=324104;file://<ipython-input-10-863391bb9b42>\u001b\\\u001b[2m<ipython-input-10-863391bb9b42>\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=930585;file://<ipython-input-10-863391bb9b42>#10\u001b\\\u001b[2m10\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;35mMainProcess\u001b[0m\u001b[1m(\u001b[0mMainThread\u001b[1m)\u001b[0m \u001b[1m<\u001b[0m\u001b[1;95mipyt\u001b[0m \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;95mhon-input-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;95m-863391bb9b42\u001b[0m\u001b[1m>\u001b[0m:\u001b[1;36m10\u001b[0m \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m           \u001b[1m[\u001b[0mEmbedding\u001b[1m]\u001b[0m done.           \u001b[2m                                  \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/03/22 14:28:02] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> root<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1376</span><span style=\"font-weight: bold\">]</span>                    <a href=\"file://<ipython-input-10-863391bb9b42>\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-10-863391bb9b42&gt;</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://<ipython-input-10-863391bb9b42>#10\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MainProcess</span><span style=\"font-weight: bold\">(</span>MainThread<span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">ipyt</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">hon-input-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">-863391bb9b42</span><span style=\"font-weight: bold\">&gt;</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>           <span style=\"font-weight: bold\">[</span>Embedding<span style=\"font-weight: bold\">]</span> done.           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m root\u001b[1m[\u001b[0m\u001b[1;36m1376\u001b[0m\u001b[1m]\u001b[0m                    \u001b]8;id=787693;file://<ipython-input-10-863391bb9b42>\u001b\\\u001b[2m<ipython-input-10-863391bb9b42>\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=513559;file://<ipython-input-10-863391bb9b42>#14\u001b\\\u001b[2m14\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;35mMainProcess\u001b[0m\u001b[1m(\u001b[0mMainThread\u001b[1m)\u001b[0m \u001b[1m<\u001b[0m\u001b[1;95mipyt\u001b[0m \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;95mhon-input-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;95m-863391bb9b42\u001b[0m\u001b[1m>\u001b[0m:\u001b[1;36m14\u001b[0m \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m           \u001b[1m[\u001b[0mPreprocessing\u001b[1m]\u001b[0m Start \u001b[33m...\u001b[0m   \u001b[2m                                  \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> root<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1376</span><span style=\"font-weight: bold\">]</span>                    <a href=\"file://<ipython-input-10-863391bb9b42>\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-10-863391bb9b42&gt;</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://<ipython-input-10-863391bb9b42>#14\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">14</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MainProcess</span><span style=\"font-weight: bold\">(</span>MainThread<span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">ipyt</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">hon-input-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">-863391bb9b42</span><span style=\"font-weight: bold\">&gt;</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>           <span style=\"font-weight: bold\">[</span>Preprocessing<span style=\"font-weight: bold\">]</span> Start <span style=\"color: #808000; text-decoration-color: #808000\">...</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[04/03/22 14:28:17]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m root\u001b[1m[\u001b[0m\u001b[1;36m1376\u001b[0m\u001b[1m]\u001b[0m                    \u001b]8;id=507209;file://<ipython-input-10-863391bb9b42>\u001b\\\u001b[2m<ipython-input-10-863391bb9b42>\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=167905;file://<ipython-input-10-863391bb9b42>#21\u001b\\\u001b[2m21\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;35mMainProcess\u001b[0m\u001b[1m(\u001b[0mMainThread\u001b[1m)\u001b[0m \u001b[1m<\u001b[0m\u001b[1;95mipyt\u001b[0m \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;95mhon-input-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;95m-863391bb9b42\u001b[0m\u001b[1m>\u001b[0m:\u001b[1;36m21\u001b[0m \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m           \u001b[1m[\u001b[0mPreprocessing\u001b[1m]\u001b[0m done.       \u001b[2m                                  \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/03/22 14:28:17] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> root<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1376</span><span style=\"font-weight: bold\">]</span>                    <a href=\"file://<ipython-input-10-863391bb9b42>\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-10-863391bb9b42&gt;</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://<ipython-input-10-863391bb9b42>#21\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MainProcess</span><span style=\"font-weight: bold\">(</span>MainThread<span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">ipyt</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">hon-input-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">-863391bb9b42</span><span style=\"font-weight: bold\">&gt;</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>           <span style=\"font-weight: bold\">[</span>Preprocessing<span style=\"font-weight: bold\">]</span> done.       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m root\u001b[1m[\u001b[0m\u001b[1;36m1376\u001b[0m\u001b[1m]\u001b[0m                    \u001b]8;id=873568;file://<ipython-input-10-863391bb9b42>\u001b\\\u001b[2m<ipython-input-10-863391bb9b42>\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=328937;file://<ipython-input-10-863391bb9b42>#23\u001b\\\u001b[2m23\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;35mMainProcess\u001b[0m\u001b[1m(\u001b[0mMainThread\u001b[1m)\u001b[0m \u001b[1m<\u001b[0m\u001b[1;95mipyt\u001b[0m \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;95mhon-input-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;95m-863391bb9b42\u001b[0m\u001b[1m>\u001b[0m:\u001b[1;36m23\u001b[0m \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m           \u001b[1m[\u001b[0mOversampling\u001b[1m]\u001b[0m Start \u001b[33m...\u001b[0m    \u001b[2m                                  \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> root<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1376</span><span style=\"font-weight: bold\">]</span>                    <a href=\"file://<ipython-input-10-863391bb9b42>\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-10-863391bb9b42&gt;</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://<ipython-input-10-863391bb9b42>#23\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">23</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MainProcess</span><span style=\"font-weight: bold\">(</span>MainThread<span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">ipyt</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">hon-input-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">-863391bb9b42</span><span style=\"font-weight: bold\">&gt;</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>           <span style=\"font-weight: bold\">[</span>Oversampling<span style=\"font-weight: bold\">]</span> Start <span style=\"color: #808000; text-decoration-color: #808000\">...</span>    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[04/03/22 14:28:25]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m root\u001b[1m[\u001b[0m\u001b[1;36m1376\u001b[0m\u001b[1m]\u001b[0m                    \u001b]8;id=557939;file://<ipython-input-4-1ab586e8972d>\u001b\\\u001b[2m<ipython-input-4-1ab586e8972d>\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=64497;file://<ipython-input-4-1ab586e8972d>#180\u001b\\\u001b[2m180\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;35mMainProcess\u001b[0m\u001b[1m(\u001b[0mMainThread\u001b[1m)\u001b[0m \u001b[1m<\u001b[0m\u001b[1;95mipyt\u001b[0m \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;95mhon-input-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;95m-1ab586e8972d\u001b[0m\u001b[1m>\u001b[0m:\u001b[1;36m180\u001b[0m \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m           \u001b[1m(\u001b[0m\u001b[1;36m1858\u001b[0m, \u001b[1;36m30\u001b[0m, \u001b[1;36m9216\u001b[0m\u001b[1m)\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1858\u001b[0m,\u001b[1m)\u001b[0m     \u001b[2m                                  \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/03/22 14:28:25] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> root<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1376</span><span style=\"font-weight: bold\">]</span>                    <a href=\"file://<ipython-input-4-1ab586e8972d>\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-4-1ab586e8972d&gt;</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://<ipython-input-4-1ab586e8972d>#180\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">180</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MainProcess</span><span style=\"font-weight: bold\">(</span>MainThread<span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">ipyt</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">hon-input-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">-1ab586e8972d</span><span style=\"font-weight: bold\">&gt;</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">180</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>           <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1858</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9216</span><span style=\"font-weight: bold\">)(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1858</span>,<span style=\"font-weight: bold\">)</span>     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m root\u001b[1m[\u001b[0m\u001b[1;36m1376\u001b[0m\u001b[1m]\u001b[0m                    \u001b]8;id=840303;file://<ipython-input-10-863391bb9b42>\u001b\\\u001b[2m<ipython-input-10-863391bb9b42>\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=880723;file://<ipython-input-10-863391bb9b42>#28\u001b\\\u001b[2m28\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;35mMainProcess\u001b[0m\u001b[1m(\u001b[0mMainThread\u001b[1m)\u001b[0m \u001b[1m<\u001b[0m\u001b[1;95mipyt\u001b[0m \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;95mhon-input-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;95m-863391bb9b42\u001b[0m\u001b[1m>\u001b[0m:\u001b[1;36m28\u001b[0m \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m           \u001b[1m[\u001b[0mOversampling\u001b[1m]\u001b[0m done.        \u001b[2m                                  \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> root<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1376</span><span style=\"font-weight: bold\">]</span>                    <a href=\"file://<ipython-input-10-863391bb9b42>\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-10-863391bb9b42&gt;</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://<ipython-input-10-863391bb9b42>#28\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">28</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MainProcess</span><span style=\"font-weight: bold\">(</span>MainThread<span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">ipyt</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">hon-input-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">-863391bb9b42</span><span style=\"font-weight: bold\">&gt;</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>           <span style=\"font-weight: bold\">[</span>Oversampling<span style=\"font-weight: bold\">]</span> done.        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcD66AnZ0WHI",
        "outputId": "41bc5adc-3423-47e1-9db5-4abe6f0531b4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def training(X_train:np.array,y_train:np.array) -> None:\n",
        "    logging.info(\"[Training] Start ...\")\n",
        "    model = _train(\n",
        "        X_train,\n",
        "        y_train\n",
        "    )\n",
        "    logging.info(\"[Training] done.\")\n",
        "\n",
        "    logging.info(\"[Testing] Start ...\")\n",
        "    _test(\"model.h5\", X_test, y_test)\n",
        "    logging.info(\"[Testing] done.\")\n",
        "    return model\n",
        "\n",
        "\n",
        "model = training(X_train,y_train)\n",
        "\"\"\"\n",
        "use gpu:\n",
        "https://www.tensorflow.org/guide/gpu\n",
        "\"\"\"\n",
        "# vimdiff ~/googlecv/train.py /home/henrychao/googlecv/train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "j4IHn9VHulk9",
        "outputId": "1bddcd5b-65b4-4f41-fd26-45835af706ad"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[04/03/22 15:36:40]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m root\u001b[1m[\u001b[0m\u001b[1;36m1376\u001b[0m\u001b[1m]\u001b[0m                     \u001b]8;id=111172;file://<ipython-input-20-70c0cdcc1dbf>\u001b\\\u001b[2m<ipython-input-20-70c0cdcc1dbf>\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=549522;file://<ipython-input-20-70c0cdcc1dbf>#2\u001b\\\u001b[2m2\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;35mMainProcess\u001b[0m\u001b[1m(\u001b[0mMainThread\u001b[1m)\u001b[0m \u001b[1m<\u001b[0m\u001b[1;95mipyth\u001b[0m \u001b[2m                                 \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;95mon-input-\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;95m-70c0cdcc1dbf\u001b[0m\u001b[1m>\u001b[0m:\u001b[1;36m2\u001b[0m    \u001b[2m                                 \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1m[\u001b[0mTraining\u001b[1m]\u001b[0m Start \u001b[33m...\u001b[0m           \u001b[2m                                 \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/03/22 15:36:40] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> root<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1376</span><span style=\"font-weight: bold\">]</span>                     <a href=\"file://<ipython-input-20-70c0cdcc1dbf>\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-20-70c0cdcc1dbf&gt;</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://<ipython-input-20-70c0cdcc1dbf>#2\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MainProcess</span><span style=\"font-weight: bold\">(</span>MainThread<span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">ipyth</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                 </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">on-input-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">-70c0cdcc1dbf</span><span style=\"font-weight: bold\">&gt;</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                 </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"font-weight: bold\">[</span>Training<span style=\"font-weight: bold\">]</span> Start <span style=\"color: #808000; text-decoration-color: #808000\">...</span>           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                 </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m tensorflow\u001b[1m[\u001b[0m\u001b[1;36m1376\u001b[0m\u001b[1m]\u001b[0m \u001b[1;35mMainProcess\u001b[0m\u001b[1m(\u001b[0mMainThread\u001b[1m)\u001b[0m    \u001b]8;id=926703;file:///usr/local/lib/python3.7/dist-packages/keras/layers/recurrent_v2.py\u001b\\\u001b[2mrecurrent_v2.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=756808;file:///usr/local/lib/python3.7/dist-packages/keras/layers/recurrent_v2.py#1130\u001b\\\u001b[2m1130\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         recurrent_v\u001b[1;92m2:1130\u001b[0m  Layer lstm_1 will not    \u001b[2m                    \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         use cuDNN kernels since it doesn't meet the \u001b[2m                    \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         criteria. It will use a generic GPU kernel  \u001b[2m                    \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         as fallback when running on GPU.            \u001b[2m                    \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> tensorflow<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1376</span><span style=\"font-weight: bold\">]</span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MainProcess</span><span style=\"font-weight: bold\">(</span>MainThread<span style=\"font-weight: bold\">)</span>    <a href=\"file:///usr/local/lib/python3.7/dist-packages/keras/layers/recurrent_v2.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">recurrent_v2.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.7/dist-packages/keras/layers/recurrent_v2.py#1130\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1130</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         recurrent_v<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">2:1130</span>  Layer lstm_1 will not    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         use cuDNN kernels since it doesn't meet the <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         criteria. It will use a generic GPU kernel  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         as fallback when running on GPU.            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m tensorflow\u001b[1m[\u001b[0m\u001b[1;36m1376\u001b[0m\u001b[1m]\u001b[0m \u001b[1;35mMainProcess\u001b[0m\u001b[1m(\u001b[0mMainThread\u001b[1m)\u001b[0m    \u001b]8;id=113408;file:///usr/local/lib/python3.7/dist-packages/keras/layers/recurrent_v2.py\u001b\\\u001b[2mrecurrent_v2.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=14719;file:///usr/local/lib/python3.7/dist-packages/keras/layers/recurrent_v2.py#1130\u001b\\\u001b[2m1130\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         recurrent_v\u001b[1;92m2:1130\u001b[0m  Layer lstm_1 will not    \u001b[2m                    \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         use cuDNN kernels since it doesn't meet the \u001b[2m                    \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         criteria. It will use a generic GPU kernel  \u001b[2m                    \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         as fallback when running on GPU.            \u001b[2m                    \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> tensorflow<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1376</span><span style=\"font-weight: bold\">]</span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MainProcess</span><span style=\"font-weight: bold\">(</span>MainThread<span style=\"font-weight: bold\">)</span>    <a href=\"file:///usr/local/lib/python3.7/dist-packages/keras/layers/recurrent_v2.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">recurrent_v2.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.7/dist-packages/keras/layers/recurrent_v2.py#1130\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1130</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         recurrent_v<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">2:1130</span>  Layer lstm_1 will not    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         use cuDNN kernels since it doesn't meet the <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         criteria. It will use a generic GPU kernel  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         as fallback when running on GPU.            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m tensorflow\u001b[1m[\u001b[0m\u001b[1;36m1376\u001b[0m\u001b[1m]\u001b[0m \u001b[1;35mMainProcess\u001b[0m\u001b[1m(\u001b[0mMainThread\u001b[1m)\u001b[0m    \u001b]8;id=490982;file:///usr/local/lib/python3.7/dist-packages/keras/layers/recurrent_v2.py\u001b\\\u001b[2mrecurrent_v2.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=751566;file:///usr/local/lib/python3.7/dist-packages/keras/layers/recurrent_v2.py#1130\u001b\\\u001b[2m1130\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         recurrent_v\u001b[1;92m2:1130\u001b[0m  Layer lstm_1 will not    \u001b[2m                    \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         use cuDNN kernels since it doesn't meet the \u001b[2m                    \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         criteria. It will use a generic GPU kernel  \u001b[2m                    \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         as fallback when running on GPU.            \u001b[2m                    \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> tensorflow<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1376</span><span style=\"font-weight: bold\">]</span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MainProcess</span><span style=\"font-weight: bold\">(</span>MainThread<span style=\"font-weight: bold\">)</span>    <a href=\"file:///usr/local/lib/python3.7/dist-packages/keras/layers/recurrent_v2.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">recurrent_v2.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.7/dist-packages/keras/layers/recurrent_v2.py#1130\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1130</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         recurrent_v<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">2:1130</span>  Layer lstm_1 will not    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         use cuDNN kernels since it doesn't meet the <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         criteria. It will use a generic GPU kernel  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         as fallback when running on GPU.            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_1 (Bidirectio  (None, 512)              19400704  \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 128)               65664     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19,466,497\n",
            "Trainable params: 19,466,497\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/1000\n",
            "2/2 [==============================] - 10s 4s/step - loss: 0.6839 - f1: 0.0000e+00 - auc: 0.6373 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - specificity: 0.5495 - specificity_at_sensitivity_8: 0.5242 - sensitivity_at_specificity_8: 0.6070 - accuracy: 0.5556 - val_loss: 0.8615 - val_f1: 0.0000e+00 - val_auc: 0.0000e+00 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6831 - f1: 0.0000e+00 - auc: 0.7676 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - specificity: 0.5827 - specificity_at_sensitivity_8: 0.8988 - sensitivity_at_specificity_8: 0.6555 - accuracy: 0.5556 - val_loss: 0.8229 - val_f1: 0.0000e+00 - val_auc: 0.0000e+00 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6802 - f1: 0.0000e+00 - auc: 0.6601 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - specificity: 0.5584 - specificity_at_sensitivity_8: 0.5597 - sensitivity_at_specificity_8: 0.7860 - accuracy: 0.5556 - val_loss: 0.7585 - val_f1: 0.0000e+00 - val_auc: 0.0000e+00 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6789 - f1: 0.0000e+00 - auc: 0.7842 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - specificity: 0.5401 - specificity_at_sensitivity_8: 0.9139 - sensitivity_at_specificity_8: 0.6931 - accuracy: 0.5556 - val_loss: 0.7616 - val_f1: 0.0000e+00 - val_auc: 0.0000e+00 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6767 - f1: 0.0000e+00 - auc: 0.7966 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - specificity: 0.5456 - specificity_at_sensitivity_8: 0.9107 - sensitivity_at_specificity_8: 0.7402 - accuracy: 0.5556 - val_loss: 0.7934 - val_f1: 0.0000e+00 - val_auc: 0.0000e+00 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6748 - f1: 0.0000e+00 - auc: 0.7869 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - specificity: 0.5655 - specificity_at_sensitivity_8: 0.8945 - sensitivity_at_specificity_8: 0.8062 - accuracy: 0.5556 - val_loss: 0.8285 - val_f1: 0.0000e+00 - val_auc: 0.0000e+00 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6734 - f1: 0.0000e+00 - auc: 0.7974 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - specificity: 0.5784 - specificity_at_sensitivity_8: 0.9150 - sensitivity_at_specificity_8: 0.7981 - accuracy: 0.5556 - val_loss: 0.8165 - val_f1: 0.0000e+00 - val_auc: 0.0000e+00 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6711 - f1: 0.0000e+00 - auc: 0.7977 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - specificity: 0.5684 - specificity_at_sensitivity_8: 0.9085 - sensitivity_at_specificity_8: 0.7900 - accuracy: 0.5556 - val_loss: 0.7742 - val_f1: 0.0000e+00 - val_auc: 0.0000e+00 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6690 - f1: 0.0000e+00 - auc: 0.8042 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - specificity: 0.5559 - specificity_at_sensitivity_8: 0.9128 - sensitivity_at_specificity_8: 0.8466 - accuracy: 0.5556 - val_loss: 0.7631 - val_f1: 0.0000e+00 - val_auc: 0.0000e+00 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6672 - f1: 0.0000e+00 - auc: 0.8018 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - specificity: 0.5545 - specificity_at_sensitivity_8: 0.9128 - sensitivity_at_specificity_8: 0.7658 - accuracy: 0.5556 - val_loss: 0.7750 - val_f1: 0.0000e+00 - val_auc: 0.0000e+00 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6648 - f1: 0.0000e+00 - auc: 0.8079 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - specificity: 0.5618 - specificity_at_sensitivity_8: 0.9117 - sensitivity_at_specificity_8: 0.8291 - accuracy: 0.5556 - val_loss: 0.7975 - val_f1: 0.0000e+00 - val_auc: 0.0000e+00 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6631 - f1: 0.0000e+00 - auc: 0.8030 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - specificity: 0.5775 - specificity_at_sensitivity_8: 0.9139 - sensitivity_at_specificity_8: 0.8223 - accuracy: 0.5556 - val_loss: 0.8147 - val_f1: 0.0000e+00 - val_auc: 0.0000e+00 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6617 - f1: 0.0000e+00 - auc: 0.8028 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - specificity: 0.5780 - specificity_at_sensitivity_8: 0.9128 - sensitivity_at_specificity_8: 0.7981 - accuracy: 0.5556 - val_loss: 0.7853 - val_f1: 0.0000e+00 - val_auc: 0.0000e+00 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6594 - f1: 0.0000e+00 - auc: 0.7990 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - specificity: 0.5677 - specificity_at_sensitivity_8: 0.9150 - sensitivity_at_specificity_8: 0.7900 - accuracy: 0.5556 - val_loss: 0.7705 - val_f1: 0.0000e+00 - val_auc: 0.0000e+00 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6570 - f1: 0.0000e+00 - auc: 0.8020 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - specificity: 0.5681 - specificity_at_sensitivity_8: 0.9171 - sensitivity_at_specificity_8: 0.7927 - accuracy: 0.5556 - val_loss: 0.7831 - val_f1: 0.0000e+00 - val_auc: 0.0000e+00 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6549 - f1: 0.0000e+00 - auc: 0.8057 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - specificity: 0.5747 - specificity_at_sensitivity_8: 0.9160 - sensitivity_at_specificity_8: 0.8466 - accuracy: 0.5556 - val_loss: 0.7848 - val_f1: 0.0000e+00 - val_auc: 0.0000e+00 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6529 - f1: 0.0000e+00 - auc: 0.8061 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - specificity: 0.5787 - specificity_at_sensitivity_8: 0.9150 - sensitivity_at_specificity_8: 0.7968 - accuracy: 0.5556 - val_loss: 0.7742 - val_f1: 0.0000e+00 - val_auc: 0.0000e+00 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6510 - f1: 0.1462 - auc: 0.8077 - precision_8: 0.7576 - recall_8: 0.0673 - specificity: 0.5703 - specificity_at_sensitivity_8: 0.9150 - sensitivity_at_specificity_8: 0.8170 - accuracy: 0.5760 - val_loss: 0.7508 - val_f1: 0.2698 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.1559 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.1559\n",
            "Epoch 19/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6487 - f1: 0.1572 - auc: 0.8056 - precision_8: 0.7414 - recall_8: 0.1157 - specificity: 0.5696 - specificity_at_sensitivity_8: 0.9160 - sensitivity_at_specificity_8: 0.7873 - accuracy: 0.5891 - val_loss: 0.7681 - val_f1: 0.0000e+00 - val_auc: 0.0000e+00 - val_precision_8: 0.0000e+00 - val_recall_8: 0.0000e+00 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6465 - f1: 0.0000e+00 - auc: 0.8062 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - specificity: 0.5794 - specificity_at_sensitivity_8: 0.9150 - sensitivity_at_specificity_8: 0.8304 - accuracy: 0.5544 - val_loss: 0.7671 - val_f1: 0.0524 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.0269 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.0269\n",
            "Epoch 21/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6441 - f1: 0.2721 - auc: 0.8092 - precision_8: 0.7468 - recall_8: 0.1548 - specificity: 0.5760 - specificity_at_sensitivity_8: 0.9160 - sensitivity_at_specificity_8: 0.8197 - accuracy: 0.6011 - val_loss: 0.7446 - val_f1: 0.6324 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.4624 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.4624\n",
            "Epoch 22/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6417 - f1: 0.6421 - auc: 0.8091 - precision_8: 0.8315 - recall_8: 0.5182 - specificity: 0.5712 - specificity_at_sensitivity_8: 0.9150 - sensitivity_at_specificity_8: 0.8075 - accuracy: 0.7392 - val_loss: 0.7511 - val_f1: 0.6015 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.4301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.4301\n",
            "Epoch 23/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6393 - f1: 0.4988 - auc: 0.8087 - precision_8: 0.8062 - recall_8: 0.3863 - specificity: 0.5802 - specificity_at_sensitivity_8: 0.9139 - sensitivity_at_specificity_8: 0.8345 - accuracy: 0.6860 - val_loss: 0.7630 - val_f1: 0.4500 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.2903 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.2903\n",
            "Epoch 24/1000\n",
            "2/2 [==============================] - 5s 3s/step - loss: 0.6370 - f1: 0.4803 - auc: 0.8073 - precision_8: 0.7938 - recall_8: 0.3472 - specificity: 0.5857 - specificity_at_sensitivity_8: 0.9150 - sensitivity_at_specificity_8: 0.8116 - accuracy: 0.6699 - val_loss: 0.7409 - val_f1: 0.7038 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.5430 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.5430\n",
            "Epoch 25/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6347 - f1: 0.7068 - auc: 0.8076 - precision_8: 0.8469 - recall_8: 0.6030 - specificity: 0.5732 - specificity_at_sensitivity_8: 0.9150 - sensitivity_at_specificity_8: 0.8183 - accuracy: 0.7751 - val_loss: 0.7257 - val_f1: 0.7763 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6344 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6344\n",
            "Epoch 26/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6322 - f1: 0.7282 - auc: 0.8087 - precision_8: 0.8520 - recall_8: 0.6353 - specificity: 0.5736 - specificity_at_sensitivity_8: 0.9150 - sensitivity_at_specificity_8: 0.8345 - accuracy: 0.7889 - val_loss: 0.7633 - val_f1: 0.6273 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.4570 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.4570\n",
            "Epoch 27/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6318 - f1: 0.2987 - auc: 0.8028 - precision_8: 0.8127 - recall_8: 0.2746 - specificity: 0.6036 - specificity_at_sensitivity_8: 0.9128 - sensitivity_at_specificity_8: 0.8183 - accuracy: 0.6495 - val_loss: 0.7707 - val_f1: 0.5856 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.4140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.4140\n",
            "Epoch 28/1000\n",
            "2/2 [==============================] - 5s 3s/step - loss: 0.6293 - f1: 0.6653 - auc: 0.7964 - precision_8: 0.8330 - recall_8: 0.5303 - specificity: 0.5831 - specificity_at_sensitivity_8: 0.9096 - sensitivity_at_specificity_8: 0.8385 - accuracy: 0.7440 - val_loss: 0.6909 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 29/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6265 - f1: 0.7352 - auc: 0.8088 - precision_8: 0.8501 - recall_8: 0.6487 - specificity: 0.5661 - specificity_at_sensitivity_8: 0.9182 - sensitivity_at_specificity_8: 0.8331 - accuracy: 0.7931 - val_loss: 0.7317 - val_f1: 0.7843 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6452 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6452\n",
            "Epoch 30/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6240 - f1: 0.6920 - auc: 0.8051 - precision_8: 0.8477 - recall_8: 0.6070 - specificity: 0.5931 - specificity_at_sensitivity_8: 0.9160 - sensitivity_at_specificity_8: 0.8264 - accuracy: 0.7769 - val_loss: 0.7711 - val_f1: 0.6619 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.4946 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.4946\n",
            "Epoch 31/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6222 - f1: 0.6817 - auc: 0.8069 - precision_8: 0.8383 - recall_8: 0.5653 - specificity: 0.5998 - specificity_at_sensitivity_8: 0.9160 - sensitivity_at_specificity_8: 0.8277 - accuracy: 0.7584 - val_loss: 0.7310 - val_f1: 0.7843 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6452 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6452\n",
            "Epoch 32/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6197 - f1: 0.7322 - auc: 0.8081 - precision_8: 0.8544 - recall_8: 0.6474 - specificity: 0.5840 - specificity_at_sensitivity_8: 0.9160 - sensitivity_at_specificity_8: 0.8277 - accuracy: 0.7943 - val_loss: 0.7162 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 33/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6172 - f1: 0.7359 - auc: 0.8093 - precision_8: 0.8544 - recall_8: 0.6474 - specificity: 0.5862 - specificity_at_sensitivity_8: 0.9150 - sensitivity_at_specificity_8: 0.8304 - accuracy: 0.7943 - val_loss: 0.7452 - val_f1: 0.7723 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6290 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6290\n",
            "Epoch 34/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6153 - f1: 0.7149 - auc: 0.8096 - precision_8: 0.8493 - recall_8: 0.6218 - specificity: 0.6026 - specificity_at_sensitivity_8: 0.9150 - sensitivity_at_specificity_8: 0.8183 - accuracy: 0.7829 - val_loss: 0.7483 - val_f1: 0.7723 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6290 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6290\n",
            "Epoch 35/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6137 - f1: 0.7302 - auc: 0.8101 - precision_8: 0.8523 - recall_8: 0.6366 - specificity: 0.5970 - specificity_at_sensitivity_8: 0.9171 - sensitivity_at_specificity_8: 0.8533 - accuracy: 0.7895 - val_loss: 0.7154 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 36/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6108 - f1: 0.7363 - auc: 0.8097 - precision_8: 0.8544 - recall_8: 0.6474 - specificity: 0.5937 - specificity_at_sensitivity_8: 0.9171 - sensitivity_at_specificity_8: 0.8358 - accuracy: 0.7943 - val_loss: 0.7284 - val_f1: 0.7843 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6452 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6452\n",
            "Epoch 37/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6092 - f1: 0.7331 - auc: 0.8083 - precision_8: 0.8533 - recall_8: 0.6420 - specificity: 0.6056 - specificity_at_sensitivity_8: 0.9150 - sensitivity_at_specificity_8: 0.8318 - accuracy: 0.7919 - val_loss: 0.7154 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 38/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6068 - f1: 0.7396 - auc: 0.8126 - precision_8: 0.8489 - recall_8: 0.6501 - specificity: 0.5892 - specificity_at_sensitivity_8: 0.9182 - sensitivity_at_specificity_8: 0.8237 - accuracy: 0.7931 - val_loss: 0.6843 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 39/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6044 - f1: 0.7416 - auc: 0.8127 - precision_8: 0.8491 - recall_8: 0.6514 - specificity: 0.5871 - specificity_at_sensitivity_8: 0.9160 - sensitivity_at_specificity_8: 0.8506 - accuracy: 0.7937 - val_loss: 0.7367 - val_f1: 0.7843 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6452 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6452\n",
            "Epoch 40/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6036 - f1: 0.7232 - auc: 0.8100 - precision_8: 0.8540 - recall_8: 0.6299 - specificity: 0.6200 - specificity_at_sensitivity_8: 0.9182 - sensitivity_at_specificity_8: 0.8331 - accuracy: 0.7877 - val_loss: 0.7402 - val_f1: 0.7843 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6452 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6452\n",
            "Epoch 41/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6005 - f1: 0.7401 - auc: 0.8108 - precision_8: 0.8516 - recall_8: 0.6487 - specificity: 0.6029 - specificity_at_sensitivity_8: 0.9193 - sensitivity_at_specificity_8: 0.8560 - accuracy: 0.7937 - val_loss: 0.6619 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 42/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5991 - f1: 0.7391 - auc: 0.8124 - precision_8: 0.8467 - recall_8: 0.6541 - specificity: 0.5790 - specificity_at_sensitivity_8: 0.9150 - sensitivity_at_specificity_8: 0.8291 - accuracy: 0.7937 - val_loss: 0.7030 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 43/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5963 - f1: 0.7393 - auc: 0.8140 - precision_8: 0.8531 - recall_8: 0.6487 - specificity: 0.6143 - specificity_at_sensitivity_8: 0.9193 - sensitivity_at_specificity_8: 0.8358 - accuracy: 0.7943 - val_loss: 0.7594 - val_f1: 0.7843 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6452 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6452\n",
            "Epoch 44/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5951 - f1: 0.7335 - auc: 0.8130 - precision_8: 0.8541 - recall_8: 0.6460 - specificity: 0.6224 - specificity_at_sensitivity_8: 0.9160 - sensitivity_at_specificity_8: 0.8587 - accuracy: 0.7937 - val_loss: 0.6898 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 45/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5930 - f1: 0.7407 - auc: 0.8099 - precision_8: 0.8464 - recall_8: 0.6528 - specificity: 0.5921 - specificity_at_sensitivity_8: 0.9160 - sensitivity_at_specificity_8: 0.8506 - accuracy: 0.7931 - val_loss: 0.6776 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 46/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5895 - f1: 0.7367 - auc: 0.8158 - precision_8: 0.8476 - recall_8: 0.6514 - specificity: 0.6020 - specificity_at_sensitivity_8: 0.9182 - sensitivity_at_specificity_8: 0.8452 - accuracy: 0.7931 - val_loss: 0.7472 - val_f1: 0.7843 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6452 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6452\n",
            "Epoch 47/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5895 - f1: 0.7374 - auc: 0.8132 - precision_8: 0.8544 - recall_8: 0.6474 - specificity: 0.6340 - specificity_at_sensitivity_8: 0.9160 - sensitivity_at_specificity_8: 0.8452 - accuracy: 0.7943 - val_loss: 0.7355 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 48/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5861 - f1: 0.7377 - auc: 0.8159 - precision_8: 0.8531 - recall_8: 0.6487 - specificity: 0.6213 - specificity_at_sensitivity_8: 0.9182 - sensitivity_at_specificity_8: 0.8520 - accuracy: 0.7943 - val_loss: 0.6501 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 49/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5856 - f1: 0.7337 - auc: 0.8143 - precision_8: 0.8339 - recall_8: 0.6555 - specificity: 0.5896 - specificity_at_sensitivity_8: 0.9150 - sensitivity_at_specificity_8: 0.8493 - accuracy: 0.7889 - val_loss: 0.6560 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 50/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5827 - f1: 0.7400 - auc: 0.8164 - precision_8: 0.8452 - recall_8: 0.6541 - specificity: 0.6036 - specificity_at_sensitivity_8: 0.9160 - sensitivity_at_specificity_8: 0.8546 - accuracy: 0.7931 - val_loss: 0.7275 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 51/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5814 - f1: 0.7424 - auc: 0.8148 - precision_8: 0.8544 - recall_8: 0.6474 - specificity: 0.6362 - specificity_at_sensitivity_8: 0.9171 - sensitivity_at_specificity_8: 0.8452 - accuracy: 0.7943 - val_loss: 0.7171 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 52/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5786 - f1: 0.7297 - auc: 0.8179 - precision_8: 0.8509 - recall_8: 0.6528 - specificity: 0.6223 - specificity_at_sensitivity_8: 0.9203 - sensitivity_at_specificity_8: 0.8425 - accuracy: 0.7949 - val_loss: 0.6443 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 53/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5777 - f1: 0.7357 - auc: 0.8160 - precision_8: 0.8339 - recall_8: 0.6555 - specificity: 0.5977 - specificity_at_sensitivity_8: 0.9150 - sensitivity_at_specificity_8: 0.8533 - accuracy: 0.7889 - val_loss: 0.6599 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 54/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5748 - f1: 0.7338 - auc: 0.8167 - precision_8: 0.8464 - recall_8: 0.6528 - specificity: 0.6131 - specificity_at_sensitivity_8: 0.9182 - sensitivity_at_specificity_8: 0.8493 - accuracy: 0.7931 - val_loss: 0.7160 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 55/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5737 - f1: 0.7364 - auc: 0.8172 - precision_8: 0.8534 - recall_8: 0.6501 - specificity: 0.6375 - specificity_at_sensitivity_8: 0.9160 - sensitivity_at_specificity_8: 0.8533 - accuracy: 0.7949 - val_loss: 0.6918 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 56/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5712 - f1: 0.7368 - auc: 0.8191 - precision_8: 0.8464 - recall_8: 0.6528 - specificity: 0.6187 - specificity_at_sensitivity_8: 0.9193 - sensitivity_at_specificity_8: 0.8493 - accuracy: 0.7931 - val_loss: 0.6437 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 57/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5695 - f1: 0.7356 - auc: 0.8189 - precision_8: 0.8394 - recall_8: 0.6541 - specificity: 0.6087 - specificity_at_sensitivity_8: 0.9160 - sensitivity_at_specificity_8: 0.8573 - accuracy: 0.7907 - val_loss: 0.6820 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 58/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5672 - f1: 0.7412 - auc: 0.8186 - precision_8: 0.8494 - recall_8: 0.6528 - specificity: 0.6324 - specificity_at_sensitivity_8: 0.9171 - sensitivity_at_specificity_8: 0.8493 - accuracy: 0.7943 - val_loss: 0.6918 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 59/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5660 - f1: 0.7328 - auc: 0.8179 - precision_8: 0.8408 - recall_8: 0.6541 - specificity: 0.6244 - specificity_at_sensitivity_8: 0.9193 - sensitivity_at_specificity_8: 0.8748 - accuracy: 0.7913 - val_loss: 0.6583 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 60/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5640 - f1: 0.7368 - auc: 0.8195 - precision_8: 0.8467 - recall_8: 0.6541 - specificity: 0.6282 - specificity_at_sensitivity_8: 0.9171 - sensitivity_at_specificity_8: 0.8587 - accuracy: 0.7937 - val_loss: 0.6776 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 61/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5617 - f1: 0.7341 - auc: 0.8219 - precision_8: 0.8449 - recall_8: 0.6528 - specificity: 0.6266 - specificity_at_sensitivity_8: 0.9171 - sensitivity_at_specificity_8: 0.8627 - accuracy: 0.7925 - val_loss: 0.6506 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 62/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5600 - f1: 0.7304 - auc: 0.8214 - precision_8: 0.8336 - recall_8: 0.6541 - specificity: 0.6211 - specificity_at_sensitivity_8: 0.9160 - sensitivity_at_specificity_8: 0.8627 - accuracy: 0.7883 - val_loss: 0.6794 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 63/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5594 - f1: 0.7357 - auc: 0.8201 - precision_8: 0.8504 - recall_8: 0.6501 - specificity: 0.6450 - specificity_at_sensitivity_8: 0.9160 - sensitivity_at_specificity_8: 0.8506 - accuracy: 0.7937 - val_loss: 0.6797 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 64/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5614 - f1: 0.7309 - auc: 0.8107 - precision_8: 0.8423 - recall_8: 0.6541 - specificity: 0.6175 - specificity_at_sensitivity_8: 0.9182 - sensitivity_at_specificity_8: 0.8587 - accuracy: 0.7919 - val_loss: 0.6310 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 65/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5548 - f1: 0.7325 - auc: 0.8260 - precision_8: 0.8368 - recall_8: 0.6555 - specificity: 0.6273 - specificity_at_sensitivity_8: 0.9203 - sensitivity_at_specificity_8: 0.8775 - accuracy: 0.7901 - val_loss: 0.7242 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 66/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5568 - f1: 0.7401 - auc: 0.8217 - precision_8: 0.8549 - recall_8: 0.6501 - specificity: 0.6663 - specificity_at_sensitivity_8: 0.9182 - sensitivity_at_specificity_8: 0.8614 - accuracy: 0.7955 - val_loss: 0.6842 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 67/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5533 - f1: 0.7294 - auc: 0.8242 - precision_8: 0.8336 - recall_8: 0.6541 - specificity: 0.6284 - specificity_at_sensitivity_8: 0.9182 - sensitivity_at_specificity_8: 0.8829 - accuracy: 0.7883 - val_loss: 0.5888 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 68/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5535 - f1: 0.7331 - auc: 0.8266 - precision_8: 0.8296 - recall_8: 0.6555 - specificity: 0.6028 - specificity_at_sensitivity_8: 0.9182 - sensitivity_at_specificity_8: 0.8721 - accuracy: 0.7871 - val_loss: 0.6605 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 69/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5497 - f1: 0.7399 - auc: 0.8275 - precision_8: 0.8497 - recall_8: 0.6541 - specificity: 0.6503 - specificity_at_sensitivity_8: 0.9182 - sensitivity_at_specificity_8: 0.8546 - accuracy: 0.7949 - val_loss: 0.7290 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 70/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5508 - f1: 0.7375 - auc: 0.8267 - precision_8: 0.8474 - recall_8: 0.6501 - specificity: 0.6651 - specificity_at_sensitivity_8: 0.9171 - sensitivity_at_specificity_8: 0.8829 - accuracy: 0.7925 - val_loss: 0.6473 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 71/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5479 - f1: 0.7360 - auc: 0.8273 - precision_8: 0.8325 - recall_8: 0.6555 - specificity: 0.6236 - specificity_at_sensitivity_8: 0.9268 - sensitivity_at_specificity_8: 0.8869 - accuracy: 0.7883 - val_loss: 0.6018 - val_f1: 0.7922 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6559 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6559\n",
            "Epoch 72/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5472 - f1: 0.7304 - auc: 0.8254 - precision_8: 0.8339 - recall_8: 0.6555 - specificity: 0.6255 - specificity_at_sensitivity_8: 0.9193 - sensitivity_at_specificity_8: 0.8923 - accuracy: 0.7889 - val_loss: 0.6698 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 73/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5454 - f1: 0.7392 - auc: 0.8270 - precision_8: 0.8464 - recall_8: 0.6528 - specificity: 0.6568 - specificity_at_sensitivity_8: 0.9171 - sensitivity_at_specificity_8: 0.8587 - accuracy: 0.7931 - val_loss: 0.6660 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 74/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5429 - f1: 0.7344 - auc: 0.8302 - precision_8: 0.8420 - recall_8: 0.6528 - specificity: 0.6435 - specificity_at_sensitivity_8: 0.9160 - sensitivity_at_specificity_8: 0.8937 - accuracy: 0.7913 - val_loss: 0.6057 - val_f1: 0.7922 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6559 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6559\n",
            "Epoch 75/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5430 - f1: 0.7300 - auc: 0.8292 - precision_8: 0.8325 - recall_8: 0.6555 - specificity: 0.6217 - specificity_at_sensitivity_8: 0.9171 - sensitivity_at_specificity_8: 0.8735 - accuracy: 0.7883 - val_loss: 0.6265 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 76/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5415 - f1: 0.7305 - auc: 0.8282 - precision_8: 0.8365 - recall_8: 0.6541 - specificity: 0.6430 - specificity_at_sensitivity_8: 0.9225 - sensitivity_at_specificity_8: 0.8762 - accuracy: 0.7895 - val_loss: 0.6757 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 77/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5399 - f1: 0.7409 - auc: 0.8309 - precision_8: 0.8452 - recall_8: 0.6541 - specificity: 0.6547 - specificity_at_sensitivity_8: 0.9182 - sensitivity_at_specificity_8: 0.8721 - accuracy: 0.7931 - val_loss: 0.6342 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 78/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5383 - f1: 0.7351 - auc: 0.8319 - precision_8: 0.8339 - recall_8: 0.6555 - specificity: 0.6372 - specificity_at_sensitivity_8: 0.9171 - sensitivity_at_specificity_8: 0.8910 - accuracy: 0.7889 - val_loss: 0.6227 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 79/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5369 - f1: 0.7357 - auc: 0.8323 - precision_8: 0.8325 - recall_8: 0.6555 - specificity: 0.6399 - specificity_at_sensitivity_8: 0.9182 - sensitivity_at_specificity_8: 0.8775 - accuracy: 0.7883 - val_loss: 0.6556 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 80/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5357 - f1: 0.7414 - auc: 0.8321 - precision_8: 0.8467 - recall_8: 0.6541 - specificity: 0.6560 - specificity_at_sensitivity_8: 0.9193 - sensitivity_at_specificity_8: 0.8708 - accuracy: 0.7937 - val_loss: 0.6529 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 81/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5344 - f1: 0.7364 - auc: 0.8326 - precision_8: 0.8365 - recall_8: 0.6541 - specificity: 0.6560 - specificity_at_sensitivity_8: 0.9182 - sensitivity_at_specificity_8: 0.8721 - accuracy: 0.7895 - val_loss: 0.6173 - val_f1: 0.7922 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6559 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6559\n",
            "Epoch 82/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5333 - f1: 0.7351 - auc: 0.8331 - precision_8: 0.8325 - recall_8: 0.6555 - specificity: 0.6398 - specificity_at_sensitivity_8: 0.9182 - sensitivity_at_specificity_8: 0.8762 - accuracy: 0.7883 - val_loss: 0.6054 - val_f1: 0.7922 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6559 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6559\n",
            "Epoch 83/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5325 - f1: 0.7348 - auc: 0.8341 - precision_8: 0.8325 - recall_8: 0.6555 - specificity: 0.6338 - specificity_at_sensitivity_8: 0.9182 - sensitivity_at_specificity_8: 0.8775 - accuracy: 0.7883 - val_loss: 0.6474 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 84/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5319 - f1: 0.7324 - auc: 0.8336 - precision_8: 0.8423 - recall_8: 0.6541 - specificity: 0.6665 - specificity_at_sensitivity_8: 0.9214 - sensitivity_at_specificity_8: 0.9085 - accuracy: 0.7919 - val_loss: 0.6736 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 85/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5301 - f1: 0.7325 - auc: 0.8357 - precision_8: 0.8408 - recall_8: 0.6541 - specificity: 0.6617 - specificity_at_sensitivity_8: 0.9247 - sensitivity_at_specificity_8: 0.8964 - accuracy: 0.7913 - val_loss: 0.5939 - val_f1: 0.7922 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6559 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6559\n",
            "Epoch 86/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5293 - f1: 0.7342 - auc: 0.8362 - precision_8: 0.8325 - recall_8: 0.6555 - specificity: 0.6333 - specificity_at_sensitivity_8: 0.9182 - sensitivity_at_specificity_8: 0.8991 - accuracy: 0.7883 - val_loss: 0.6072 - val_f1: 0.7922 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6559 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6559\n",
            "Epoch 87/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5278 - f1: 0.7345 - auc: 0.8363 - precision_8: 0.8325 - recall_8: 0.6555 - specificity: 0.6524 - specificity_at_sensitivity_8: 0.9160 - sensitivity_at_specificity_8: 0.9125 - accuracy: 0.7883 - val_loss: 0.6495 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 88/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5263 - f1: 0.7299 - auc: 0.8372 - precision_8: 0.8336 - recall_8: 0.6541 - specificity: 0.6618 - specificity_at_sensitivity_8: 0.9193 - sensitivity_at_specificity_8: 0.8991 - accuracy: 0.7883 - val_loss: 0.6209 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 89/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5257 - f1: 0.7295 - auc: 0.8378 - precision_8: 0.8325 - recall_8: 0.6555 - specificity: 0.6434 - specificity_at_sensitivity_8: 0.9214 - sensitivity_at_specificity_8: 0.9004 - accuracy: 0.7883 - val_loss: 0.6117 - val_f1: 0.7922 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6559 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6559\n",
            "Epoch 90/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5251 - f1: 0.7378 - auc: 0.8361 - precision_8: 0.8382 - recall_8: 0.6555 - specificity: 0.6636 - specificity_at_sensitivity_8: 0.9225 - sensitivity_at_specificity_8: 0.9179 - accuracy: 0.7907 - val_loss: 0.6469 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 91/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5229 - f1: 0.7322 - auc: 0.8394 - precision_8: 0.8325 - recall_8: 0.6555 - specificity: 0.6611 - specificity_at_sensitivity_8: 0.9247 - sensitivity_at_specificity_8: 0.9085 - accuracy: 0.7883 - val_loss: 0.5977 - val_f1: 0.7922 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6559 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6559\n",
            "Epoch 92/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5221 - f1: 0.7355 - auc: 0.8390 - precision_8: 0.8325 - recall_8: 0.6555 - specificity: 0.6530 - specificity_at_sensitivity_8: 0.9214 - sensitivity_at_specificity_8: 0.9017 - accuracy: 0.7883 - val_loss: 0.6073 - val_f1: 0.7922 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6559 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6559\n",
            "Epoch 93/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5204 - f1: 0.7374 - auc: 0.8408 - precision_8: 0.8325 - recall_8: 0.6555 - specificity: 0.6539 - specificity_at_sensitivity_8: 0.9214 - sensitivity_at_specificity_8: 0.9031 - accuracy: 0.7883 - val_loss: 0.6138 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 94/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5193 - f1: 0.7344 - auc: 0.8417 - precision_8: 0.8325 - recall_8: 0.6555 - specificity: 0.6561 - specificity_at_sensitivity_8: 0.9257 - sensitivity_at_specificity_8: 0.8843 - accuracy: 0.7883 - val_loss: 0.6266 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 95/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5185 - f1: 0.7292 - auc: 0.8428 - precision_8: 0.8322 - recall_8: 0.6541 - specificity: 0.6694 - specificity_at_sensitivity_8: 0.9203 - sensitivity_at_specificity_8: 0.9031 - accuracy: 0.7877 - val_loss: 0.6298 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 96/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5172 - f1: 0.7314 - auc: 0.8434 - precision_8: 0.8325 - recall_8: 0.6555 - specificity: 0.6596 - specificity_at_sensitivity_8: 0.9247 - sensitivity_at_specificity_8: 0.9179 - accuracy: 0.7883 - val_loss: 0.5915 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 97/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5164 - f1: 0.7366 - auc: 0.8430 - precision_8: 0.8325 - recall_8: 0.6555 - specificity: 0.6570 - specificity_at_sensitivity_8: 0.9225 - sensitivity_at_specificity_8: 0.9206 - accuracy: 0.7883 - val_loss: 0.6135 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 98/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5149 - f1: 0.7363 - auc: 0.8443 - precision_8: 0.8325 - recall_8: 0.6555 - specificity: 0.6604 - specificity_at_sensitivity_8: 0.9214 - sensitivity_at_specificity_8: 0.9031 - accuracy: 0.7883 - val_loss: 0.6183 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 99/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5137 - f1: 0.7337 - auc: 0.8454 - precision_8: 0.8325 - recall_8: 0.6555 - specificity: 0.6670 - specificity_at_sensitivity_8: 0.9279 - sensitivity_at_specificity_8: 0.9031 - accuracy: 0.7883 - val_loss: 0.6233 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 100/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5128 - f1: 0.7369 - auc: 0.8459 - precision_8: 0.8325 - recall_8: 0.6555 - specificity: 0.6727 - specificity_at_sensitivity_8: 0.9268 - sensitivity_at_specificity_8: 0.9031 - accuracy: 0.7883 - val_loss: 0.5989 - val_f1: 0.7922 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6559 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6559\n",
            "Epoch 101/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5116 - f1: 0.7349 - auc: 0.8475 - precision_8: 0.8325 - recall_8: 0.6555 - specificity: 0.6581 - specificity_at_sensitivity_8: 0.9290 - sensitivity_at_specificity_8: 0.9314 - accuracy: 0.7883 - val_loss: 0.5853 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 102/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5109 - f1: 0.7320 - auc: 0.8461 - precision_8: 0.8330 - recall_8: 0.6581 - specificity: 0.6616 - specificity_at_sensitivity_8: 0.9290 - sensitivity_at_specificity_8: 0.9233 - accuracy: 0.7895 - val_loss: 0.6197 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 103/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5095 - f1: 0.7286 - auc: 0.8486 - precision_8: 0.8325 - recall_8: 0.6555 - specificity: 0.6724 - specificity_at_sensitivity_8: 0.9279 - sensitivity_at_specificity_8: 0.9300 - accuracy: 0.7883 - val_loss: 0.6007 - val_f1: 0.7922 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6559 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6559\n",
            "Epoch 104/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5082 - f1: 0.7353 - auc: 0.8489 - precision_8: 0.8325 - recall_8: 0.6555 - specificity: 0.6650 - specificity_at_sensitivity_8: 0.9290 - sensitivity_at_specificity_8: 0.9381 - accuracy: 0.7883 - val_loss: 0.5912 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 105/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5075 - f1: 0.7353 - auc: 0.8502 - precision_8: 0.8328 - recall_8: 0.6568 - specificity: 0.6655 - specificity_at_sensitivity_8: 0.9279 - sensitivity_at_specificity_8: 0.9367 - accuracy: 0.7889 - val_loss: 0.5967 - val_f1: 0.7922 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6559 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6559\n",
            "Epoch 106/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5065 - f1: 0.7306 - auc: 0.8496 - precision_8: 0.8328 - recall_8: 0.6568 - specificity: 0.6705 - specificity_at_sensitivity_8: 0.9290 - sensitivity_at_specificity_8: 0.9435 - accuracy: 0.7889 - val_loss: 0.5851 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 107/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5056 - f1: 0.7340 - auc: 0.8518 - precision_8: 0.8305 - recall_8: 0.6595 - specificity: 0.6586 - specificity_at_sensitivity_8: 0.9214 - sensitivity_at_specificity_8: 0.9475 - accuracy: 0.7889 - val_loss: 0.5902 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 108/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5047 - f1: 0.7347 - auc: 0.8505 - precision_8: 0.8328 - recall_8: 0.6568 - specificity: 0.6742 - specificity_at_sensitivity_8: 0.9343 - sensitivity_at_specificity_8: 0.9206 - accuracy: 0.7889 - val_loss: 0.6244 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 109/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5035 - f1: 0.7325 - auc: 0.8527 - precision_8: 0.8325 - recall_8: 0.6555 - specificity: 0.6837 - specificity_at_sensitivity_8: 0.9290 - sensitivity_at_specificity_8: 0.9475 - accuracy: 0.7883 - val_loss: 0.5677 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 110/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5039 - f1: 0.7346 - auc: 0.8539 - precision_8: 0.8263 - recall_8: 0.6595 - specificity: 0.6500 - specificity_at_sensitivity_8: 0.9257 - sensitivity_at_specificity_8: 0.9327 - accuracy: 0.7871 - val_loss: 0.5762 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 111/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5013 - f1: 0.7367 - auc: 0.8547 - precision_8: 0.8333 - recall_8: 0.6595 - specificity: 0.6743 - specificity_at_sensitivity_8: 0.9311 - sensitivity_at_specificity_8: 0.9421 - accuracy: 0.7901 - val_loss: 0.6554 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 112/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5022 - f1: 0.7414 - auc: 0.8534 - precision_8: 0.8423 - recall_8: 0.6541 - specificity: 0.6970 - specificity_at_sensitivity_8: 0.9311 - sensitivity_at_specificity_8: 0.9408 - accuracy: 0.7919 - val_loss: 0.5874 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 113/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5000 - f1: 0.7368 - auc: 0.8552 - precision_8: 0.8314 - recall_8: 0.6635 - specificity: 0.6613 - specificity_at_sensitivity_8: 0.9268 - sensitivity_at_specificity_8: 0.9367 - accuracy: 0.7907 - val_loss: 0.5514 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 114/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4994 - f1: 0.7380 - auc: 0.8562 - precision_8: 0.8311 - recall_8: 0.6622 - specificity: 0.6639 - specificity_at_sensitivity_8: 0.9311 - sensitivity_at_specificity_8: 0.9394 - accuracy: 0.7901 - val_loss: 0.6244 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 115/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4990 - f1: 0.7318 - auc: 0.8557 - precision_8: 0.8322 - recall_8: 0.6541 - specificity: 0.6953 - specificity_at_sensitivity_8: 0.9290 - sensitivity_at_specificity_8: 0.9542 - accuracy: 0.7877 - val_loss: 0.6045 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 116/1000\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.4965 - f1: 0.7337 - auc: 0.8583 - precision_8: 0.8330 - recall_8: 0.6581 - specificity: 0.6775 - specificity_at_sensitivity_8: 0.9300 - sensitivity_at_specificity_8: 0.9435 - accuracy: 0.7895 - val_loss: 0.5376 - val_f1: 0.8000 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6667 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6667\n",
            "Epoch 117/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4977 - f1: 0.7438 - auc: 0.8579 - precision_8: 0.8270 - recall_8: 0.6689 - specificity: 0.6531 - specificity_at_sensitivity_8: 0.9300 - sensitivity_at_specificity_8: 0.9596 - accuracy: 0.7907 - val_loss: 0.5903 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 118/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4959 - f1: 0.7367 - auc: 0.8582 - precision_8: 0.8328 - recall_8: 0.6568 - specificity: 0.6901 - specificity_at_sensitivity_8: 0.9322 - sensitivity_at_specificity_8: 0.9408 - accuracy: 0.7889 - val_loss: 0.6304 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 119/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4949 - f1: 0.7285 - auc: 0.8591 - precision_8: 0.8325 - recall_8: 0.6555 - specificity: 0.6962 - specificity_at_sensitivity_8: 0.9311 - sensitivity_at_specificity_8: 0.9677 - accuracy: 0.7883 - val_loss: 0.5417 - val_f1: 0.8000 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6667 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6667\n",
            "Epoch 120/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4956 - f1: 0.7390 - auc: 0.8609 - precision_8: 0.8210 - recall_8: 0.6729 - specificity: 0.6511 - specificity_at_sensitivity_8: 0.9290 - sensitivity_at_specificity_8: 0.9569 - accuracy: 0.7895 - val_loss: 0.5417 - val_f1: 0.8000 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6667 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6667\n",
            "Epoch 121/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4932 - f1: 0.7275 - auc: 0.8603 - precision_8: 0.8289 - recall_8: 0.6649 - specificity: 0.6709 - specificity_at_sensitivity_8: 0.9322 - sensitivity_at_specificity_8: 0.9448 - accuracy: 0.7901 - val_loss: 0.6477 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 122/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4947 - f1: 0.7368 - auc: 0.8600 - precision_8: 0.8438 - recall_8: 0.6541 - specificity: 0.7107 - specificity_at_sensitivity_8: 0.9300 - sensitivity_at_specificity_8: 0.9556 - accuracy: 0.7925 - val_loss: 0.5901 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 123/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4912 - f1: 0.7390 - auc: 0.8623 - precision_8: 0.8316 - recall_8: 0.6649 - specificity: 0.6709 - specificity_at_sensitivity_8: 0.9333 - sensitivity_at_specificity_8: 0.9475 - accuracy: 0.7913 - val_loss: 0.5162 - val_f1: 0.8039 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6720 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6720\n",
            "Epoch 124/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4929 - f1: 0.7336 - auc: 0.8616 - precision_8: 0.8228 - recall_8: 0.6689 - specificity: 0.6572 - specificity_at_sensitivity_8: 0.9300 - sensitivity_at_specificity_8: 0.9583 - accuracy: 0.7889 - val_loss: 0.6018 - val_f1: 0.7922 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6559 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6559\n",
            "Epoch 125/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4902 - f1: 0.7333 - auc: 0.8619 - precision_8: 0.8325 - recall_8: 0.6555 - specificity: 0.7000 - specificity_at_sensitivity_8: 0.9333 - sensitivity_at_specificity_8: 0.9542 - accuracy: 0.7883 - val_loss: 0.6136 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 126/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4893 - f1: 0.7381 - auc: 0.8625 - precision_8: 0.8302 - recall_8: 0.6581 - specificity: 0.6901 - specificity_at_sensitivity_8: 0.9300 - sensitivity_at_specificity_8: 0.9610 - accuracy: 0.7883 - val_loss: 0.5466 - val_f1: 0.8000 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6667 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6667\n",
            "Epoch 127/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4885 - f1: 0.7440 - auc: 0.8644 - precision_8: 0.8264 - recall_8: 0.6662 - specificity: 0.6713 - specificity_at_sensitivity_8: 0.9300 - sensitivity_at_specificity_8: 0.9677 - accuracy: 0.7895 - val_loss: 0.5744 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 128/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4874 - f1: 0.7323 - auc: 0.8647 - precision_8: 0.8330 - recall_8: 0.6581 - specificity: 0.6909 - specificity_at_sensitivity_8: 0.9333 - sensitivity_at_specificity_8: 0.9542 - accuracy: 0.7895 - val_loss: 0.6015 - val_f1: 0.7922 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6559 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6559\n",
            "Epoch 129/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4865 - f1: 0.7418 - auc: 0.8659 - precision_8: 0.8333 - recall_8: 0.6595 - specificity: 0.6950 - specificity_at_sensitivity_8: 0.9300 - sensitivity_at_specificity_8: 0.9731 - accuracy: 0.7901 - val_loss: 0.5568 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 130/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4870 - f1: 0.7484 - auc: 0.8653 - precision_8: 0.8262 - recall_8: 0.6716 - specificity: 0.6696 - specificity_at_sensitivity_8: 0.9322 - sensitivity_at_specificity_8: 0.9758 - accuracy: 0.7913 - val_loss: 0.5701 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 131/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4850 - f1: 0.7369 - auc: 0.8658 - precision_8: 0.8319 - recall_8: 0.6595 - specificity: 0.6964 - specificity_at_sensitivity_8: 0.9311 - sensitivity_at_specificity_8: 0.9758 - accuracy: 0.7895 - val_loss: 0.6238 - val_f1: 0.7883 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6505 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6505\n",
            "Epoch 132/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4852 - f1: 0.7331 - auc: 0.8667 - precision_8: 0.8328 - recall_8: 0.6568 - specificity: 0.7033 - specificity_at_sensitivity_8: 0.9322 - sensitivity_at_specificity_8: 0.9744 - accuracy: 0.7889 - val_loss: 0.5557 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 133/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4838 - f1: 0.7391 - auc: 0.8680 - precision_8: 0.8270 - recall_8: 0.6689 - specificity: 0.6751 - specificity_at_sensitivity_8: 0.9300 - sensitivity_at_specificity_8: 0.9731 - accuracy: 0.7907 - val_loss: 0.5519 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 134/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4823 - f1: 0.7379 - auc: 0.8688 - precision_8: 0.8264 - recall_8: 0.6662 - specificity: 0.6825 - specificity_at_sensitivity_8: 0.9279 - sensitivity_at_specificity_8: 0.9731 - accuracy: 0.7895 - val_loss: 0.6102 - val_f1: 0.7922 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6559 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6559\n",
            "Epoch 135/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4835 - f1: 0.7368 - auc: 0.8683 - precision_8: 0.8328 - recall_8: 0.6568 - specificity: 0.7122 - specificity_at_sensitivity_8: 0.9333 - sensitivity_at_specificity_8: 0.9785 - accuracy: 0.7889 - val_loss: 0.5794 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 136/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4816 - f1: 0.7371 - auc: 0.8689 - precision_8: 0.8275 - recall_8: 0.6649 - specificity: 0.6804 - specificity_at_sensitivity_8: 0.9300 - sensitivity_at_specificity_8: 0.9798 - accuracy: 0.7895 - val_loss: 0.5172 - val_f1: 0.8039 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6720 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6720\n",
            "Epoch 137/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4817 - f1: 0.7411 - auc: 0.8694 - precision_8: 0.8215 - recall_8: 0.6689 - specificity: 0.6738 - specificity_at_sensitivity_8: 0.9300 - sensitivity_at_specificity_8: 0.9704 - accuracy: 0.7883 - val_loss: 0.5947 - val_f1: 0.7922 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6559 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6559\n",
            "Epoch 138/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4798 - f1: 0.7377 - auc: 0.8699 - precision_8: 0.8333 - recall_8: 0.6595 - specificity: 0.7032 - specificity_at_sensitivity_8: 0.9300 - sensitivity_at_specificity_8: 0.9744 - accuracy: 0.7901 - val_loss: 0.5910 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 139/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4788 - f1: 0.7337 - auc: 0.8704 - precision_8: 0.8305 - recall_8: 0.6595 - specificity: 0.6973 - specificity_at_sensitivity_8: 0.9322 - sensitivity_at_specificity_8: 0.9758 - accuracy: 0.7889 - val_loss: 0.5454 - val_f1: 0.8000 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6667 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6667\n",
            "Epoch 140/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4781 - f1: 0.7404 - auc: 0.8712 - precision_8: 0.8270 - recall_8: 0.6689 - specificity: 0.6828 - specificity_at_sensitivity_8: 0.9300 - sensitivity_at_specificity_8: 0.9771 - accuracy: 0.7907 - val_loss: 0.5568 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 141/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4770 - f1: 0.7420 - auc: 0.8720 - precision_8: 0.8278 - recall_8: 0.6662 - specificity: 0.6919 - specificity_at_sensitivity_8: 0.9300 - sensitivity_at_specificity_8: 0.9744 - accuracy: 0.7901 - val_loss: 0.5830 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 142/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4766 - f1: 0.7345 - auc: 0.8722 - precision_8: 0.8294 - recall_8: 0.6608 - specificity: 0.6998 - specificity_at_sensitivity_8: 0.9333 - sensitivity_at_specificity_8: 0.9785 - accuracy: 0.7889 - val_loss: 0.5764 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 143/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4767 - f1: 0.7285 - auc: 0.8719 - precision_8: 0.8297 - recall_8: 0.6622 - specificity: 0.7035 - specificity_at_sensitivity_8: 0.9333 - sensitivity_at_specificity_8: 0.9744 - accuracy: 0.7895 - val_loss: 0.5421 - val_f1: 0.8039 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6720 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6720\n",
            "Epoch 144/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4767 - f1: 0.7427 - auc: 0.8707 - precision_8: 0.8231 - recall_8: 0.6824 - specificity: 0.6708 - specificity_at_sensitivity_8: 0.9322 - sensitivity_at_specificity_8: 0.9744 - accuracy: 0.7937 - val_loss: 0.5292 - val_f1: 0.8039 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6720 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6720\n",
            "Epoch 145/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4755 - f1: 0.7341 - auc: 0.8706 - precision_8: 0.8283 - recall_8: 0.6689 - specificity: 0.6945 - specificity_at_sensitivity_8: 0.9333 - sensitivity_at_specificity_8: 0.9650 - accuracy: 0.7913 - val_loss: 0.6109 - val_f1: 0.7922 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6559 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6559\n",
            "Epoch 146/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4748 - f1: 0.7277 - auc: 0.8735 - precision_8: 0.8333 - recall_8: 0.6595 - specificity: 0.7122 - specificity_at_sensitivity_8: 0.9300 - sensitivity_at_specificity_8: 0.9812 - accuracy: 0.7901 - val_loss: 0.5439 - val_f1: 0.8039 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6720 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6720\n",
            "Epoch 147/1000\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.4732 - f1: 0.7354 - auc: 0.8750 - precision_8: 0.8251 - recall_8: 0.6729 - specificity: 0.6830 - specificity_at_sensitivity_8: 0.9322 - sensitivity_at_specificity_8: 0.9825 - accuracy: 0.7913 - val_loss: 0.5197 - val_f1: 0.8077 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6774 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6774\n",
            "Epoch 148/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4730 - f1: 0.7377 - auc: 0.8752 - precision_8: 0.8237 - recall_8: 0.6729 - specificity: 0.6840 - specificity_at_sensitivity_8: 0.9322 - sensitivity_at_specificity_8: 0.9825 - accuracy: 0.7907 - val_loss: 0.5860 - val_f1: 0.7922 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6559 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6559\n",
            "Epoch 149/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4725 - f1: 0.7348 - auc: 0.8749 - precision_8: 0.8325 - recall_8: 0.6622 - specificity: 0.7144 - specificity_at_sensitivity_8: 0.9322 - sensitivity_at_specificity_8: 0.9838 - accuracy: 0.7907 - val_loss: 0.5707 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 150/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4734 - f1: 0.7452 - auc: 0.8723 - precision_8: 0.8257 - recall_8: 0.6756 - specificity: 0.6872 - specificity_at_sensitivity_8: 0.9322 - sensitivity_at_specificity_8: 0.9731 - accuracy: 0.7925 - val_loss: 0.5311 - val_f1: 0.8039 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6720 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6720\n",
            "Epoch 151/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4701 - f1: 0.7375 - auc: 0.8765 - precision_8: 0.8231 - recall_8: 0.6703 - specificity: 0.6926 - specificity_at_sensitivity_8: 0.9322 - sensitivity_at_specificity_8: 0.9879 - accuracy: 0.7895 - val_loss: 0.6054 - val_f1: 0.7922 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6559 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6559\n",
            "Epoch 152/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4711 - f1: 0.7355 - auc: 0.8771 - precision_8: 0.8333 - recall_8: 0.6595 - specificity: 0.7201 - specificity_at_sensitivity_8: 0.9300 - sensitivity_at_specificity_8: 0.9906 - accuracy: 0.7901 - val_loss: 0.5644 - val_f1: 0.8000 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6667 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6667\n",
            "Epoch 153/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4689 - f1: 0.7356 - auc: 0.8772 - precision_8: 0.8270 - recall_8: 0.6689 - specificity: 0.6964 - specificity_at_sensitivity_8: 0.9343 - sensitivity_at_specificity_8: 0.9825 - accuracy: 0.7907 - val_loss: 0.5096 - val_f1: 0.8115 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6828 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6828\n",
            "Epoch 154/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4699 - f1: 0.7529 - auc: 0.8779 - precision_8: 0.8208 - recall_8: 0.6904 - specificity: 0.6743 - specificity_at_sensitivity_8: 0.9300 - sensitivity_at_specificity_8: 0.9919 - accuracy: 0.7955 - val_loss: 0.5773 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 155/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4700 - f1: 0.7417 - auc: 0.8753 - precision_8: 0.8376 - recall_8: 0.6595 - specificity: 0.7250 - specificity_at_sensitivity_8: 0.9333 - sensitivity_at_specificity_8: 0.9838 - accuracy: 0.7919 - val_loss: 0.6078 - val_f1: 0.7922 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6559 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6559\n",
            "Epoch 156/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4683 - f1: 0.7378 - auc: 0.8777 - precision_8: 0.8289 - recall_8: 0.6649 - specificity: 0.7081 - specificity_at_sensitivity_8: 0.9343 - sensitivity_at_specificity_8: 0.9812 - accuracy: 0.7901 - val_loss: 0.4930 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 157/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4687 - f1: 0.7510 - auc: 0.8796 - precision_8: 0.8173 - recall_8: 0.6985 - specificity: 0.6742 - specificity_at_sensitivity_8: 0.9343 - sensitivity_at_specificity_8: 0.9879 - accuracy: 0.7967 - val_loss: 0.5401 - val_f1: 0.8039 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6720 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6720\n",
            "Epoch 158/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4659 - f1: 0.7414 - auc: 0.8785 - precision_8: 0.8272 - recall_8: 0.6703 - specificity: 0.7061 - specificity_at_sensitivity_8: 0.9322 - sensitivity_at_specificity_8: 0.9838 - accuracy: 0.7913 - val_loss: 0.5939 - val_f1: 0.7922 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6559 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6559\n",
            "Epoch 159/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4660 - f1: 0.7415 - auc: 0.8799 - precision_8: 0.8316 - recall_8: 0.6649 - specificity: 0.7176 - specificity_at_sensitivity_8: 0.9343 - sensitivity_at_specificity_8: 0.9865 - accuracy: 0.7913 - val_loss: 0.5343 - val_f1: 0.8077 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6774 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6774\n",
            "Epoch 160/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4648 - f1: 0.7484 - auc: 0.8813 - precision_8: 0.8235 - recall_8: 0.6783 - specificity: 0.6913 - specificity_at_sensitivity_8: 0.9365 - sensitivity_at_specificity_8: 0.9906 - accuracy: 0.7925 - val_loss: 0.5252 - val_f1: 0.8077 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6774 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6774\n",
            "Epoch 161/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4638 - f1: 0.7475 - auc: 0.8808 - precision_8: 0.8219 - recall_8: 0.6770 - specificity: 0.6959 - specificity_at_sensitivity_8: 0.9333 - sensitivity_at_specificity_8: 0.9906 - accuracy: 0.7913 - val_loss: 0.5781 - val_f1: 0.7922 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6559 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6559\n",
            "Epoch 162/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4641 - f1: 0.7344 - auc: 0.8815 - precision_8: 0.8297 - recall_8: 0.6622 - specificity: 0.7185 - specificity_at_sensitivity_8: 0.9322 - sensitivity_at_specificity_8: 0.9919 - accuracy: 0.7895 - val_loss: 0.5592 - val_f1: 0.8000 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6667 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6667\n",
            "Epoch 163/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4631 - f1: 0.7443 - auc: 0.8816 - precision_8: 0.8252 - recall_8: 0.6797 - specificity: 0.6993 - specificity_at_sensitivity_8: 0.9343 - sensitivity_at_specificity_8: 0.9825 - accuracy: 0.7937 - val_loss: 0.5137 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 164/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4626 - f1: 0.7475 - auc: 0.8822 - precision_8: 0.8236 - recall_8: 0.6851 - specificity: 0.6955 - specificity_at_sensitivity_8: 0.9333 - sensitivity_at_specificity_8: 0.9906 - accuracy: 0.7949 - val_loss: 0.5598 - val_f1: 0.8000 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6667 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6667\n",
            "Epoch 165/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4615 - f1: 0.7365 - auc: 0.8826 - precision_8: 0.8283 - recall_8: 0.6689 - specificity: 0.7109 - specificity_at_sensitivity_8: 0.9333 - sensitivity_at_specificity_8: 0.9919 - accuracy: 0.7913 - val_loss: 0.5493 - val_f1: 0.8039 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6720 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6720\n",
            "Epoch 166/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4606 - f1: 0.7401 - auc: 0.8835 - precision_8: 0.8240 - recall_8: 0.6743 - specificity: 0.7040 - specificity_at_sensitivity_8: 0.9343 - sensitivity_at_specificity_8: 0.9919 - accuracy: 0.7913 - val_loss: 0.5217 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 167/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4604 - f1: 0.7446 - auc: 0.8834 - precision_8: 0.8244 - recall_8: 0.6824 - specificity: 0.6966 - specificity_at_sensitivity_8: 0.9343 - sensitivity_at_specificity_8: 0.9919 - accuracy: 0.7943 - val_loss: 0.5429 - val_f1: 0.8077 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6774 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6774\n",
            "Epoch 168/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4594 - f1: 0.7434 - auc: 0.8841 - precision_8: 0.8254 - recall_8: 0.6743 - specificity: 0.7066 - specificity_at_sensitivity_8: 0.9354 - sensitivity_at_specificity_8: 0.9919 - accuracy: 0.7919 - val_loss: 0.5454 - val_f1: 0.8077 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6774 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6774\n",
            "Epoch 169/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4591 - f1: 0.7381 - auc: 0.8839 - precision_8: 0.8248 - recall_8: 0.6716 - specificity: 0.7142 - specificity_at_sensitivity_8: 0.9343 - sensitivity_at_specificity_8: 0.9919 - accuracy: 0.7907 - val_loss: 0.5271 - val_f1: 0.8115 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6828 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6828\n",
            "Epoch 170/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4594 - f1: 0.7518 - auc: 0.8831 - precision_8: 0.8218 - recall_8: 0.6891 - specificity: 0.6910 - specificity_at_sensitivity_8: 0.9333 - sensitivity_at_specificity_8: 0.9933 - accuracy: 0.7955 - val_loss: 0.5290 - val_f1: 0.8115 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6828 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6828\n",
            "Epoch 171/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4599 - f1: 0.7401 - auc: 0.8819 - precision_8: 0.8278 - recall_8: 0.6729 - specificity: 0.7161 - specificity_at_sensitivity_8: 0.9376 - sensitivity_at_specificity_8: 0.9879 - accuracy: 0.7925 - val_loss: 0.5711 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 172/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4579 - f1: 0.7387 - auc: 0.8836 - precision_8: 0.8231 - recall_8: 0.6703 - specificity: 0.7094 - specificity_at_sensitivity_8: 0.9354 - sensitivity_at_specificity_8: 0.9906 - accuracy: 0.7895 - val_loss: 0.4997 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 173/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4572 - f1: 0.7537 - auc: 0.8865 - precision_8: 0.8193 - recall_8: 0.6958 - specificity: 0.6895 - specificity_at_sensitivity_8: 0.9333 - sensitivity_at_specificity_8: 0.9919 - accuracy: 0.7967 - val_loss: 0.5554 - val_f1: 0.8039 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6720 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6720\n",
            "Epoch 174/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4572 - f1: 0.7379 - auc: 0.8840 - precision_8: 0.8336 - recall_8: 0.6676 - specificity: 0.7272 - specificity_at_sensitivity_8: 0.9343 - sensitivity_at_specificity_8: 0.9865 - accuracy: 0.7931 - val_loss: 0.5691 - val_f1: 0.7961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6613 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6613\n",
            "Epoch 175/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4557 - f1: 0.7440 - auc: 0.8840 - precision_8: 0.8273 - recall_8: 0.6770 - specificity: 0.7100 - specificity_at_sensitivity_8: 0.9386 - sensitivity_at_specificity_8: 0.9933 - accuracy: 0.7937 - val_loss: 0.4883 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 176/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4564 - f1: 0.7531 - auc: 0.8851 - precision_8: 0.8155 - recall_8: 0.6958 - specificity: 0.6962 - specificity_at_sensitivity_8: 0.9300 - sensitivity_at_specificity_8: 0.9906 - accuracy: 0.7949 - val_loss: 0.5314 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 177/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4538 - f1: 0.7515 - auc: 0.8876 - precision_8: 0.8231 - recall_8: 0.6824 - specificity: 0.7121 - specificity_at_sensitivity_8: 0.9343 - sensitivity_at_specificity_8: 0.9919 - accuracy: 0.7937 - val_loss: 0.5451 - val_f1: 0.8077 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6774 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6774\n",
            "Epoch 178/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4533 - f1: 0.7422 - auc: 0.8881 - precision_8: 0.8252 - recall_8: 0.6797 - specificity: 0.7128 - specificity_at_sensitivity_8: 0.9376 - sensitivity_at_specificity_8: 0.9919 - accuracy: 0.7937 - val_loss: 0.5332 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 179/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4533 - f1: 0.7508 - auc: 0.8875 - precision_8: 0.8205 - recall_8: 0.6891 - specificity: 0.7044 - specificity_at_sensitivity_8: 0.9333 - sensitivity_at_specificity_8: 0.9919 - accuracy: 0.7949 - val_loss: 0.5405 - val_f1: 0.8077 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6774 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6774\n",
            "Epoch 180/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4520 - f1: 0.7454 - auc: 0.8893 - precision_8: 0.8265 - recall_8: 0.6797 - specificity: 0.7145 - specificity_at_sensitivity_8: 0.9376 - sensitivity_at_specificity_8: 0.9919 - accuracy: 0.7943 - val_loss: 0.5635 - val_f1: 0.8000 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6667 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6667\n",
            "Epoch 181/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4521 - f1: 0.7438 - auc: 0.8885 - precision_8: 0.8257 - recall_8: 0.6756 - specificity: 0.7203 - specificity_at_sensitivity_8: 0.9386 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.7925 - val_loss: 0.5250 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 182/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4508 - f1: 0.7500 - auc: 0.8899 - precision_8: 0.8221 - recall_8: 0.6904 - specificity: 0.7094 - specificity_at_sensitivity_8: 0.9354 - sensitivity_at_specificity_8: 0.9919 - accuracy: 0.7961 - val_loss: 0.5175 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 183/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4505 - f1: 0.7548 - auc: 0.8890 - precision_8: 0.8205 - recall_8: 0.6891 - specificity: 0.7140 - specificity_at_sensitivity_8: 0.9354 - sensitivity_at_specificity_8: 0.9919 - accuracy: 0.7949 - val_loss: 0.5296 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 184/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4496 - f1: 0.7459 - auc: 0.8901 - precision_8: 0.8239 - recall_8: 0.6864 - specificity: 0.7148 - specificity_at_sensitivity_8: 0.9365 - sensitivity_at_specificity_8: 0.9933 - accuracy: 0.7955 - val_loss: 0.5125 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 185/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4492 - f1: 0.7500 - auc: 0.8905 - precision_8: 0.8203 - recall_8: 0.6945 - specificity: 0.7069 - specificity_at_sensitivity_8: 0.9365 - sensitivity_at_specificity_8: 0.9919 - accuracy: 0.7967 - val_loss: 0.5163 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 186/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4484 - f1: 0.7492 - auc: 0.8908 - precision_8: 0.8192 - recall_8: 0.6891 - specificity: 0.7104 - specificity_at_sensitivity_8: 0.9343 - sensitivity_at_specificity_8: 0.9919 - accuracy: 0.7943 - val_loss: 0.5342 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 187/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4479 - f1: 0.7466 - auc: 0.8910 - precision_8: 0.8236 - recall_8: 0.6851 - specificity: 0.7152 - specificity_at_sensitivity_8: 0.9365 - sensitivity_at_specificity_8: 0.9933 - accuracy: 0.7949 - val_loss: 0.5257 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 188/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4472 - f1: 0.7454 - auc: 0.8920 - precision_8: 0.8229 - recall_8: 0.6878 - specificity: 0.7150 - specificity_at_sensitivity_8: 0.9354 - sensitivity_at_specificity_8: 0.9919 - accuracy: 0.7955 - val_loss: 0.5048 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 189/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4468 - f1: 0.7518 - auc: 0.8917 - precision_8: 0.8170 - recall_8: 0.6972 - specificity: 0.7075 - specificity_at_sensitivity_8: 0.9365 - sensitivity_at_specificity_8: 0.9919 - accuracy: 0.7961 - val_loss: 0.5142 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 190/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4460 - f1: 0.7575 - auc: 0.8930 - precision_8: 0.8201 - recall_8: 0.6931 - specificity: 0.7113 - specificity_at_sensitivity_8: 0.9354 - sensitivity_at_specificity_8: 0.9933 - accuracy: 0.7961 - val_loss: 0.5468 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 191/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4458 - f1: 0.7498 - auc: 0.8929 - precision_8: 0.8274 - recall_8: 0.6837 - specificity: 0.7257 - specificity_at_sensitivity_8: 0.9365 - sensitivity_at_specificity_8: 0.9919 - accuracy: 0.7961 - val_loss: 0.5359 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 192/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4466 - f1: 0.7489 - auc: 0.8901 - precision_8: 0.8201 - recall_8: 0.6931 - specificity: 0.7099 - specificity_at_sensitivity_8: 0.9376 - sensitivity_at_specificity_8: 0.9933 - accuracy: 0.7961 - val_loss: 0.5188 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 193/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4441 - f1: 0.7499 - auc: 0.8938 - precision_8: 0.8242 - recall_8: 0.6878 - specificity: 0.7219 - specificity_at_sensitivity_8: 0.9429 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.7961 - val_loss: 0.5558 - val_f1: 0.8039 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6720 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6720\n",
            "Epoch 194/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4441 - f1: 0.7468 - auc: 0.8938 - precision_8: 0.8295 - recall_8: 0.6810 - specificity: 0.7286 - specificity_at_sensitivity_8: 0.9397 - sensitivity_at_specificity_8: 0.9933 - accuracy: 0.7961 - val_loss: 0.4983 - val_f1: 0.8190 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6935 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6935\n",
            "Epoch 195/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4439 - f1: 0.7548 - auc: 0.8934 - precision_8: 0.8128 - recall_8: 0.7012 - specificity: 0.7013 - specificity_at_sensitivity_8: 0.9386 - sensitivity_at_specificity_8: 0.9919 - accuracy: 0.7955 - val_loss: 0.5082 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 196/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4421 - f1: 0.7586 - auc: 0.8945 - precision_8: 0.8214 - recall_8: 0.6931 - specificity: 0.7202 - specificity_at_sensitivity_8: 0.9419 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.7967 - val_loss: 0.5676 - val_f1: 0.8000 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6667 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6667\n",
            "Epoch 197/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4437 - f1: 0.7510 - auc: 0.8936 - precision_8: 0.8292 - recall_8: 0.6797 - specificity: 0.7313 - specificity_at_sensitivity_8: 0.9429 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.7955 - val_loss: 0.5193 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 198/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4426 - f1: 0.7533 - auc: 0.8925 - precision_8: 0.8155 - recall_8: 0.6958 - specificity: 0.7080 - specificity_at_sensitivity_8: 0.9365 - sensitivity_at_specificity_8: 0.9933 - accuracy: 0.7949 - val_loss: 0.5247 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 199/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4420 - f1: 0.7501 - auc: 0.8938 - precision_8: 0.8306 - recall_8: 0.6864 - specificity: 0.7325 - specificity_at_sensitivity_8: 0.9408 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.7984 - val_loss: 0.5411 - val_f1: 0.8115 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6828 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6828\n",
            "Epoch 200/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4400 - f1: 0.7550 - auc: 0.8957 - precision_8: 0.8240 - recall_8: 0.6931 - specificity: 0.7224 - specificity_at_sensitivity_8: 0.9408 - sensitivity_at_specificity_8: 0.9933 - accuracy: 0.7978 - val_loss: 0.4660 - val_f1: 0.8265 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7043 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7043\n",
            "Epoch 201/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4413 - f1: 0.7634 - auc: 0.8954 - precision_8: 0.8135 - recall_8: 0.7160 - specificity: 0.7041 - specificity_at_sensitivity_8: 0.9429 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.8008 - val_loss: 0.5138 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 202/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4392 - f1: 0.7517 - auc: 0.8961 - precision_8: 0.8166 - recall_8: 0.6891 - specificity: 0.7266 - specificity_at_sensitivity_8: 0.9365 - sensitivity_at_specificity_8: 0.9919 - accuracy: 0.7931 - val_loss: 0.5241 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 203/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4383 - f1: 0.7511 - auc: 0.8970 - precision_8: 0.8198 - recall_8: 0.6918 - specificity: 0.7240 - specificity_at_sensitivity_8: 0.9462 - sensitivity_at_specificity_8: 0.9933 - accuracy: 0.7955 - val_loss: 0.4883 - val_f1: 0.8190 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6935 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6935\n",
            "Epoch 204/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4382 - f1: 0.7553 - auc: 0.8972 - precision_8: 0.8169 - recall_8: 0.7026 - specificity: 0.7133 - specificity_at_sensitivity_8: 0.9451 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.7978 - val_loss: 0.5047 - val_f1: 0.8190 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6935 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6935\n",
            "Epoch 205/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4372 - f1: 0.7518 - auc: 0.8979 - precision_8: 0.8183 - recall_8: 0.6972 - specificity: 0.7190 - specificity_at_sensitivity_8: 0.9462 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.7967 - val_loss: 0.5162 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 206/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4367 - f1: 0.7522 - auc: 0.8979 - precision_8: 0.8177 - recall_8: 0.6945 - specificity: 0.7246 - specificity_at_sensitivity_8: 0.9451 - sensitivity_at_specificity_8: 0.9933 - accuracy: 0.7955 - val_loss: 0.5032 - val_f1: 0.8190 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6935 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6935\n",
            "Epoch 207/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4368 - f1: 0.7555 - auc: 0.8976 - precision_8: 0.8140 - recall_8: 0.7066 - specificity: 0.7103 - specificity_at_sensitivity_8: 0.9429 - sensitivity_at_specificity_8: 0.9933 - accuracy: 0.7978 - val_loss: 0.5106 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 208/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4355 - f1: 0.7463 - auc: 0.8985 - precision_8: 0.8227 - recall_8: 0.6931 - specificity: 0.7283 - specificity_at_sensitivity_8: 0.9440 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.7972 - val_loss: 0.5391 - val_f1: 0.8115 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6828 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6828\n",
            "Epoch 209/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4355 - f1: 0.7541 - auc: 0.8972 - precision_8: 0.8304 - recall_8: 0.6918 - specificity: 0.7308 - specificity_at_sensitivity_8: 0.9419 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8002 - val_loss: 0.4887 - val_f1: 0.8190 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6935 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6935\n",
            "Epoch 210/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4349 - f1: 0.7562 - auc: 0.8993 - precision_8: 0.8098 - recall_8: 0.7106 - specificity: 0.7116 - specificity_at_sensitivity_8: 0.9419 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.7972 - val_loss: 0.5171 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 211/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4351 - f1: 0.7544 - auc: 0.8974 - precision_8: 0.8261 - recall_8: 0.6904 - specificity: 0.7391 - specificity_at_sensitivity_8: 0.9451 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.7978 - val_loss: 0.5242 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 212/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4332 - f1: 0.7586 - auc: 0.9002 - precision_8: 0.8156 - recall_8: 0.7026 - specificity: 0.7259 - specificity_at_sensitivity_8: 0.9483 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.7972 - val_loss: 0.4591 - val_f1: 0.8265 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7043 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7043\n",
            "Epoch 213/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4339 - f1: 0.7656 - auc: 0.9004 - precision_8: 0.8126 - recall_8: 0.7295 - specificity: 0.7069 - specificity_at_sensitivity_8: 0.9419 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.8050 - val_loss: 0.5237 - val_f1: 0.8115 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6828 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6828\n",
            "Epoch 214/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4325 - f1: 0.7461 - auc: 0.9006 - precision_8: 0.8347 - recall_8: 0.6931 - specificity: 0.7349 - specificity_at_sensitivity_8: 0.9419 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.8026 - val_loss: 0.5227 - val_f1: 0.8115 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6828 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6828\n",
            "Epoch 215/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4315 - f1: 0.7567 - auc: 0.9011 - precision_8: 0.8256 - recall_8: 0.6945 - specificity: 0.7311 - specificity_at_sensitivity_8: 0.9473 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.7990 - val_loss: 0.4627 - val_f1: 0.8265 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7043 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7043\n",
            "Epoch 216/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4321 - f1: 0.7712 - auc: 0.9010 - precision_8: 0.8111 - recall_8: 0.7281 - specificity: 0.7101 - specificity_at_sensitivity_8: 0.9451 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.8038 - val_loss: 0.4911 - val_f1: 0.8190 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6935 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6935\n",
            "Epoch 217/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4308 - f1: 0.7631 - auc: 0.9010 - precision_8: 0.8200 - recall_8: 0.7052 - specificity: 0.7307 - specificity_at_sensitivity_8: 0.9440 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8002 - val_loss: 0.5229 - val_f1: 0.8115 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6828 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6828\n",
            "Epoch 218/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4305 - f1: 0.7603 - auc: 0.9003 - precision_8: 0.8246 - recall_8: 0.7026 - specificity: 0.7281 - specificity_at_sensitivity_8: 0.9462 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8014 - val_loss: 0.4859 - val_f1: 0.8190 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6935 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6935\n",
            "Epoch 219/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4296 - f1: 0.7549 - auc: 0.9019 - precision_8: 0.8101 - recall_8: 0.7120 - specificity: 0.7175 - specificity_at_sensitivity_8: 0.9462 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.7978 - val_loss: 0.5267 - val_f1: 0.8115 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6828 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6828\n",
            "Epoch 220/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4293 - f1: 0.7490 - auc: 0.9024 - precision_8: 0.8306 - recall_8: 0.6931 - specificity: 0.7391 - specificity_at_sensitivity_8: 0.9462 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.8008 - val_loss: 0.5247 - val_f1: 0.8115 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6828 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6828\n",
            "Epoch 221/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4287 - f1: 0.7582 - auc: 0.9017 - precision_8: 0.8254 - recall_8: 0.6999 - specificity: 0.7304 - specificity_at_sensitivity_8: 0.9429 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.8008 - val_loss: 0.4763 - val_f1: 0.8265 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7043 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7043\n",
            "Epoch 222/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4280 - f1: 0.7637 - auc: 0.9037 - precision_8: 0.8106 - recall_8: 0.7201 - specificity: 0.7136 - specificity_at_sensitivity_8: 0.9505 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.8008 - val_loss: 0.5115 - val_f1: 0.8190 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6935 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6935\n",
            "Epoch 223/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4272 - f1: 0.7543 - auc: 0.9034 - precision_8: 0.8301 - recall_8: 0.6972 - specificity: 0.7323 - specificity_at_sensitivity_8: 0.9473 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.8020 - val_loss: 0.5244 - val_f1: 0.8115 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6828 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6828\n",
            "Epoch 224/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4269 - f1: 0.7608 - auc: 0.9037 - precision_8: 0.8296 - recall_8: 0.7012 - specificity: 0.7374 - specificity_at_sensitivity_8: 0.9494 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8032 - val_loss: 0.4800 - val_f1: 0.8265 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7043 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7043\n",
            "Epoch 225/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4271 - f1: 0.7692 - auc: 0.9027 - precision_8: 0.8105 - recall_8: 0.7254 - specificity: 0.7139 - specificity_at_sensitivity_8: 0.9494 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.8026 - val_loss: 0.5038 - val_f1: 0.8190 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6935 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6935\n",
            "Epoch 226/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4253 - f1: 0.7614 - auc: 0.9049 - precision_8: 0.8323 - recall_8: 0.7012 - specificity: 0.7342 - specificity_at_sensitivity_8: 0.9473 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.8044 - val_loss: 0.5443 - val_f1: 0.8115 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6828 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6828\n",
            "Epoch 227/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4265 - f1: 0.7548 - auc: 0.9026 - precision_8: 0.8350 - recall_8: 0.6945 - specificity: 0.7398 - specificity_at_sensitivity_8: 0.9494 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.8032 - val_loss: 0.4782 - val_f1: 0.8265 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7043 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7043\n",
            "Epoch 228/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4246 - f1: 0.7664 - auc: 0.9052 - precision_8: 0.8121 - recall_8: 0.7214 - specificity: 0.7169 - specificity_at_sensitivity_8: 0.9505 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.8020 - val_loss: 0.4888 - val_f1: 0.8228 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6989 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6989\n",
            "Epoch 229/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4237 - f1: 0.7625 - auc: 0.9054 - precision_8: 0.8266 - recall_8: 0.7120 - specificity: 0.7303 - specificity_at_sensitivity_8: 0.9483 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8056 - val_loss: 0.5125 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 230/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4236 - f1: 0.7629 - auc: 0.9062 - precision_8: 0.8304 - recall_8: 0.6985 - specificity: 0.7426 - specificity_at_sensitivity_8: 0.9494 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8026 - val_loss: 0.4694 - val_f1: 0.8302 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7097 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7097\n",
            "Epoch 231/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4231 - f1: 0.7768 - auc: 0.9067 - precision_8: 0.8137 - recall_8: 0.7349 - specificity: 0.7217 - specificity_at_sensitivity_8: 0.9483 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.8074 - val_loss: 0.4701 - val_f1: 0.8302 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7097 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7097\n",
            "Epoch 232/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4223 - f1: 0.7689 - auc: 0.9063 - precision_8: 0.8190 - recall_8: 0.7187 - specificity: 0.7278 - specificity_at_sensitivity_8: 0.9537 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.8044 - val_loss: 0.5172 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 233/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4221 - f1: 0.7590 - auc: 0.9068 - precision_8: 0.8283 - recall_8: 0.7012 - specificity: 0.7421 - specificity_at_sensitivity_8: 0.9494 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8026 - val_loss: 0.4813 - val_f1: 0.8265 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7043 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7043\n",
            "Epoch 234/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4211 - f1: 0.7593 - auc: 0.9071 - precision_8: 0.8243 - recall_8: 0.7201 - specificity: 0.7279 - specificity_at_sensitivity_8: 0.9505 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.8074 - val_loss: 0.4708 - val_f1: 0.8302 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7097 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7097\n",
            "Epoch 235/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4208 - f1: 0.7730 - auc: 0.9068 - precision_8: 0.8229 - recall_8: 0.7254 - specificity: 0.7287 - specificity_at_sensitivity_8: 0.9505 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8086 - val_loss: 0.4873 - val_f1: 0.8228 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6989 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6989\n",
            "Epoch 236/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4200 - f1: 0.7677 - auc: 0.9079 - precision_8: 0.8251 - recall_8: 0.7174 - specificity: 0.7302 - specificity_at_sensitivity_8: 0.9548 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8068 - val_loss: 0.4871 - val_f1: 0.8228 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6989 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6989\n",
            "Epoch 237/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4195 - f1: 0.7661 - auc: 0.9080 - precision_8: 0.8264 - recall_8: 0.7174 - specificity: 0.7317 - specificity_at_sensitivity_8: 0.9494 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8074 - val_loss: 0.4965 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 238/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4190 - f1: 0.7559 - auc: 0.9087 - precision_8: 0.8270 - recall_8: 0.7079 - specificity: 0.7394 - specificity_at_sensitivity_8: 0.9505 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8044 - val_loss: 0.4825 - val_f1: 0.8265 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7043 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7043\n",
            "Epoch 239/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4183 - f1: 0.7696 - auc: 0.9089 - precision_8: 0.8259 - recall_8: 0.7214 - specificity: 0.7315 - specificity_at_sensitivity_8: 0.9505 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8086 - val_loss: 0.4706 - val_f1: 0.8302 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7097 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7097\n",
            "Epoch 240/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4179 - f1: 0.7766 - auc: 0.9088 - precision_8: 0.8267 - recall_8: 0.7322 - specificity: 0.7313 - specificity_at_sensitivity_8: 0.9526 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8128 - val_loss: 0.4873 - val_f1: 0.8190 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6935 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6935\n",
            "Epoch 241/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4173 - f1: 0.7684 - auc: 0.9092 - precision_8: 0.8256 - recall_8: 0.7201 - specificity: 0.7348 - specificity_at_sensitivity_8: 0.9505 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.8080 - val_loss: 0.4876 - val_f1: 0.8190 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6935 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6935\n",
            "Epoch 242/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4167 - f1: 0.7643 - auc: 0.9099 - precision_8: 0.8261 - recall_8: 0.7160 - specificity: 0.7372 - specificity_at_sensitivity_8: 0.9505 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8068 - val_loss: 0.4801 - val_f1: 0.8265 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7043 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7043\n",
            "Epoch 243/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4165 - f1: 0.7718 - auc: 0.9090 - precision_8: 0.8250 - recall_8: 0.7295 - specificity: 0.7285 - specificity_at_sensitivity_8: 0.9537 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8110 - val_loss: 0.4888 - val_f1: 0.8190 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6935 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6935\n",
            "Epoch 244/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4158 - f1: 0.7625 - auc: 0.9106 - precision_8: 0.8281 - recall_8: 0.7133 - specificity: 0.7411 - specificity_at_sensitivity_8: 0.9559 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8068 - val_loss: 0.4902 - val_f1: 0.8190 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6935 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6935\n",
            "Epoch 245/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4176 - f1: 0.7743 - auc: 0.9064 - precision_8: 0.8175 - recall_8: 0.7295 - specificity: 0.7255 - specificity_at_sensitivity_8: 0.9559 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.8074 - val_loss: 0.4851 - val_f1: 0.8228 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6989 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6989\n",
            "Epoch 246/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4146 - f1: 0.7648 - auc: 0.9105 - precision_8: 0.8266 - recall_8: 0.7120 - specificity: 0.7483 - specificity_at_sensitivity_8: 0.9537 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8056 - val_loss: 0.5208 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 247/1000\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.4145 - f1: 0.7747 - auc: 0.9107 - precision_8: 0.8325 - recall_8: 0.7093 - specificity: 0.7504 - specificity_at_sensitivity_8: 0.9516 - sensitivity_at_specificity_8: 0.9946 - accuracy: 0.8074 - val_loss: 0.4344 - val_f1: 0.8554 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7473 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7473\n",
            "Epoch 248/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4148 - f1: 0.7884 - auc: 0.9114 - precision_8: 0.8198 - recall_8: 0.7591 - specificity: 0.7172 - specificity_at_sensitivity_8: 0.9548 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8188 - val_loss: 0.4870 - val_f1: 0.8190 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6935 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6935\n",
            "Epoch 249/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4138 - f1: 0.7649 - auc: 0.9106 - precision_8: 0.8292 - recall_8: 0.7120 - specificity: 0.7480 - specificity_at_sensitivity_8: 0.9537 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8068 - val_loss: 0.4965 - val_f1: 0.8190 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6935 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6935\n",
            "Epoch 250/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4135 - f1: 0.7709 - auc: 0.9094 - precision_8: 0.8221 - recall_8: 0.7214 - specificity: 0.7326 - specificity_at_sensitivity_8: 0.9537 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8068 - val_loss: 0.4427 - val_f1: 0.8483 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7366 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7366\n",
            "Epoch 251/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4120 - f1: 0.7863 - auc: 0.9122 - precision_8: 0.8269 - recall_8: 0.7524 - specificity: 0.7252 - specificity_at_sensitivity_8: 0.9591 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8200 - val_loss: 0.5348 - val_f1: 0.8115 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6828 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6828\n",
            "Epoch 252/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4141 - f1: 0.7552 - auc: 0.9121 - precision_8: 0.8328 - recall_8: 0.6904 - specificity: 0.7641 - specificity_at_sensitivity_8: 0.9537 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8008 - val_loss: 0.4699 - val_f1: 0.8339 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7151 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7151\n",
            "Epoch 253/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4105 - f1: 0.7861 - auc: 0.9114 - precision_8: 0.8257 - recall_8: 0.7524 - specificity: 0.7276 - specificity_at_sensitivity_8: 0.9526 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8194 - val_loss: 0.4169 - val_f1: 0.8659 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7634 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7634\n",
            "Epoch 254/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4117 - f1: 0.7906 - auc: 0.9114 - precision_8: 0.8220 - recall_8: 0.7645 - specificity: 0.7215 - specificity_at_sensitivity_8: 0.9516 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8218 - val_loss: 0.5241 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 255/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4123 - f1: 0.7576 - auc: 0.9123 - precision_8: 0.8339 - recall_8: 0.6958 - specificity: 0.7643 - specificity_at_sensitivity_8: 0.9591 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8032 - val_loss: 0.4662 - val_f1: 0.8339 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7151 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7151\n",
            "Epoch 256/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4102 - f1: 0.7944 - auc: 0.9115 - precision_8: 0.8248 - recall_8: 0.7604 - specificity: 0.7225 - specificity_at_sensitivity_8: 0.9537 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8218 - val_loss: 0.4258 - val_f1: 0.8589 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7527 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7527\n",
            "Epoch 257/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4101 - f1: 0.7814 - auc: 0.9101 - precision_8: 0.8256 - recall_8: 0.7456 - specificity: 0.7356 - specificity_at_sensitivity_8: 0.9483 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8170 - val_loss: 0.5215 - val_f1: 0.8153 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6882 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6882\n",
            "Epoch 258/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4105 - f1: 0.7747 - auc: 0.9104 - precision_8: 0.8315 - recall_8: 0.7174 - specificity: 0.7464 - specificity_at_sensitivity_8: 0.9516 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8098 - val_loss: 0.4573 - val_f1: 0.8375 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7204 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7204\n",
            "Epoch 259/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4077 - f1: 0.7842 - auc: 0.9137 - precision_8: 0.8263 - recall_8: 0.7429 - specificity: 0.7326 - specificity_at_sensitivity_8: 0.9612 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8164 - val_loss: 0.4797 - val_f1: 0.8302 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7097 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7097\n",
            "Epoch 260/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4073 - f1: 0.7780 - auc: 0.9137 - precision_8: 0.8285 - recall_8: 0.7281 - specificity: 0.7469 - specificity_at_sensitivity_8: 0.9580 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8122 - val_loss: 0.4725 - val_f1: 0.8339 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7151 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7151\n",
            "Epoch 261/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4069 - f1: 0.7845 - auc: 0.9134 - precision_8: 0.8251 - recall_8: 0.7429 - specificity: 0.7359 - specificity_at_sensitivity_8: 0.9548 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8158 - val_loss: 0.4530 - val_f1: 0.8375 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7204 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7204\n",
            "Epoch 262/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4060 - f1: 0.7835 - auc: 0.9142 - precision_8: 0.8266 - recall_8: 0.7443 - specificity: 0.7389 - specificity_at_sensitivity_8: 0.9634 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8170 - val_loss: 0.4915 - val_f1: 0.8190 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6935 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6935\n",
            "Epoch 263/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4063 - f1: 0.7767 - auc: 0.9132 - precision_8: 0.8277 - recall_8: 0.7308 - specificity: 0.7436 - specificity_at_sensitivity_8: 0.9580 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8128 - val_loss: 0.4692 - val_f1: 0.8339 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7151 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7151\n",
            "Epoch 264/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4055 - f1: 0.7762 - auc: 0.9141 - precision_8: 0.8278 - recall_8: 0.7376 - specificity: 0.7451 - specificity_at_sensitivity_8: 0.9645 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8152 - val_loss: 0.4501 - val_f1: 0.8447 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7312 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7312\n",
            "Epoch 265/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4047 - f1: 0.7894 - auc: 0.9150 - precision_8: 0.8276 - recall_8: 0.7497 - specificity: 0.7334 - specificity_at_sensitivity_8: 0.9634 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8194 - val_loss: 0.4466 - val_f1: 0.8447 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7312 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7312\n",
            "Epoch 266/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4040 - f1: 0.7860 - auc: 0.9153 - precision_8: 0.8262 - recall_8: 0.7483 - specificity: 0.7378 - specificity_at_sensitivity_8: 0.9580 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8182 - val_loss: 0.4924 - val_f1: 0.8228 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.6989 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.6989\n",
            "Epoch 267/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4046 - f1: 0.7781 - auc: 0.9140 - precision_8: 0.8275 - recall_8: 0.7295 - specificity: 0.7473 - specificity_at_sensitivity_8: 0.9559 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8122 - val_loss: 0.4696 - val_f1: 0.8375 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7204 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7204\n",
            "Epoch 268/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4030 - f1: 0.7830 - auc: 0.9155 - precision_8: 0.8273 - recall_8: 0.7416 - specificity: 0.7459 - specificity_at_sensitivity_8: 0.9612 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8164 - val_loss: 0.4641 - val_f1: 0.8375 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7204 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7204\n",
            "Epoch 269/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4031 - f1: 0.7891 - auc: 0.9150 - precision_8: 0.8271 - recall_8: 0.7470 - specificity: 0.7377 - specificity_at_sensitivity_8: 0.9612 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8182 - val_loss: 0.4736 - val_f1: 0.8339 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7151 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7151\n",
            "Epoch 270/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4020 - f1: 0.7814 - auc: 0.9157 - precision_8: 0.8278 - recall_8: 0.7376 - specificity: 0.7494 - specificity_at_sensitivity_8: 0.9656 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8152 - val_loss: 0.4817 - val_f1: 0.8339 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7151 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7151\n",
            "Epoch 271/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4015 - f1: 0.7792 - auc: 0.9158 - precision_8: 0.8260 - recall_8: 0.7349 - specificity: 0.7466 - specificity_at_sensitivity_8: 0.9634 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8134 - val_loss: 0.4325 - val_f1: 0.8589 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7527 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7527\n",
            "Epoch 272/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4014 - f1: 0.7891 - auc: 0.9164 - precision_8: 0.8258 - recall_8: 0.7591 - specificity: 0.7326 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8218 - val_loss: 0.4610 - val_f1: 0.8411 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7258 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7258\n",
            "Epoch 273/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4003 - f1: 0.7839 - auc: 0.9164 - precision_8: 0.8281 - recall_8: 0.7456 - specificity: 0.7450 - specificity_at_sensitivity_8: 0.9656 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8182 - val_loss: 0.4777 - val_f1: 0.8339 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7151 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7151\n",
            "Epoch 274/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4001 - f1: 0.7823 - auc: 0.9165 - precision_8: 0.8278 - recall_8: 0.7376 - specificity: 0.7534 - specificity_at_sensitivity_8: 0.9656 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8152 - val_loss: 0.4368 - val_f1: 0.8554 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7473 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7473\n",
            "Epoch 275/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3998 - f1: 0.7895 - auc: 0.9166 - precision_8: 0.8292 - recall_8: 0.7577 - specificity: 0.7411 - specificity_at_sensitivity_8: 0.9666 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8230 - val_loss: 0.4355 - val_f1: 0.8554 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7473 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7473\n",
            "Epoch 276/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3990 - f1: 0.7897 - auc: 0.9173 - precision_8: 0.8277 - recall_8: 0.7564 - specificity: 0.7365 - specificity_at_sensitivity_8: 0.9656 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8218 - val_loss: 0.4660 - val_f1: 0.8375 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7204 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7204\n",
            "Epoch 277/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3987 - f1: 0.7808 - auc: 0.9171 - precision_8: 0.8283 - recall_8: 0.7402 - specificity: 0.7537 - specificity_at_sensitivity_8: 0.9677 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8164 - val_loss: 0.4421 - val_f1: 0.8519 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7419 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7419\n",
            "Epoch 278/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3980 - f1: 0.7929 - auc: 0.9174 - precision_8: 0.8287 - recall_8: 0.7618 - specificity: 0.7353 - specificity_at_sensitivity_8: 0.9623 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8242 - val_loss: 0.4393 - val_f1: 0.8519 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7419 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7419\n",
            "Epoch 279/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3972 - f1: 0.7893 - auc: 0.9173 - precision_8: 0.8281 - recall_8: 0.7524 - specificity: 0.7443 - specificity_at_sensitivity_8: 0.9612 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8206 - val_loss: 0.4816 - val_f1: 0.8339 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7151 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7151\n",
            "Epoch 280/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3971 - f1: 0.7829 - auc: 0.9179 - precision_8: 0.8333 - recall_8: 0.7402 - specificity: 0.7540 - specificity_at_sensitivity_8: 0.9623 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8188 - val_loss: 0.4320 - val_f1: 0.8589 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7527 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7527\n",
            "Epoch 281/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3989 - f1: 0.8112 - auc: 0.9172 - precision_8: 0.8274 - recall_8: 0.7806 - specificity: 0.7268 - specificity_at_sensitivity_8: 0.9623 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8301 - val_loss: 0.4870 - val_f1: 0.8339 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7151 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7151\n",
            "Epoch 282/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4000 - f1: 0.7815 - auc: 0.9151 - precision_8: 0.8433 - recall_8: 0.7241 - specificity: 0.7770 - specificity_at_sensitivity_8: 0.9516 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8176 - val_loss: 0.4622 - val_f1: 0.8447 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7312 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7312\n",
            "Epoch 283/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3950 - f1: 0.8026 - auc: 0.9182 - precision_8: 0.8280 - recall_8: 0.7712 - specificity: 0.7389 - specificity_at_sensitivity_8: 0.9623 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8272 - val_loss: 0.3753 - val_f1: 0.8829 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7903 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7903\n",
            "Epoch 284/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4024 - f1: 0.7991 - auc: 0.9104 - precision_8: 0.8204 - recall_8: 0.7806 - specificity: 0.7353 - specificity_at_sensitivity_8: 0.9494 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8266 - val_loss: 0.4828 - val_f1: 0.8339 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7151 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7151\n",
            "Epoch 285/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3945 - f1: 0.7851 - auc: 0.9181 - precision_8: 0.8348 - recall_8: 0.7416 - specificity: 0.7565 - specificity_at_sensitivity_8: 0.9645 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8200 - val_loss: 0.4062 - val_f1: 0.8727 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7742 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7742\n",
            "Epoch 286/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3953 - f1: 0.8151 - auc: 0.9194 - precision_8: 0.8292 - recall_8: 0.8035 - specificity: 0.7265 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8391 - val_loss: 0.4634 - val_f1: 0.8447 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7312 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7312\n",
            "Epoch 287/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3942 - f1: 0.7796 - auc: 0.9182 - precision_8: 0.8321 - recall_8: 0.7335 - specificity: 0.7629 - specificity_at_sensitivity_8: 0.9656 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8158 - val_loss: 0.4714 - val_f1: 0.8411 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7258 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7258\n",
            "Epoch 288/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3935 - f1: 0.7983 - auc: 0.9179 - precision_8: 0.8297 - recall_8: 0.7604 - specificity: 0.7483 - specificity_at_sensitivity_8: 0.9623 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8242 - val_loss: 0.4056 - val_f1: 0.8727 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7742 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7742\n",
            "Epoch 289/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3935 - f1: 0.8089 - auc: 0.9188 - precision_8: 0.8338 - recall_8: 0.7900 - specificity: 0.7396 - specificity_at_sensitivity_8: 0.9656 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8367 - val_loss: 0.4756 - val_f1: 0.8375 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7204 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7204\n",
            "Epoch 290/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3923 - f1: 0.7813 - auc: 0.9198 - precision_8: 0.8316 - recall_8: 0.7376 - specificity: 0.7634 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8170 - val_loss: 0.4269 - val_f1: 0.8659 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7634 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7634\n",
            "Epoch 291/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3922 - f1: 0.8127 - auc: 0.9194 - precision_8: 0.8336 - recall_8: 0.7954 - specificity: 0.7350 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8385 - val_loss: 0.4458 - val_f1: 0.8519 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7419 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7419\n",
            "Epoch 292/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3907 - f1: 0.7881 - auc: 0.9202 - precision_8: 0.8326 - recall_8: 0.7497 - specificity: 0.7550 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8218 - val_loss: 0.4838 - val_f1: 0.8375 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7204 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7204\n",
            "Epoch 293/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3912 - f1: 0.7912 - auc: 0.9198 - precision_8: 0.8361 - recall_8: 0.7483 - specificity: 0.7606 - specificity_at_sensitivity_8: 0.9677 - sensitivity_at_specificity_8: 0.9960 - accuracy: 0.8230 - val_loss: 0.4144 - val_f1: 0.8693 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7688 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7688\n",
            "Epoch 294/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3905 - f1: 0.8099 - auc: 0.9205 - precision_8: 0.8310 - recall_8: 0.7873 - specificity: 0.7397 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8343 - val_loss: 0.4378 - val_f1: 0.8554 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7473 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7473\n",
            "Epoch 295/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3893 - f1: 0.7940 - auc: 0.9208 - precision_8: 0.8328 - recall_8: 0.7645 - specificity: 0.7507 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8272 - val_loss: 0.4568 - val_f1: 0.8519 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7419 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7419\n",
            "Epoch 296/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3891 - f1: 0.7933 - auc: 0.9211 - precision_8: 0.8323 - recall_8: 0.7483 - specificity: 0.7596 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8212 - val_loss: 0.4284 - val_f1: 0.8659 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7634 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7634\n",
            "Epoch 297/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3886 - f1: 0.8072 - auc: 0.9213 - precision_8: 0.8333 - recall_8: 0.7873 - specificity: 0.7439 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8355 - val_loss: 0.4342 - val_f1: 0.8589 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7527 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7527\n",
            "Epoch 298/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3880 - f1: 0.7956 - auc: 0.9210 - precision_8: 0.8343 - recall_8: 0.7658 - specificity: 0.7553 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8283 - val_loss: 0.4555 - val_f1: 0.8519 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7419 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7419\n",
            "Epoch 299/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3875 - f1: 0.7951 - auc: 0.9214 - precision_8: 0.8356 - recall_8: 0.7591 - specificity: 0.7553 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8266 - val_loss: 0.4208 - val_f1: 0.8693 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7688 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7688\n",
            "Epoch 300/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3873 - f1: 0.8044 - auc: 0.9214 - precision_8: 0.8329 - recall_8: 0.7779 - specificity: 0.7509 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8319 - val_loss: 0.4359 - val_f1: 0.8589 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7527 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7527\n",
            "Epoch 301/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3867 - f1: 0.8051 - auc: 0.9219 - precision_8: 0.8345 - recall_8: 0.7806 - specificity: 0.7488 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8337 - val_loss: 0.4448 - val_f1: 0.8554 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7473 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7473\n",
            "Epoch 302/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3868 - f1: 0.7905 - auc: 0.9213 - precision_8: 0.8376 - recall_8: 0.7497 - specificity: 0.7628 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8242 - val_loss: 0.4282 - val_f1: 0.8659 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7634 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7634\n",
            "Epoch 303/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3858 - f1: 0.8108 - auc: 0.9222 - precision_8: 0.8331 - recall_8: 0.7927 - specificity: 0.7422 - specificity_at_sensitivity_8: 0.9666 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8373 - val_loss: 0.4157 - val_f1: 0.8693 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7688 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7688\n",
            "Epoch 304/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3849 - f1: 0.8077 - auc: 0.9225 - precision_8: 0.8374 - recall_8: 0.7833 - specificity: 0.7507 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8361 - val_loss: 0.4819 - val_f1: 0.8411 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7258 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7258\n",
            "Epoch 305/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3858 - f1: 0.7895 - auc: 0.9225 - precision_8: 0.8501 - recall_8: 0.7402 - specificity: 0.7719 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8266 - val_loss: 0.4062 - val_f1: 0.8727 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7742 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7742\n",
            "Epoch 306/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3847 - f1: 0.8134 - auc: 0.9231 - precision_8: 0.8319 - recall_8: 0.7995 - specificity: 0.7430 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8391 - val_loss: 0.4098 - val_f1: 0.8693 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7688 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7688\n",
            "Epoch 307/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3839 - f1: 0.8092 - auc: 0.9229 - precision_8: 0.8333 - recall_8: 0.7873 - specificity: 0.7535 - specificity_at_sensitivity_8: 0.9666 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8355 - val_loss: 0.4560 - val_f1: 0.8519 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7419 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7419\n",
            "Epoch 308/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3833 - f1: 0.7930 - auc: 0.9234 - precision_8: 0.8381 - recall_8: 0.7524 - specificity: 0.7643 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8254 - val_loss: 0.4051 - val_f1: 0.8727 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7742 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7742\n",
            "Epoch 309/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3849 - f1: 0.8108 - auc: 0.9228 - precision_8: 0.8237 - recall_8: 0.8048 - specificity: 0.7308 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8367 - val_loss: 0.4630 - val_f1: 0.8519 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7419 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7419\n",
            "Epoch 310/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3842 - f1: 0.7873 - auc: 0.9233 - precision_8: 0.8531 - recall_8: 0.7349 - specificity: 0.7786 - specificity_at_sensitivity_8: 0.9666 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8260 - val_loss: 0.4529 - val_f1: 0.8554 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7473 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7473\n",
            "Epoch 311/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3820 - f1: 0.8105 - auc: 0.9227 - precision_8: 0.8409 - recall_8: 0.7752 - specificity: 0.7510 - specificity_at_sensitivity_8: 0.9656 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8349 - val_loss: 0.3749 - val_f1: 0.8795 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7849 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7849\n",
            "Epoch 312/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3835 - f1: 0.8139 - auc: 0.9228 - precision_8: 0.8294 - recall_8: 0.7981 - specificity: 0.7439 - specificity_at_sensitivity_8: 0.9569 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8373 - val_loss: 0.4771 - val_f1: 0.8483 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7366 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7366\n",
            "Epoch 313/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3822 - f1: 0.7982 - auc: 0.9246 - precision_8: 0.8465 - recall_8: 0.7497 - specificity: 0.7713 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8283 - val_loss: 0.4110 - val_f1: 0.8693 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7688 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7688\n",
            "Epoch 314/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3805 - f1: 0.8187 - auc: 0.9250 - precision_8: 0.8355 - recall_8: 0.7927 - specificity: 0.7530 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8385 - val_loss: 0.4185 - val_f1: 0.8693 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7688 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7688\n",
            "Epoch 315/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3798 - f1: 0.8126 - auc: 0.9255 - precision_8: 0.8398 - recall_8: 0.7900 - specificity: 0.7561 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8397 - val_loss: 0.4299 - val_f1: 0.8659 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7634 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7634\n",
            "Epoch 316/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3795 - f1: 0.8148 - auc: 0.9255 - precision_8: 0.8410 - recall_8: 0.7900 - specificity: 0.7541 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8403 - val_loss: 0.4291 - val_f1: 0.8659 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7634 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7634\n",
            "Epoch 317/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3788 - f1: 0.8142 - auc: 0.9257 - precision_8: 0.8476 - recall_8: 0.7860 - specificity: 0.7615 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8421 - val_loss: 0.4320 - val_f1: 0.8659 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7634 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7634\n",
            "Epoch 318/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3783 - f1: 0.8147 - auc: 0.9260 - precision_8: 0.8478 - recall_8: 0.7873 - specificity: 0.7587 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8427 - val_loss: 0.4010 - val_f1: 0.8761 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7796 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7796\n",
            "Epoch 319/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3783 - f1: 0.8211 - auc: 0.9263 - precision_8: 0.8333 - recall_8: 0.8008 - specificity: 0.7520 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8403 - val_loss: 0.4295 - val_f1: 0.8659 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7634 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7634\n",
            "Epoch 320/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3774 - f1: 0.8183 - auc: 0.9265 - precision_8: 0.8468 - recall_8: 0.7887 - specificity: 0.7613 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8427 - val_loss: 0.4379 - val_f1: 0.8624 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7581 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7581\n",
            "Epoch 321/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3772 - f1: 0.8150 - auc: 0.9265 - precision_8: 0.8464 - recall_8: 0.7860 - specificity: 0.7606 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8415 - val_loss: 0.4245 - val_f1: 0.8659 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7634 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7634\n",
            "Epoch 322/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3778 - f1: 0.8241 - auc: 0.9257 - precision_8: 0.8418 - recall_8: 0.8022 - specificity: 0.7518 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8451 - val_loss: 0.4654 - val_f1: 0.8519 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7419 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7419\n",
            "Epoch 323/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3780 - f1: 0.7922 - auc: 0.9267 - precision_8: 0.8514 - recall_8: 0.7402 - specificity: 0.7793 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8272 - val_loss: 0.4138 - val_f1: 0.8727 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7742 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7742\n",
            "Epoch 324/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3754 - f1: 0.8210 - auc: 0.9274 - precision_8: 0.8408 - recall_8: 0.8035 - specificity: 0.7478 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8451 - val_loss: 0.3765 - val_f1: 0.8795 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7849 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7849\n",
            "Epoch 325/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3761 - f1: 0.8169 - auc: 0.9269 - precision_8: 0.8273 - recall_8: 0.8062 - specificity: 0.7502 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8391 - val_loss: 0.4670 - val_f1: 0.8519 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7419 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7419\n",
            "Epoch 326/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3762 - f1: 0.8047 - auc: 0.9262 - precision_8: 0.8498 - recall_8: 0.7618 - specificity: 0.7726 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8343 - val_loss: 0.4090 - val_f1: 0.8761 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7796 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7796\n",
            "Epoch 327/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3763 - f1: 0.8194 - auc: 0.9263 - precision_8: 0.8383 - recall_8: 0.8022 - specificity: 0.7454 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8433 - val_loss: 0.4551 - val_f1: 0.8554 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7473 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7473\n",
            "Epoch 328/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3770 - f1: 0.7931 - auc: 0.9267 - precision_8: 0.8534 - recall_8: 0.7443 - specificity: 0.7891 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8295 - val_loss: 0.4163 - val_f1: 0.8727 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7742 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7742\n",
            "Epoch 329/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3742 - f1: 0.8257 - auc: 0.9268 - precision_8: 0.8380 - recall_8: 0.8075 - specificity: 0.7440 - specificity_at_sensitivity_8: 0.9645 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8451 - val_loss: 0.3701 - val_f1: 0.8829 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7903 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7903\n",
            "Epoch 330/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3751 - f1: 0.8086 - auc: 0.9251 - precision_8: 0.8291 - recall_8: 0.7968 - specificity: 0.7553 - specificity_at_sensitivity_8: 0.9634 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8367 - val_loss: 0.4765 - val_f1: 0.8483 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7366 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7366\n",
            "Epoch 331/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3747 - f1: 0.8079 - auc: 0.9260 - precision_8: 0.8529 - recall_8: 0.7645 - specificity: 0.7699 - specificity_at_sensitivity_8: 0.9634 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8367 - val_loss: 0.3784 - val_f1: 0.8795 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7849 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7849\n",
            "Epoch 332/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3738 - f1: 0.8241 - auc: 0.9288 - precision_8: 0.8290 - recall_8: 0.8223 - specificity: 0.7394 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8457 - val_loss: 0.4703 - val_f1: 0.8519 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7419 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7419\n",
            "Epoch 333/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3748 - f1: 0.7895 - auc: 0.9282 - precision_8: 0.8523 - recall_8: 0.7376 - specificity: 0.7900 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8266 - val_loss: 0.4232 - val_f1: 0.8659 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7634 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7634\n",
            "Epoch 334/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3720 - f1: 0.8174 - auc: 0.9274 - precision_8: 0.8383 - recall_8: 0.8022 - specificity: 0.7509 - specificity_at_sensitivity_8: 0.9666 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8433 - val_loss: 0.3665 - val_f1: 0.8829 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7903 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7903\n",
            "Epoch 335/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3731 - f1: 0.8106 - auc: 0.9267 - precision_8: 0.8310 - recall_8: 0.8008 - specificity: 0.7561 - specificity_at_sensitivity_8: 0.9645 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8391 - val_loss: 0.4640 - val_f1: 0.8554 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7473 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7473\n",
            "Epoch 336/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3715 - f1: 0.8124 - auc: 0.9285 - precision_8: 0.8526 - recall_8: 0.7631 - specificity: 0.7755 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8361 - val_loss: 0.3767 - val_f1: 0.8829 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7903 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7903\n",
            "Epoch 337/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3713 - f1: 0.8299 - auc: 0.9282 - precision_8: 0.8456 - recall_8: 0.8183 - specificity: 0.7543 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8529 - val_loss: 0.4058 - val_f1: 0.8761 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7796 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7796\n",
            "Epoch 338/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3693 - f1: 0.8248 - auc: 0.9298 - precision_8: 0.8468 - recall_8: 0.8035 - specificity: 0.7560 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8481 - val_loss: 0.4224 - val_f1: 0.8693 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7688 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7688\n",
            "Epoch 339/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3690 - f1: 0.8206 - auc: 0.9298 - precision_8: 0.8501 - recall_8: 0.7941 - specificity: 0.7671 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8463 - val_loss: 0.4218 - val_f1: 0.8693 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7688 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7688\n",
            "Epoch 340/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3689 - f1: 0.8172 - auc: 0.9293 - precision_8: 0.8473 - recall_8: 0.7914 - specificity: 0.7607 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8439 - val_loss: 0.4176 - val_f1: 0.8761 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7796 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7796\n",
            "Epoch 341/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3680 - f1: 0.8221 - auc: 0.9300 - precision_8: 0.8497 - recall_8: 0.7914 - specificity: 0.7682 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8451 - val_loss: 0.4197 - val_f1: 0.8727 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7742 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7742\n",
            "Epoch 342/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3676 - f1: 0.8200 - auc: 0.9301 - precision_8: 0.8507 - recall_8: 0.7900 - specificity: 0.7724 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8451 - val_loss: 0.3851 - val_f1: 0.8795 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7849 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7849\n",
            "Epoch 343/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3676 - f1: 0.8259 - auc: 0.9303 - precision_8: 0.8407 - recall_8: 0.8170 - specificity: 0.7543 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8499 - val_loss: 0.4112 - val_f1: 0.8761 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7796 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7796\n",
            "Epoch 344/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3666 - f1: 0.8244 - auc: 0.9304 - precision_8: 0.8518 - recall_8: 0.7968 - specificity: 0.7700 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8481 - val_loss: 0.4279 - val_f1: 0.8659 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7634 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7634\n",
            "Epoch 345/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3664 - f1: 0.8176 - auc: 0.9306 - precision_8: 0.8503 - recall_8: 0.7873 - specificity: 0.7722 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8439 - val_loss: 0.3888 - val_f1: 0.8795 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7849 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7849\n",
            "Epoch 346/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3666 - f1: 0.8273 - auc: 0.9305 - precision_8: 0.8419 - recall_8: 0.8170 - specificity: 0.7530 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8505 - val_loss: 0.4293 - val_f1: 0.8659 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7634 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7634\n",
            "Epoch 347/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3673 - f1: 0.8059 - auc: 0.9296 - precision_8: 0.8531 - recall_8: 0.7739 - specificity: 0.7846 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8403 - val_loss: 0.3971 - val_f1: 0.8795 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7849 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7849\n",
            "Epoch 348/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3662 - f1: 0.8222 - auc: 0.9302 - precision_8: 0.8324 - recall_8: 0.8156 - specificity: 0.7507 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8451 - val_loss: 0.3807 - val_f1: 0.8829 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7903 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7903\n",
            "Epoch 349/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3667 - f1: 0.8201 - auc: 0.9284 - precision_8: 0.8447 - recall_8: 0.7981 - specificity: 0.7738 - specificity_at_sensitivity_8: 0.9612 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8451 - val_loss: 0.4343 - val_f1: 0.8659 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7634 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7634\n",
            "Epoch 350/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3640 - f1: 0.8198 - auc: 0.9311 - precision_8: 0.8530 - recall_8: 0.7887 - specificity: 0.7760 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8457 - val_loss: 0.3287 - val_f1: 0.9027 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8226 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8226\n",
            "Epoch 351/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3689 - f1: 0.8242 - auc: 0.9308 - precision_8: 0.8172 - recall_8: 0.8304 - specificity: 0.7357 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8421 - val_loss: 0.4211 - val_f1: 0.8727 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7742 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7742\n",
            "Epoch 352/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3645 - f1: 0.8151 - auc: 0.9308 - precision_8: 0.8540 - recall_8: 0.7793 - specificity: 0.7842 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8427 - val_loss: 0.4150 - val_f1: 0.8761 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7796 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7796\n",
            "Epoch 353/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3625 - f1: 0.8279 - auc: 0.9315 - precision_8: 0.8470 - recall_8: 0.8048 - specificity: 0.7673 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8487 - val_loss: 0.3534 - val_f1: 0.8896 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8011 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8011\n",
            "Epoch 354/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3645 - f1: 0.8266 - auc: 0.9298 - precision_8: 0.8349 - recall_8: 0.8170 - specificity: 0.7573 - specificity_at_sensitivity_8: 0.9666 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8469 - val_loss: 0.4281 - val_f1: 0.8727 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7742 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7742\n",
            "Epoch 355/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3625 - f1: 0.8207 - auc: 0.9315 - precision_8: 0.8528 - recall_8: 0.7873 - specificity: 0.7803 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8451 - val_loss: 0.3767 - val_f1: 0.8829 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7903 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7903\n",
            "Epoch 356/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3626 - f1: 0.8297 - auc: 0.9312 - precision_8: 0.8424 - recall_8: 0.8129 - specificity: 0.7663 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8493 - val_loss: 0.3689 - val_f1: 0.8829 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7903 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7903\n",
            "Epoch 357/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3622 - f1: 0.8301 - auc: 0.9319 - precision_8: 0.8368 - recall_8: 0.8210 - specificity: 0.7501 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8493 - val_loss: 0.4340 - val_f1: 0.8659 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7634 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7634\n",
            "Epoch 358/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3620 - f1: 0.8121 - auc: 0.9318 - precision_8: 0.8546 - recall_8: 0.7752 - specificity: 0.7853 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8415 - val_loss: 0.4077 - val_f1: 0.8761 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7796 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7796\n",
            "Epoch 359/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3619 - f1: 0.8195 - auc: 0.9298 - precision_8: 0.8368 - recall_8: 0.8075 - specificity: 0.7551 - specificity_at_sensitivity_8: 0.9645 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8445 - val_loss: 0.3803 - val_f1: 0.8829 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7903 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7903\n",
            "Epoch 360/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3602 - f1: 0.8251 - auc: 0.9317 - precision_8: 0.8432 - recall_8: 0.8035 - specificity: 0.7742 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8463 - val_loss: 0.4382 - val_f1: 0.8659 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7634 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7634\n",
            "Epoch 361/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3600 - f1: 0.8165 - auc: 0.9325 - precision_8: 0.8540 - recall_8: 0.7793 - specificity: 0.7838 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8427 - val_loss: 0.3449 - val_f1: 0.8961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8118 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8118\n",
            "Epoch 362/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3615 - f1: 0.8203 - auc: 0.9327 - precision_8: 0.8246 - recall_8: 0.8291 - specificity: 0.7436 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8457 - val_loss: 0.4155 - val_f1: 0.8727 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7742 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7742\n",
            "Epoch 363/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3590 - f1: 0.8179 - auc: 0.9325 - precision_8: 0.8540 - recall_8: 0.7873 - specificity: 0.7811 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8457 - val_loss: 0.4261 - val_f1: 0.8727 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7742 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7742\n",
            "Epoch 364/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3607 - f1: 0.8210 - auc: 0.9304 - precision_8: 0.8414 - recall_8: 0.7995 - specificity: 0.7666 - specificity_at_sensitivity_8: 0.9677 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8439 - val_loss: 0.3839 - val_f1: 0.8829 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7903 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7903\n",
            "Epoch 365/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3588 - f1: 0.8232 - auc: 0.9319 - precision_8: 0.8486 - recall_8: 0.7995 - specificity: 0.7762 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8475 - val_loss: 0.4140 - val_f1: 0.8727 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7742 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7742\n",
            "Epoch 366/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3574 - f1: 0.8200 - auc: 0.9332 - precision_8: 0.8530 - recall_8: 0.7968 - specificity: 0.7767 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8487 - val_loss: 0.3383 - val_f1: 0.8994 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8172 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8172\n",
            "Epoch 367/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3600 - f1: 0.8225 - auc: 0.9327 - precision_8: 0.8270 - recall_8: 0.8237 - specificity: 0.7511 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8451 - val_loss: 0.4005 - val_f1: 0.8761 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7796 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7796\n",
            "Epoch 368/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3584 - f1: 0.8137 - auc: 0.9327 - precision_8: 0.8526 - recall_8: 0.7860 - specificity: 0.7848 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8445 - val_loss: 0.3798 - val_f1: 0.8829 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7903 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7903\n",
            "Epoch 369/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3569 - f1: 0.8309 - auc: 0.9331 - precision_8: 0.8395 - recall_8: 0.8237 - specificity: 0.7582 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8517 - val_loss: 0.3676 - val_f1: 0.8829 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7903 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7903\n",
            "Epoch 370/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3572 - f1: 0.8214 - auc: 0.9324 - precision_8: 0.8467 - recall_8: 0.8102 - specificity: 0.7763 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8505 - val_loss: 0.4231 - val_f1: 0.8727 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7742 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7742\n",
            "Epoch 371/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3563 - f1: 0.8203 - auc: 0.9324 - precision_8: 0.8446 - recall_8: 0.8048 - specificity: 0.7722 - specificity_at_sensitivity_8: 0.9688 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8475 - val_loss: 0.3634 - val_f1: 0.8896 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8011 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8011\n",
            "Epoch 372/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3565 - f1: 0.8282 - auc: 0.9331 - precision_8: 0.8440 - recall_8: 0.8156 - specificity: 0.7685 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8511 - val_loss: 0.3992 - val_f1: 0.8795 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7849 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7849\n",
            "Epoch 373/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3560 - f1: 0.8226 - auc: 0.9330 - precision_8: 0.8410 - recall_8: 0.8116 - specificity: 0.7666 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8481 - val_loss: 0.3963 - val_f1: 0.8829 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7903 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7903\n",
            "Epoch 374/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3545 - f1: 0.8235 - auc: 0.9338 - precision_8: 0.8539 - recall_8: 0.8022 - specificity: 0.7831 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8511 - val_loss: 0.4103 - val_f1: 0.8761 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7796 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7796\n",
            "Epoch 375/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3538 - f1: 0.8224 - auc: 0.9341 - precision_8: 0.8547 - recall_8: 0.7995 - specificity: 0.7793 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8505 - val_loss: 0.3394 - val_f1: 0.9027 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8226 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8226\n",
            "Epoch 376/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3565 - f1: 0.8294 - auc: 0.9344 - precision_8: 0.8195 - recall_8: 0.8371 - specificity: 0.7456 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8457 - val_loss: 0.4373 - val_f1: 0.8727 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7742 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7742\n",
            "Epoch 377/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3556 - f1: 0.8106 - auc: 0.9343 - precision_8: 0.8627 - recall_8: 0.7699 - specificity: 0.7984 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8433 - val_loss: 0.4126 - val_f1: 0.8727 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7742 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7742\n",
            "Epoch 378/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3547 - f1: 0.8282 - auc: 0.9324 - precision_8: 0.8445 - recall_8: 0.8116 - specificity: 0.7649 - specificity_at_sensitivity_8: 0.9688 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8499 - val_loss: 0.3501 - val_f1: 0.8929 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8065 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8065\n",
            "Epoch 379/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3560 - f1: 0.8263 - auc: 0.9313 - precision_8: 0.8431 - recall_8: 0.8102 - specificity: 0.7776 - specificity_at_sensitivity_8: 0.9634 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8487 - val_loss: 0.4152 - val_f1: 0.8727 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7742 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7742\n",
            "Epoch 380/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3515 - f1: 0.8266 - auc: 0.9351 - precision_8: 0.8524 - recall_8: 0.8008 - specificity: 0.7831 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8499 - val_loss: 0.3219 - val_f1: 0.9155 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8441 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8441\n",
            "Epoch 381/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3550 - f1: 0.8344 - auc: 0.9349 - precision_8: 0.8244 - recall_8: 0.8466 - specificity: 0.7490 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8517 - val_loss: 0.4164 - val_f1: 0.8727 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7742 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7742\n",
            "Epoch 382/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3528 - f1: 0.8186 - auc: 0.9347 - precision_8: 0.8609 - recall_8: 0.7833 - specificity: 0.7931 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8475 - val_loss: 0.3904 - val_f1: 0.8829 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7903 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7903\n",
            "Epoch 383/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3513 - f1: 0.8358 - auc: 0.9348 - precision_8: 0.8458 - recall_8: 0.8197 - specificity: 0.7676 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8535 - val_loss: 0.3467 - val_f1: 0.8961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8118 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8118\n",
            "Epoch 384/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3545 - f1: 0.8255 - auc: 0.9313 - precision_8: 0.8406 - recall_8: 0.8089 - specificity: 0.7811 - specificity_at_sensitivity_8: 0.9634 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8469 - val_loss: 0.3968 - val_f1: 0.8795 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7849 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7849\n",
            "Epoch 385/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3506 - f1: 0.8317 - auc: 0.9346 - precision_8: 0.8448 - recall_8: 0.8129 - specificity: 0.7718 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8505 - val_loss: 0.3519 - val_f1: 0.8929 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8065 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8065\n",
            "Epoch 386/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3499 - f1: 0.8310 - auc: 0.9352 - precision_8: 0.8425 - recall_8: 0.8277 - specificity: 0.7684 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8547 - val_loss: 0.4288 - val_f1: 0.8727 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7742 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7742\n",
            "Epoch 387/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3505 - f1: 0.8269 - auc: 0.9350 - precision_8: 0.8620 - recall_8: 0.7900 - specificity: 0.7943 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8505 - val_loss: 0.3473 - val_f1: 0.8961 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8118 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8118\n",
            "Epoch 388/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3503 - f1: 0.8343 - auc: 0.9359 - precision_8: 0.8316 - recall_8: 0.8371 - specificity: 0.7580 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8523 - val_loss: 0.3798 - val_f1: 0.8829 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7903 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7903\n",
            "Epoch 389/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3483 - f1: 0.8276 - auc: 0.9354 - precision_8: 0.8477 - recall_8: 0.8089 - specificity: 0.7816 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8505 - val_loss: 0.4184 - val_f1: 0.8727 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7742 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7742\n",
            "Epoch 390/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3488 - f1: 0.8302 - auc: 0.9351 - precision_8: 0.8553 - recall_8: 0.8035 - specificity: 0.7846 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8523 - val_loss: 0.3455 - val_f1: 0.8994 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8172 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8172\n",
            "Epoch 391/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3484 - f1: 0.8357 - auc: 0.9355 - precision_8: 0.8432 - recall_8: 0.8250 - specificity: 0.7666 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8541 - val_loss: 0.3977 - val_f1: 0.8795 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7849 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7849\n",
            "Epoch 392/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3476 - f1: 0.8285 - auc: 0.9355 - precision_8: 0.8584 - recall_8: 0.7995 - specificity: 0.7875 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8523 - val_loss: 0.3698 - val_f1: 0.8829 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7903 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7903\n",
            "Epoch 393/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3469 - f1: 0.8232 - auc: 0.9358 - precision_8: 0.8417 - recall_8: 0.8156 - specificity: 0.7709 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8499 - val_loss: 0.3643 - val_f1: 0.8896 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8011 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8011\n",
            "Epoch 394/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3465 - f1: 0.8295 - auc: 0.9358 - precision_8: 0.8448 - recall_8: 0.8129 - specificity: 0.7766 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8505 - val_loss: 0.3983 - val_f1: 0.8795 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7849 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7849\n",
            "Epoch 395/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3463 - f1: 0.8323 - auc: 0.9357 - precision_8: 0.8545 - recall_8: 0.8062 - specificity: 0.7830 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8529 - val_loss: 0.3639 - val_f1: 0.8896 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8011 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8011\n",
            "Epoch 396/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3457 - f1: 0.8335 - auc: 0.9363 - precision_8: 0.8466 - recall_8: 0.8170 - specificity: 0.7740 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8529 - val_loss: 0.3841 - val_f1: 0.8829 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7903 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7903\n",
            "Epoch 397/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3457 - f1: 0.8282 - auc: 0.9355 - precision_8: 0.8464 - recall_8: 0.8156 - specificity: 0.7751 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8523 - val_loss: 0.3916 - val_f1: 0.8829 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7903 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7903\n",
            "Epoch 398/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3452 - f1: 0.8281 - auc: 0.9361 - precision_8: 0.8576 - recall_8: 0.8022 - specificity: 0.7867 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8529 - val_loss: 0.3712 - val_f1: 0.8829 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7903 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7903\n",
            "Epoch 399/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3445 - f1: 0.8327 - auc: 0.9363 - precision_8: 0.8447 - recall_8: 0.8197 - specificity: 0.7726 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8529 - val_loss: 0.3632 - val_f1: 0.8929 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8065 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8065\n",
            "Epoch 400/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3441 - f1: 0.8365 - auc: 0.9364 - precision_8: 0.8499 - recall_8: 0.8156 - specificity: 0.7790 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8541 - val_loss: 0.3936 - val_f1: 0.8795 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7849 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7849\n",
            "Epoch 401/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3442 - f1: 0.8367 - auc: 0.9362 - precision_8: 0.8547 - recall_8: 0.8075 - specificity: 0.7841 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8535 - val_loss: 0.3707 - val_f1: 0.8862 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7957 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7957\n",
            "Epoch 402/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3435 - f1: 0.8330 - auc: 0.9367 - precision_8: 0.8490 - recall_8: 0.8170 - specificity: 0.7767 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8541 - val_loss: 0.4006 - val_f1: 0.8795 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7849 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7849\n",
            "Epoch 403/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3437 - f1: 0.8335 - auc: 0.9367 - precision_8: 0.8634 - recall_8: 0.7995 - specificity: 0.7940 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8547 - val_loss: 0.3564 - val_f1: 0.8929 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8065 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8065\n",
            "Epoch 404/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3437 - f1: 0.8352 - auc: 0.9367 - precision_8: 0.8408 - recall_8: 0.8318 - specificity: 0.7635 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8553 - val_loss: 0.3769 - val_f1: 0.8829 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7903 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7903\n",
            "Epoch 405/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3420 - f1: 0.8338 - auc: 0.9368 - precision_8: 0.8571 - recall_8: 0.8075 - specificity: 0.7885 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8547 - val_loss: 0.4115 - val_f1: 0.8761 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7796 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7796\n",
            "Epoch 406/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3424 - f1: 0.8296 - auc: 0.9367 - precision_8: 0.8601 - recall_8: 0.7941 - specificity: 0.7966 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8511 - val_loss: 0.3223 - val_f1: 0.9155 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8441 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8441\n",
            "Epoch 407/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3439 - f1: 0.8397 - auc: 0.9373 - precision_8: 0.8269 - recall_8: 0.8614 - specificity: 0.7569 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8583 - val_loss: 0.4035 - val_f1: 0.8795 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7849 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7849\n",
            "Epoch 408/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3451 - f1: 0.8230 - auc: 0.9359 - precision_8: 0.8661 - recall_8: 0.7833 - specificity: 0.8076 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8499 - val_loss: 0.3510 - val_f1: 0.9059 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8280 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8280\n",
            "Epoch 409/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3429 - f1: 0.8450 - auc: 0.9373 - precision_8: 0.8378 - recall_8: 0.8479 - specificity: 0.7600 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8594 - val_loss: 0.3339 - val_f1: 0.9123 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8387 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8387\n",
            "Epoch 410/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3421 - f1: 0.8365 - auc: 0.9365 - precision_8: 0.8405 - recall_8: 0.8371 - specificity: 0.7835 - specificity_at_sensitivity_8: 0.9688 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8571 - val_loss: 0.4325 - val_f1: 0.8693 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7688 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7688\n",
            "Epoch 411/1000\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.3418 - f1: 0.8215 - auc: 0.9372 - precision_8: 0.8515 - recall_8: 0.7873 - specificity: 0.7935 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8445 - val_loss: 0.3072 - val_f1: 0.9280 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8656 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8656\n",
            "Epoch 412/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3426 - f1: 0.8475 - auc: 0.9377 - precision_8: 0.8240 - recall_8: 0.8694 - specificity: 0.7593 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8594 - val_loss: 0.4180 - val_f1: 0.8727 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7742 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7742\n",
            "Epoch 413/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3422 - f1: 0.8210 - auc: 0.9372 - precision_8: 0.8661 - recall_8: 0.7833 - specificity: 0.8075 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8499 - val_loss: 0.3488 - val_f1: 0.9091 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8333 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8333\n",
            "Epoch 414/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3415 - f1: 0.8423 - auc: 0.9371 - precision_8: 0.8355 - recall_8: 0.8546 - specificity: 0.7569 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8606 - val_loss: 0.3511 - val_f1: 0.9091 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8333 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8333\n",
            "Epoch 415/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3482 - f1: 0.8124 - auc: 0.9321 - precision_8: 0.8576 - recall_8: 0.7941 - specificity: 0.8042 - specificity_at_sensitivity_8: 0.9666 - sensitivity_at_specificity_8: 0.9973 - accuracy: 0.8499 - val_loss: 0.3637 - val_f1: 0.8896 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8011 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8011\n",
            "Epoch 416/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3391 - f1: 0.8480 - auc: 0.9376 - precision_8: 0.8418 - recall_8: 0.8520 - specificity: 0.7648 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8630 - val_loss: 0.2998 - val_f1: 0.9432 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8925 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8925\n",
            "Epoch 417/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3402 - f1: 0.8526 - auc: 0.9371 - precision_8: 0.8348 - recall_8: 0.8708 - specificity: 0.7701 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8660 - val_loss: 0.4674 - val_f1: 0.8589 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7527 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7527\n",
            "Epoch 418/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3437 - f1: 0.8245 - auc: 0.9372 - precision_8: 0.8686 - recall_8: 0.7739 - specificity: 0.8110 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8475 - val_loss: 0.3240 - val_f1: 0.9186 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8495 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8495\n",
            "Epoch 419/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3390 - f1: 0.8486 - auc: 0.9382 - precision_8: 0.8353 - recall_8: 0.8600 - specificity: 0.7665 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8624 - val_loss: 0.3411 - val_f1: 0.9123 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8387 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8387\n",
            "Epoch 420/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3380 - f1: 0.8330 - auc: 0.9377 - precision_8: 0.8471 - recall_8: 0.8277 - specificity: 0.7825 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8571 - val_loss: 0.3781 - val_f1: 0.8862 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7957 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7957\n",
            "Epoch 421/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3368 - f1: 0.8393 - auc: 0.9379 - precision_8: 0.8522 - recall_8: 0.8223 - specificity: 0.7821 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8577 - val_loss: 0.3462 - val_f1: 0.9091 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8333 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8333\n",
            "Epoch 422/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3361 - f1: 0.8416 - auc: 0.9387 - precision_8: 0.8489 - recall_8: 0.8318 - specificity: 0.7797 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8594 - val_loss: 0.3893 - val_f1: 0.8795 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7849 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7849\n",
            "Epoch 423/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3374 - f1: 0.8308 - auc: 0.9371 - precision_8: 0.8521 - recall_8: 0.8143 - specificity: 0.7839 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8547 - val_loss: 0.3801 - val_f1: 0.8862 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7957 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7957\n",
            "Epoch 424/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3368 - f1: 0.8288 - auc: 0.9382 - precision_8: 0.8642 - recall_8: 0.8048 - specificity: 0.7991 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8571 - val_loss: 0.3394 - val_f1: 0.9123 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8387 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8387\n",
            "Epoch 425/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3389 - f1: 0.8571 - auc: 0.9374 - precision_8: 0.8331 - recall_8: 0.8802 - specificity: 0.7574 - specificity_at_sensitivity_8: 0.9688 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8684 - val_loss: 0.3694 - val_f1: 0.8896 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8011 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8011\n",
            "Epoch 426/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3365 - f1: 0.8270 - auc: 0.9380 - precision_8: 0.8640 - recall_8: 0.8035 - specificity: 0.8028 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8565 - val_loss: 0.4055 - val_f1: 0.8761 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7796 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7796\n",
            "Epoch 427/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3358 - f1: 0.8460 - auc: 0.9385 - precision_8: 0.8591 - recall_8: 0.8291 - specificity: 0.7882 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8636 - val_loss: 0.3034 - val_f1: 0.9432 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8925 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8925\n",
            "Epoch 428/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3373 - f1: 0.8498 - auc: 0.9383 - precision_8: 0.8388 - recall_8: 0.8681 - specificity: 0.7716 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8672 - val_loss: 0.4023 - val_f1: 0.8761 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7796 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7796\n",
            "Epoch 429/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3349 - f1: 0.8312 - auc: 0.9388 - precision_8: 0.8652 - recall_8: 0.8035 - specificity: 0.7981 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8571 - val_loss: 0.3415 - val_f1: 0.9155 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8441 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8441\n",
            "Epoch 430/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3338 - f1: 0.8475 - auc: 0.9394 - precision_8: 0.8453 - recall_8: 0.8533 - specificity: 0.7773 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8654 - val_loss: 0.3639 - val_f1: 0.8929 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8065 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8065\n",
            "Epoch 431/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3339 - f1: 0.8322 - auc: 0.9388 - precision_8: 0.8596 - recall_8: 0.8075 - specificity: 0.7977 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8559 - val_loss: 0.3581 - val_f1: 0.9059 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8280 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8280\n",
            "Epoch 432/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3368 - f1: 0.8460 - auc: 0.9379 - precision_8: 0.8397 - recall_8: 0.8533 - specificity: 0.7684 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8624 - val_loss: 0.3668 - val_f1: 0.8896 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8011 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8011\n",
            "Epoch 433/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3320 - f1: 0.8341 - auc: 0.9396 - precision_8: 0.8649 - recall_8: 0.8102 - specificity: 0.7971 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8594 - val_loss: 0.4268 - val_f1: 0.8727 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7742 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7742\n",
            "Epoch 434/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3359 - f1: 0.8293 - auc: 0.9383 - precision_8: 0.8584 - recall_8: 0.7995 - specificity: 0.7992 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8523 - val_loss: 0.3201 - val_f1: 0.9280 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8656 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8656\n",
            "Epoch 435/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3326 - f1: 0.8557 - auc: 0.9401 - precision_8: 0.8401 - recall_8: 0.8694 - specificity: 0.7739 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8684 - val_loss: 0.3916 - val_f1: 0.8829 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7903 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7903\n",
            "Epoch 436/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3324 - f1: 0.8326 - auc: 0.9396 - precision_8: 0.8640 - recall_8: 0.8035 - specificity: 0.8006 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8565 - val_loss: 0.3676 - val_f1: 0.8929 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8065 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8065\n",
            "Epoch 437/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3309 - f1: 0.8413 - auc: 0.9399 - precision_8: 0.8558 - recall_8: 0.8304 - specificity: 0.7878 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8624 - val_loss: 0.3301 - val_f1: 0.9186 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8495 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8495\n",
            "Epoch 438/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3311 - f1: 0.8482 - auc: 0.9398 - precision_8: 0.8476 - recall_8: 0.8533 - specificity: 0.7829 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8666 - val_loss: 0.3717 - val_f1: 0.8896 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8011 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8011\n",
            "Epoch 439/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3306 - f1: 0.8427 - auc: 0.9399 - precision_8: 0.8555 - recall_8: 0.8210 - specificity: 0.7914 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8589 - val_loss: 0.3591 - val_f1: 0.9059 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8280 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8280\n",
            "Epoch 440/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3299 - f1: 0.8409 - auc: 0.9402 - precision_8: 0.8589 - recall_8: 0.8277 - specificity: 0.7906 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8630 - val_loss: 0.3569 - val_f1: 0.9059 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8280 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8280\n",
            "Epoch 441/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3306 - f1: 0.8385 - auc: 0.9395 - precision_8: 0.8567 - recall_8: 0.8210 - specificity: 0.7994 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8594 - val_loss: 0.3247 - val_f1: 0.9249 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8602 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8602\n",
            "Epoch 442/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3329 - f1: 0.8585 - auc: 0.9398 - precision_8: 0.8319 - recall_8: 0.8923 - specificity: 0.7637 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8720 - val_loss: 0.3810 - val_f1: 0.8862 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7957 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7957\n",
            "Epoch 443/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3336 - f1: 0.8288 - auc: 0.9390 - precision_8: 0.8636 - recall_8: 0.7927 - specificity: 0.8167 - specificity_at_sensitivity_8: 0.9677 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8523 - val_loss: 0.3658 - val_f1: 0.9027 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8226 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8226\n",
            "Epoch 444/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3314 - f1: 0.8458 - auc: 0.9385 - precision_8: 0.8415 - recall_8: 0.8506 - specificity: 0.7719 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8624 - val_loss: 0.2978 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 445/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3304 - f1: 0.8515 - auc: 0.9404 - precision_8: 0.8385 - recall_8: 0.8668 - specificity: 0.7820 - specificity_at_sensitivity_8: 0.9677 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8666 - val_loss: 0.4387 - val_f1: 0.8727 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7742 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7742\n",
            "Epoch 446/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3321 - f1: 0.8295 - auc: 0.9398 - precision_8: 0.8707 - recall_8: 0.7887 - specificity: 0.8121 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8541 - val_loss: 0.2993 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 447/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3309 - f1: 0.8718 - auc: 0.9413 - precision_8: 0.8301 - recall_8: 0.9139 - specificity: 0.7626 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8786 - val_loss: 0.3660 - val_f1: 0.9027 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8226 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8226\n",
            "Epoch 448/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3293 - f1: 0.8333 - auc: 0.9398 - precision_8: 0.8612 - recall_8: 0.8102 - specificity: 0.8080 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8577 - val_loss: 0.3723 - val_f1: 0.8994 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8172 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8172\n",
            "Epoch 449/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3283 - f1: 0.8462 - auc: 0.9403 - precision_8: 0.8473 - recall_8: 0.8439 - specificity: 0.7851 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8630 - val_loss: 0.3089 - val_f1: 0.9432 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8925 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8925\n",
            "Epoch 450/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3292 - f1: 0.8505 - auc: 0.9394 - precision_8: 0.8425 - recall_8: 0.8641 - specificity: 0.7888 - specificity_at_sensitivity_8: 0.9677 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8678 - val_loss: 0.3855 - val_f1: 0.8862 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7957 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7957\n",
            "Epoch 451/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3293 - f1: 0.8434 - auc: 0.9392 - precision_8: 0.8583 - recall_8: 0.8237 - specificity: 0.7890 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8612 - val_loss: 0.3401 - val_f1: 0.9186 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8495 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8495\n",
            "Epoch 452/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3265 - f1: 0.8484 - auc: 0.9409 - precision_8: 0.8623 - recall_8: 0.8425 - specificity: 0.7957 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8702 - val_loss: 0.3656 - val_f1: 0.9027 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8226 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8226\n",
            "Epoch 453/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3277 - f1: 0.8528 - auc: 0.9397 - precision_8: 0.8558 - recall_8: 0.8466 - specificity: 0.7844 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8684 - val_loss: 0.3457 - val_f1: 0.9155 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8441 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8441\n",
            "Epoch 454/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3259 - f1: 0.8395 - auc: 0.9410 - precision_8: 0.8597 - recall_8: 0.8250 - specificity: 0.7971 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8624 - val_loss: 0.3642 - val_f1: 0.9027 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8226 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8226\n",
            "Epoch 455/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3269 - f1: 0.8506 - auc: 0.9406 - precision_8: 0.8507 - recall_8: 0.8439 - specificity: 0.7858 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8648 - val_loss: 0.3337 - val_f1: 0.9186 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8495 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8495\n",
            "Epoch 456/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3255 - f1: 0.8479 - auc: 0.9409 - precision_8: 0.8560 - recall_8: 0.8479 - specificity: 0.7973 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8690 - val_loss: 0.3704 - val_f1: 0.9027 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8226 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8226\n",
            "Epoch 457/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3244 - f1: 0.8550 - auc: 0.9414 - precision_8: 0.8641 - recall_8: 0.8385 - specificity: 0.7967 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8696 - val_loss: 0.3077 - val_f1: 0.9462 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8978 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8978\n",
            "Epoch 458/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3252 - f1: 0.8598 - auc: 0.9418 - precision_8: 0.8382 - recall_8: 0.8856 - specificity: 0.7769 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8732 - val_loss: 0.3714 - val_f1: 0.9027 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8226 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8226\n",
            "Epoch 459/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3245 - f1: 0.8476 - auc: 0.9415 - precision_8: 0.8638 - recall_8: 0.8277 - specificity: 0.7989 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8654 - val_loss: 0.3638 - val_f1: 0.9027 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8226 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8226\n",
            "Epoch 460/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3237 - f1: 0.8472 - auc: 0.9417 - precision_8: 0.8589 - recall_8: 0.8358 - specificity: 0.7948 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8660 - val_loss: 0.3403 - val_f1: 0.9186 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8495 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8495\n",
            "Epoch 461/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3232 - f1: 0.8584 - auc: 0.9419 - precision_8: 0.8626 - recall_8: 0.8533 - specificity: 0.7961 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8744 - val_loss: 0.3354 - val_f1: 0.9186 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8495 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8495\n",
            "Epoch 462/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3229 - f1: 0.8622 - auc: 0.9423 - precision_8: 0.8573 - recall_8: 0.8735 - specificity: 0.7828 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8792 - val_loss: 0.3469 - val_f1: 0.9155 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8441 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8441\n",
            "Epoch 463/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3231 - f1: 0.8498 - auc: 0.9416 - precision_8: 0.8605 - recall_8: 0.8385 - specificity: 0.8048 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8678 - val_loss: 0.3396 - val_f1: 0.9186 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8495 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8495\n",
            "Epoch 464/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3237 - f1: 0.8648 - auc: 0.9414 - precision_8: 0.8472 - recall_8: 0.8802 - specificity: 0.7810 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8762 - val_loss: 0.3458 - val_f1: 0.9186 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8495 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8495\n",
            "Epoch 465/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3240 - f1: 0.8366 - auc: 0.9408 - precision_8: 0.8644 - recall_8: 0.8237 - specificity: 0.8088 - specificity_at_sensitivity_8: 0.9688 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8642 - val_loss: 0.3481 - val_f1: 0.9155 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8441 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8441\n",
            "Epoch 466/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3265 - f1: 0.8591 - auc: 0.9401 - precision_8: 0.8453 - recall_8: 0.8748 - specificity: 0.7736 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8732 - val_loss: 0.3326 - val_f1: 0.9280 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8656 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8656\n",
            "Epoch 467/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3232 - f1: 0.8436 - auc: 0.9412 - precision_8: 0.8635 - recall_8: 0.8345 - specificity: 0.8066 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8678 - val_loss: 0.3914 - val_f1: 0.8795 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7849 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7849\n",
            "Epoch 468/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3241 - f1: 0.8507 - auc: 0.9407 - precision_8: 0.8529 - recall_8: 0.8425 - specificity: 0.7948 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8654 - val_loss: 0.2923 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 469/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3221 - f1: 0.8591 - auc: 0.9419 - precision_8: 0.8445 - recall_8: 0.8843 - specificity: 0.7841 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8762 - val_loss: 0.3981 - val_f1: 0.8761 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7796 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7796\n",
            "Epoch 470/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3220 - f1: 0.8421 - auc: 0.9427 - precision_8: 0.8689 - recall_8: 0.8116 - specificity: 0.8120 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8618 - val_loss: 0.3015 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 471/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3219 - f1: 0.8738 - auc: 0.9434 - precision_8: 0.8368 - recall_8: 0.9179 - specificity: 0.7740 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8840 - val_loss: 0.3481 - val_f1: 0.9186 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8495 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8495\n",
            "Epoch 472/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3195 - f1: 0.8467 - auc: 0.9432 - precision_8: 0.8658 - recall_8: 0.8425 - specificity: 0.8059 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8720 - val_loss: 0.3787 - val_f1: 0.8994 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8172 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8172\n",
            "Epoch 473/1000\n",
            "2/2 [==============================] - 5s 3s/step - loss: 0.3193 - f1: 0.8512 - auc: 0.9431 - precision_8: 0.8680 - recall_8: 0.8318 - specificity: 0.8065 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8690 - val_loss: 0.2836 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 474/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3219 - f1: 0.8744 - auc: 0.9436 - precision_8: 0.8321 - recall_8: 0.9206 - specificity: 0.7712 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8822 - val_loss: 0.3708 - val_f1: 0.9059 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8280 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8280\n",
            "Epoch 475/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3208 - f1: 0.8307 - auc: 0.9431 - precision_8: 0.8681 - recall_8: 0.8062 - specificity: 0.8142 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8594 - val_loss: 0.3362 - val_f1: 0.9280 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8656 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8656\n",
            "Epoch 476/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3207 - f1: 0.8706 - auc: 0.9415 - precision_8: 0.8469 - recall_8: 0.8937 - specificity: 0.7773 - specificity_at_sensitivity_8: 0.9677 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8810 - val_loss: 0.3142 - val_f1: 0.9462 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8978 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8978\n",
            "Epoch 477/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3189 - f1: 0.8592 - auc: 0.9431 - precision_8: 0.8637 - recall_8: 0.8614 - specificity: 0.8017 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8780 - val_loss: 0.3899 - val_f1: 0.8829 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.7903 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.7903\n",
            "Epoch 478/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3188 - f1: 0.8477 - auc: 0.9427 - precision_8: 0.8626 - recall_8: 0.8277 - specificity: 0.8076 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8648 - val_loss: 0.2862 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 479/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3194 - f1: 0.8691 - auc: 0.9436 - precision_8: 0.8425 - recall_8: 0.9071 - specificity: 0.7803 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8834 - val_loss: 0.3646 - val_f1: 0.9059 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8280 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8280\n",
            "Epoch 480/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3188 - f1: 0.8408 - auc: 0.9435 - precision_8: 0.8675 - recall_8: 0.8197 - specificity: 0.8168 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8642 - val_loss: 0.3125 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 481/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3185 - f1: 0.8719 - auc: 0.9438 - precision_8: 0.8402 - recall_8: 0.9058 - specificity: 0.7755 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8816 - val_loss: 0.3217 - val_f1: 0.9402 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8871 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8871\n",
            "Epoch 482/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3174 - f1: 0.8659 - auc: 0.9426 - precision_8: 0.8671 - recall_8: 0.8694 - specificity: 0.8089 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8828 - val_loss: 0.3774 - val_f1: 0.8994 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8172 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8172\n",
            "Epoch 483/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3185 - f1: 0.8606 - auc: 0.9426 - precision_8: 0.8595 - recall_8: 0.8479 - specificity: 0.7998 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8708 - val_loss: 0.2996 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 484/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3165 - f1: 0.8751 - auc: 0.9439 - precision_8: 0.8553 - recall_8: 0.8991 - specificity: 0.7907 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8876 - val_loss: 0.3741 - val_f1: 0.9027 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8226 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8226\n",
            "Epoch 485/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3173 - f1: 0.8530 - auc: 0.9432 - precision_8: 0.8646 - recall_8: 0.8425 - specificity: 0.8041 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8714 - val_loss: 0.3251 - val_f1: 0.9402 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8871 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8871\n",
            "Epoch 486/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3152 - f1: 0.8675 - auc: 0.9443 - precision_8: 0.8675 - recall_8: 0.8721 - specificity: 0.7972 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8840 - val_loss: 0.3316 - val_f1: 0.9371 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8817 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8817\n",
            "Epoch 487/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3147 - f1: 0.8705 - auc: 0.9444 - precision_8: 0.8623 - recall_8: 0.8762 - specificity: 0.7951 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8828 - val_loss: 0.3277 - val_f1: 0.9402 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8871 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8871\n",
            "Epoch 488/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3144 - f1: 0.8715 - auc: 0.9444 - precision_8: 0.8628 - recall_8: 0.8802 - specificity: 0.7953 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8846 - val_loss: 0.3613 - val_f1: 0.9091 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8333 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8333\n",
            "Epoch 489/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3147 - f1: 0.8549 - auc: 0.9443 - precision_8: 0.8672 - recall_8: 0.8439 - specificity: 0.8065 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8732 - val_loss: 0.3286 - val_f1: 0.9402 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8871 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8871\n",
            "Epoch 490/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3137 - f1: 0.8717 - auc: 0.9447 - precision_8: 0.8628 - recall_8: 0.8802 - specificity: 0.7968 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8846 - val_loss: 0.3185 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 491/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3158 - f1: 0.8616 - auc: 0.9431 - precision_8: 0.8615 - recall_8: 0.8708 - specificity: 0.8079 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8804 - val_loss: 0.3041 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 492/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3158 - f1: 0.8738 - auc: 0.9442 - precision_8: 0.8376 - recall_8: 0.9233 - specificity: 0.7772 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8864 - val_loss: 0.3440 - val_f1: 0.9280 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8656 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8656\n",
            "Epoch 493/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3134 - f1: 0.8540 - auc: 0.9444 - precision_8: 0.8694 - recall_8: 0.8425 - specificity: 0.8139 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8738 - val_loss: 0.3712 - val_f1: 0.9027 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8226 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8226\n",
            "Epoch 494/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3150 - f1: 0.8620 - auc: 0.9440 - precision_8: 0.8573 - recall_8: 0.8573 - specificity: 0.8006 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8732 - val_loss: 0.2959 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 495/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3127 - f1: 0.8778 - auc: 0.9446 - precision_8: 0.8597 - recall_8: 0.9071 - specificity: 0.7909 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8929 - val_loss: 0.3814 - val_f1: 0.9027 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8226 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8226\n",
            "Epoch 496/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3141 - f1: 0.8543 - auc: 0.9440 - precision_8: 0.8657 - recall_8: 0.8331 - specificity: 0.8121 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8684 - val_loss: 0.3145 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 497/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3118 - f1: 0.8809 - auc: 0.9452 - precision_8: 0.8618 - recall_8: 0.8977 - specificity: 0.7984 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8906 - val_loss: 0.3226 - val_f1: 0.9402 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8871 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8871\n",
            "Epoch 498/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3112 - f1: 0.8735 - auc: 0.9453 - precision_8: 0.8635 - recall_8: 0.8856 - specificity: 0.7998 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8870 - val_loss: 0.3382 - val_f1: 0.9371 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8817 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8817\n",
            "Epoch 499/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3110 - f1: 0.8691 - auc: 0.9452 - precision_8: 0.8671 - recall_8: 0.8694 - specificity: 0.8060 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8828 - val_loss: 0.3231 - val_f1: 0.9402 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8871 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8871\n",
            "Epoch 500/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3117 - f1: 0.8770 - auc: 0.9450 - precision_8: 0.8584 - recall_8: 0.8977 - specificity: 0.7928 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8888 - val_loss: 0.3506 - val_f1: 0.9217 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8548 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8548\n",
            "Epoch 501/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3124 - f1: 0.8497 - auc: 0.9450 - precision_8: 0.8696 - recall_8: 0.8345 - specificity: 0.8184 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8708 - val_loss: 0.3132 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 502/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3121 - f1: 0.8715 - auc: 0.9452 - precision_8: 0.8414 - recall_8: 0.9139 - specificity: 0.7825 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8852 - val_loss: 0.3140 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 503/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3110 - f1: 0.8631 - auc: 0.9443 - precision_8: 0.8609 - recall_8: 0.8748 - specificity: 0.8131 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8816 - val_loss: 0.3657 - val_f1: 0.9059 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8280 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8280\n",
            "Epoch 504/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3117 - f1: 0.8650 - auc: 0.9440 - precision_8: 0.8598 - recall_8: 0.8668 - specificity: 0.8009 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8780 - val_loss: 0.2875 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 505/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3108 - f1: 0.8754 - auc: 0.9445 - precision_8: 0.8493 - recall_8: 0.8950 - specificity: 0.8007 - specificity_at_sensitivity_8: 0.9677 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8828 - val_loss: 0.3640 - val_f1: 0.9091 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8333 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8333\n",
            "Epoch 506/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3100 - f1: 0.8644 - auc: 0.9451 - precision_8: 0.8660 - recall_8: 0.8614 - specificity: 0.8104 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8792 - val_loss: 0.3020 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 507/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3094 - f1: 0.8741 - auc: 0.9453 - precision_8: 0.8595 - recall_8: 0.8977 - specificity: 0.7976 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8894 - val_loss: 0.3235 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 508/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3086 - f1: 0.8737 - auc: 0.9456 - precision_8: 0.8685 - recall_8: 0.8802 - specificity: 0.8084 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8876 - val_loss: 0.2979 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 509/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3086 - f1: 0.8914 - auc: 0.9462 - precision_8: 0.8600 - recall_8: 0.9179 - specificity: 0.7958 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8971 - val_loss: 0.3270 - val_f1: 0.9432 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8925 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8925\n",
            "Epoch 510/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3079 - f1: 0.8717 - auc: 0.9457 - precision_8: 0.8657 - recall_8: 0.8762 - specificity: 0.8110 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8846 - val_loss: 0.3260 - val_f1: 0.9462 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8978 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8978\n",
            "Epoch 511/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3085 - f1: 0.8828 - auc: 0.9459 - precision_8: 0.8606 - recall_8: 0.8977 - specificity: 0.7961 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8900 - val_loss: 0.3268 - val_f1: 0.9432 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8925 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8925\n",
            "Epoch 512/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3077 - f1: 0.8654 - auc: 0.9457 - precision_8: 0.8691 - recall_8: 0.8668 - specificity: 0.8150 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8828 - val_loss: 0.3323 - val_f1: 0.9402 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8871 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8871\n",
            "Epoch 513/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3107 - f1: 0.8711 - auc: 0.9444 - precision_8: 0.8500 - recall_8: 0.8923 - specificity: 0.7912 - specificity_at_sensitivity_8: 0.9688 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8822 - val_loss: 0.3311 - val_f1: 0.9402 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8871 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8871\n",
            "Epoch 514/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3073 - f1: 0.8533 - auc: 0.9460 - precision_8: 0.8663 - recall_8: 0.8546 - specificity: 0.8139 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8768 - val_loss: 0.3542 - val_f1: 0.9249 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8602 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8602\n",
            "Epoch 515/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3097 - f1: 0.8665 - auc: 0.9437 - precision_8: 0.8538 - recall_8: 0.8802 - specificity: 0.7930 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8798 - val_loss: 0.2953 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 516/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3062 - f1: 0.8740 - auc: 0.9461 - precision_8: 0.8606 - recall_8: 0.8977 - specificity: 0.8045 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8900 - val_loss: 0.3708 - val_f1: 0.9091 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8333 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8333\n",
            "Epoch 517/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3071 - f1: 0.8722 - auc: 0.9458 - precision_8: 0.8718 - recall_8: 0.8694 - specificity: 0.8133 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8852 - val_loss: 0.2828 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 518/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3076 - f1: 0.8795 - auc: 0.9469 - precision_8: 0.8450 - recall_8: 0.9246 - specificity: 0.7823 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8911 - val_loss: 0.3816 - val_f1: 0.9027 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8226 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8226\n",
            "Epoch 519/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3090 - f1: 0.8486 - auc: 0.9464 - precision_8: 0.8734 - recall_8: 0.8264 - specificity: 0.8300 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8696 - val_loss: 0.3131 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 520/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3072 - f1: 0.8846 - auc: 0.9463 - precision_8: 0.8564 - recall_8: 0.9152 - specificity: 0.7876 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8941 - val_loss: 0.3010 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 521/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3069 - f1: 0.8672 - auc: 0.9454 - precision_8: 0.8679 - recall_8: 0.8843 - specificity: 0.8134 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8888 - val_loss: 0.3546 - val_f1: 0.9310 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8710 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8710\n",
            "Epoch 522/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3057 - f1: 0.8764 - auc: 0.9454 - precision_8: 0.8639 - recall_8: 0.8802 - specificity: 0.8044 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8852 - val_loss: 0.2746 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 523/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3070 - f1: 0.8828 - auc: 0.9453 - precision_8: 0.8568 - recall_8: 0.9098 - specificity: 0.8003 - specificity_at_sensitivity_8: 0.9688 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8923 - val_loss: 0.3506 - val_f1: 0.9402 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8871 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8871\n",
            "Epoch 524/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3044 - f1: 0.8730 - auc: 0.9461 - precision_8: 0.8675 - recall_8: 0.8816 - specificity: 0.8071 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8876 - val_loss: 0.2944 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 525/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3035 - f1: 0.8864 - auc: 0.9473 - precision_8: 0.8600 - recall_8: 0.9179 - specificity: 0.7979 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8971 - val_loss: 0.3531 - val_f1: 0.9341 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8763 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8763\n",
            "Epoch 526/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3041 - f1: 0.8642 - auc: 0.9470 - precision_8: 0.8709 - recall_8: 0.8627 - specificity: 0.8205 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8822 - val_loss: 0.3021 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 527/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3029 - f1: 0.8884 - auc: 0.9474 - precision_8: 0.8589 - recall_8: 0.9179 - specificity: 0.7990 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8965 - val_loss: 0.3032 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 528/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3024 - f1: 0.8784 - auc: 0.9471 - precision_8: 0.8675 - recall_8: 0.8991 - specificity: 0.8077 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8941 - val_loss: 0.3435 - val_f1: 0.9402 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8871 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8871\n",
            "Epoch 529/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3024 - f1: 0.8756 - auc: 0.9471 - precision_8: 0.8708 - recall_8: 0.8802 - specificity: 0.8119 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8888 - val_loss: 0.2926 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 530/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3022 - f1: 0.8887 - auc: 0.9476 - precision_8: 0.8622 - recall_8: 0.9179 - specificity: 0.7965 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8983 - val_loss: 0.3331 - val_f1: 0.9521 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9086 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9086\n",
            "Epoch 531/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3018 - f1: 0.8780 - auc: 0.9474 - precision_8: 0.8708 - recall_8: 0.8802 - specificity: 0.8160 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8888 - val_loss: 0.3091 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 532/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3015 - f1: 0.8872 - auc: 0.9475 - precision_8: 0.8619 - recall_8: 0.9152 - specificity: 0.8000 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8971 - val_loss: 0.3169 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 533/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3007 - f1: 0.8800 - auc: 0.9476 - precision_8: 0.8704 - recall_8: 0.8950 - specificity: 0.8094 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8941 - val_loss: 0.3360 - val_f1: 0.9432 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8925 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8925\n",
            "Epoch 534/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3007 - f1: 0.8800 - auc: 0.9478 - precision_8: 0.8704 - recall_8: 0.8856 - specificity: 0.8128 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8906 - val_loss: 0.2887 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 535/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3007 - f1: 0.8890 - auc: 0.9476 - precision_8: 0.8602 - recall_8: 0.9192 - specificity: 0.8010 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8977 - val_loss: 0.3297 - val_f1: 0.9521 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9086 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9086\n",
            "Epoch 536/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3002 - f1: 0.8715 - auc: 0.9476 - precision_8: 0.8712 - recall_8: 0.8829 - specificity: 0.8133 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8900 - val_loss: 0.3043 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 537/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3019 - f1: 0.8812 - auc: 0.9470 - precision_8: 0.8559 - recall_8: 0.9192 - specificity: 0.7934 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8953 - val_loss: 0.3433 - val_f1: 0.9371 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8817 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8817\n",
            "Epoch 538/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3038 - f1: 0.8570 - auc: 0.9471 - precision_8: 0.8724 - recall_8: 0.8466 - specificity: 0.8314 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8768 - val_loss: 0.2863 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 539/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3041 - f1: 0.8859 - auc: 0.9475 - precision_8: 0.8477 - recall_8: 0.9287 - specificity: 0.7794 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8941 - val_loss: 0.3029 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 540/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2981 - f1: 0.8702 - auc: 0.9486 - precision_8: 0.8697 - recall_8: 0.8802 - specificity: 0.8162 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8882 - val_loss: 0.4036 - val_f1: 0.8929 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8065 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8065\n",
            "Epoch 541/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3058 - f1: 0.8649 - auc: 0.9451 - precision_8: 0.8695 - recall_8: 0.8520 - specificity: 0.8181 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8774 - val_loss: 0.2570 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 542/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3005 - f1: 0.8837 - auc: 0.9485 - precision_8: 0.8462 - recall_8: 0.9260 - specificity: 0.7910 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8923 - val_loss: 0.3796 - val_f1: 0.9027 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8226 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8226\n",
            "Epoch 543/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3020 - f1: 0.8571 - auc: 0.9479 - precision_8: 0.8748 - recall_8: 0.8371 - specificity: 0.8370 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8744 - val_loss: 0.2720 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 544/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3003 - f1: 0.8851 - auc: 0.9487 - precision_8: 0.8475 - recall_8: 0.9273 - specificity: 0.7882 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8935 - val_loss: 0.2997 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 545/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3004 - f1: 0.8669 - auc: 0.9469 - precision_8: 0.8662 - recall_8: 0.8802 - specificity: 0.8213 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8864 - val_loss: 0.3248 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 546/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2969 - f1: 0.8868 - auc: 0.9482 - precision_8: 0.8676 - recall_8: 0.9085 - specificity: 0.8075 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8977 - val_loss: 0.2608 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 547/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2988 - f1: 0.8849 - auc: 0.9484 - precision_8: 0.8525 - recall_8: 0.9179 - specificity: 0.7985 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8929 - val_loss: 0.3664 - val_f1: 0.9249 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8602 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8602\n",
            "Epoch 548/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2989 - f1: 0.8670 - auc: 0.9485 - precision_8: 0.8740 - recall_8: 0.8587 - specificity: 0.8316 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8822 - val_loss: 0.2646 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 549/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3009 - f1: 0.8827 - auc: 0.9486 - precision_8: 0.8439 - recall_8: 0.9314 - specificity: 0.7806 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8929 - val_loss: 0.3433 - val_f1: 0.9402 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8871 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8871\n",
            "Epoch 550/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3018 - f1: 0.8600 - auc: 0.9474 - precision_8: 0.8736 - recall_8: 0.8466 - specificity: 0.8381 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8774 - val_loss: 0.3034 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 551/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2959 - f1: 0.8911 - auc: 0.9493 - precision_8: 0.8573 - recall_8: 0.9219 - specificity: 0.7985 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8971 - val_loss: 0.2455 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 552/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3037 - f1: 0.8717 - auc: 0.9456 - precision_8: 0.8413 - recall_8: 0.9058 - specificity: 0.8071 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 0.9987 - accuracy: 0.8822 - val_loss: 0.3434 - val_f1: 0.9402 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8871 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8871\n",
            "Epoch 553/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2975 - f1: 0.8765 - auc: 0.9477 - precision_8: 0.8626 - recall_8: 0.8869 - specificity: 0.8075 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8870 - val_loss: 0.2783 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 554/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2956 - f1: 0.8887 - auc: 0.9489 - precision_8: 0.8658 - recall_8: 0.9206 - specificity: 0.8083 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9013 - val_loss: 0.3443 - val_f1: 0.9402 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8871 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8871\n",
            "Epoch 555/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2961 - f1: 0.8782 - auc: 0.9483 - precision_8: 0.8688 - recall_8: 0.8910 - specificity: 0.8180 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8917 - val_loss: 0.2901 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 556/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2947 - f1: 0.8899 - auc: 0.9492 - precision_8: 0.8640 - recall_8: 0.9152 - specificity: 0.8116 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8983 - val_loss: 0.3103 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 557/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2941 - f1: 0.8901 - auc: 0.9493 - precision_8: 0.8678 - recall_8: 0.9098 - specificity: 0.8113 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8983 - val_loss: 0.3056 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 558/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2938 - f1: 0.8881 - auc: 0.9494 - precision_8: 0.8678 - recall_8: 0.9098 - specificity: 0.8113 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8983 - val_loss: 0.3197 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 559/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2940 - f1: 0.8831 - auc: 0.9493 - precision_8: 0.8702 - recall_8: 0.8937 - specificity: 0.8218 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8935 - val_loss: 0.2843 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 560/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2947 - f1: 0.8917 - auc: 0.9497 - precision_8: 0.8547 - recall_8: 0.9260 - specificity: 0.7988 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8971 - val_loss: 0.3214 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 561/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2932 - f1: 0.8833 - auc: 0.9495 - precision_8: 0.8716 - recall_8: 0.8950 - specificity: 0.8224 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8947 - val_loss: 0.3325 - val_f1: 0.9521 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9086 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9086\n",
            "Epoch 562/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2932 - f1: 0.8869 - auc: 0.9495 - precision_8: 0.8700 - recall_8: 0.9004 - specificity: 0.8162 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8959 - val_loss: 0.2722 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 563/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2931 - f1: 0.8865 - auc: 0.9499 - precision_8: 0.8596 - recall_8: 0.9233 - specificity: 0.8045 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8989 - val_loss: 0.3335 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 564/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2930 - f1: 0.8815 - auc: 0.9497 - precision_8: 0.8684 - recall_8: 0.8883 - specificity: 0.8209 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8906 - val_loss: 0.3008 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 565/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2918 - f1: 0.8881 - auc: 0.9499 - precision_8: 0.8657 - recall_8: 0.9112 - specificity: 0.8131 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8977 - val_loss: 0.3046 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 566/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2916 - f1: 0.8883 - auc: 0.9499 - precision_8: 0.8697 - recall_8: 0.9071 - specificity: 0.8185 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8983 - val_loss: 0.3003 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 567/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2917 - f1: 0.8891 - auc: 0.9498 - precision_8: 0.8629 - recall_8: 0.9152 - specificity: 0.8068 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8977 - val_loss: 0.3101 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 568/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2910 - f1: 0.8879 - auc: 0.9499 - precision_8: 0.8693 - recall_8: 0.9044 - specificity: 0.8208 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8971 - val_loss: 0.3157 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 569/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2908 - f1: 0.8871 - auc: 0.9502 - precision_8: 0.8706 - recall_8: 0.9058 - specificity: 0.8195 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8983 - val_loss: 0.2646 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 570/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2924 - f1: 0.8914 - auc: 0.9496 - precision_8: 0.8613 - recall_8: 0.9192 - specificity: 0.8086 - specificity_at_sensitivity_8: 0.9688 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8983 - val_loss: 0.2942 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 571/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2903 - f1: 0.8908 - auc: 0.9502 - precision_8: 0.8655 - recall_8: 0.9179 - specificity: 0.8054 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9001 - val_loss: 0.3241 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 572/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2905 - f1: 0.8780 - auc: 0.9502 - precision_8: 0.8702 - recall_8: 0.8937 - specificity: 0.8227 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8935 - val_loss: 0.2855 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 573/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2916 - f1: 0.8845 - auc: 0.9500 - precision_8: 0.8536 - recall_8: 0.9260 - specificity: 0.8004 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8965 - val_loss: 0.3228 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 574/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2901 - f1: 0.8786 - auc: 0.9503 - precision_8: 0.8728 - recall_8: 0.8869 - specificity: 0.8289 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8923 - val_loss: 0.3188 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 575/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2888 - f1: 0.8904 - auc: 0.9506 - precision_8: 0.8687 - recall_8: 0.9085 - specificity: 0.8187 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8983 - val_loss: 0.2555 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 576/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2904 - f1: 0.8915 - auc: 0.9511 - precision_8: 0.8584 - recall_8: 0.9300 - specificity: 0.7991 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9007 - val_loss: 0.3436 - val_f1: 0.9462 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8978 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8978\n",
            "Epoch 577/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2926 - f1: 0.8677 - auc: 0.9501 - precision_8: 0.8779 - recall_8: 0.8614 - specificity: 0.8379 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8852 - val_loss: 0.2389 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 578/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2934 - f1: 0.8892 - auc: 0.9511 - precision_8: 0.8432 - recall_8: 0.9408 - specificity: 0.7891 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8959 - val_loss: 0.2835 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 579/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2889 - f1: 0.8923 - auc: 0.9503 - precision_8: 0.8719 - recall_8: 0.9071 - specificity: 0.8244 - specificity_at_sensitivity_8: 0.9677 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8995 - val_loss: 0.3348 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 580/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2888 - f1: 0.8853 - auc: 0.9504 - precision_8: 0.8677 - recall_8: 0.9004 - specificity: 0.8182 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8947 - val_loss: 0.2556 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 581/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2899 - f1: 0.8907 - auc: 0.9502 - precision_8: 0.8562 - recall_8: 0.9219 - specificity: 0.8115 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8965 - val_loss: 0.3216 - val_f1: 0.9521 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9086 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9086\n",
            "Epoch 582/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2874 - f1: 0.8847 - auc: 0.9510 - precision_8: 0.8708 - recall_8: 0.8977 - specificity: 0.8272 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8953 - val_loss: 0.2574 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 583/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2887 - f1: 0.8937 - auc: 0.9513 - precision_8: 0.8593 - recall_8: 0.9287 - specificity: 0.7995 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9007 - val_loss: 0.3213 - val_f1: 0.9521 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9086 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9086\n",
            "Epoch 584/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2875 - f1: 0.8800 - auc: 0.9512 - precision_8: 0.8722 - recall_8: 0.8910 - specificity: 0.8301 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8935 - val_loss: 0.2965 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 585/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2865 - f1: 0.8887 - auc: 0.9514 - precision_8: 0.8642 - recall_8: 0.9166 - specificity: 0.8103 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8989 - val_loss: 0.2794 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 586/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2865 - f1: 0.8865 - auc: 0.9512 - precision_8: 0.8661 - recall_8: 0.9139 - specificity: 0.8182 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8989 - val_loss: 0.3127 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 587/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2860 - f1: 0.8893 - auc: 0.9513 - precision_8: 0.8681 - recall_8: 0.9125 - specificity: 0.8189 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8995 - val_loss: 0.2747 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 588/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2862 - f1: 0.8902 - auc: 0.9515 - precision_8: 0.8650 - recall_8: 0.9139 - specificity: 0.8155 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8983 - val_loss: 0.3003 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 589/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2861 - f1: 0.8905 - auc: 0.9513 - precision_8: 0.8655 - recall_8: 0.9179 - specificity: 0.8089 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9001 - val_loss: 0.3067 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 590/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2855 - f1: 0.8848 - auc: 0.9518 - precision_8: 0.8709 - recall_8: 0.8991 - specificity: 0.8271 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8959 - val_loss: 0.2910 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 591/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2865 - f1: 0.8952 - auc: 0.9516 - precision_8: 0.8640 - recall_8: 0.9233 - specificity: 0.8068 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9013 - val_loss: 0.3031 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 592/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2849 - f1: 0.8835 - auc: 0.9519 - precision_8: 0.8726 - recall_8: 0.9031 - specificity: 0.8297 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8983 - val_loss: 0.3117 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 593/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2847 - f1: 0.8955 - auc: 0.9516 - precision_8: 0.8699 - recall_8: 0.9179 - specificity: 0.8165 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9025 - val_loss: 0.2672 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 594/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2860 - f1: 0.8901 - auc: 0.9509 - precision_8: 0.8668 - recall_8: 0.9112 - specificity: 0.8227 - specificity_at_sensitivity_8: 0.9688 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8983 - val_loss: 0.3002 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 595/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2845 - f1: 0.8862 - auc: 0.9522 - precision_8: 0.8626 - recall_8: 0.9125 - specificity: 0.8145 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8965 - val_loss: 0.2810 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 596/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2843 - f1: 0.8896 - auc: 0.9515 - precision_8: 0.8657 - recall_8: 0.9112 - specificity: 0.8257 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8977 - val_loss: 0.2999 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 597/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2830 - f1: 0.8962 - auc: 0.9525 - precision_8: 0.8686 - recall_8: 0.9166 - specificity: 0.8202 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9013 - val_loss: 0.2656 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 598/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2829 - f1: 0.8928 - auc: 0.9526 - precision_8: 0.8654 - recall_8: 0.9260 - specificity: 0.8128 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9031 - val_loss: 0.3349 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 599/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2843 - f1: 0.8869 - auc: 0.9519 - precision_8: 0.8743 - recall_8: 0.8991 - specificity: 0.8289 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8977 - val_loss: 0.2746 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 600/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2828 - f1: 0.8919 - auc: 0.9525 - precision_8: 0.8622 - recall_8: 0.9260 - specificity: 0.8115 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9013 - val_loss: 0.3241 - val_f1: 0.9521 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9086 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9086\n",
            "Epoch 601/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2855 - f1: 0.8792 - auc: 0.9520 - precision_8: 0.8770 - recall_8: 0.8829 - specificity: 0.8420 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8929 - val_loss: 0.2407 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 602/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2883 - f1: 0.8881 - auc: 0.9527 - precision_8: 0.8422 - recall_8: 0.9408 - specificity: 0.7856 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8953 - val_loss: 0.3174 - val_f1: 0.9521 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9086 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9086\n",
            "Epoch 603/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2834 - f1: 0.8784 - auc: 0.9528 - precision_8: 0.8758 - recall_8: 0.8829 - specificity: 0.8411 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8923 - val_loss: 0.3249 - val_f1: 0.9521 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9086 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9086\n",
            "Epoch 604/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2840 - f1: 0.8861 - auc: 0.9515 - precision_8: 0.8659 - recall_8: 0.9125 - specificity: 0.8119 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8983 - val_loss: 0.2526 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 605/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2827 - f1: 0.8918 - auc: 0.9529 - precision_8: 0.8660 - recall_8: 0.9219 - specificity: 0.8170 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9019 - val_loss: 0.3374 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 606/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2868 - f1: 0.8855 - auc: 0.9506 - precision_8: 0.8698 - recall_8: 0.8991 - specificity: 0.8192 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8953 - val_loss: 0.2745 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 607/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2819 - f1: 0.8929 - auc: 0.9527 - precision_8: 0.8731 - recall_8: 0.9166 - specificity: 0.8254 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9037 - val_loss: 0.3073 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 608/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2810 - f1: 0.8915 - auc: 0.9530 - precision_8: 0.8668 - recall_8: 0.9192 - specificity: 0.8162 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9013 - val_loss: 0.2613 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 609/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2802 - f1: 0.8962 - auc: 0.9535 - precision_8: 0.8665 - recall_8: 0.9260 - specificity: 0.8182 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9037 - val_loss: 0.3322 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 610/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2812 - f1: 0.8877 - auc: 0.9529 - precision_8: 0.8724 - recall_8: 0.9017 - specificity: 0.8303 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8977 - val_loss: 0.2597 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 611/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2799 - f1: 0.8940 - auc: 0.9538 - precision_8: 0.8625 - recall_8: 0.9287 - specificity: 0.8141 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9025 - val_loss: 0.3094 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 612/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2795 - f1: 0.8893 - auc: 0.9536 - precision_8: 0.8729 - recall_8: 0.9058 - specificity: 0.8263 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8995 - val_loss: 0.2838 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 613/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2787 - f1: 0.8942 - auc: 0.9537 - precision_8: 0.8690 - recall_8: 0.9192 - specificity: 0.8224 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9025 - val_loss: 0.2842 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 614/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2791 - f1: 0.8897 - auc: 0.9534 - precision_8: 0.8686 - recall_8: 0.9071 - specificity: 0.8302 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8977 - val_loss: 0.2642 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 615/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2806 - f1: 0.8924 - auc: 0.9537 - precision_8: 0.8580 - recall_8: 0.9354 - specificity: 0.8029 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9025 - val_loss: 0.3243 - val_f1: 0.9521 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9086 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9086\n",
            "Epoch 616/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2803 - f1: 0.8842 - auc: 0.9537 - precision_8: 0.8796 - recall_8: 0.8950 - specificity: 0.8438 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8989 - val_loss: 0.2807 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 617/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2796 - f1: 0.8956 - auc: 0.9538 - precision_8: 0.8625 - recall_8: 0.9287 - specificity: 0.8094 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9025 - val_loss: 0.2768 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 618/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2769 - f1: 0.8843 - auc: 0.9544 - precision_8: 0.8724 - recall_8: 0.9112 - specificity: 0.8262 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9013 - val_loss: 0.3437 - val_f1: 0.9432 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8925 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8925\n",
            "Epoch 619/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2789 - f1: 0.8829 - auc: 0.9538 - precision_8: 0.8777 - recall_8: 0.8883 - specificity: 0.8366 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8953 - val_loss: 0.2252 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 620/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2810 - f1: 0.8941 - auc: 0.9544 - precision_8: 0.8504 - recall_8: 0.9408 - specificity: 0.8003 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9001 - val_loss: 0.3278 - val_f1: 0.9521 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9086 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9086\n",
            "Epoch 621/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2788 - f1: 0.8905 - auc: 0.9542 - precision_8: 0.8775 - recall_8: 0.8964 - specificity: 0.8420 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8983 - val_loss: 0.2679 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 622/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2786 - f1: 0.8960 - auc: 0.9542 - precision_8: 0.8598 - recall_8: 0.9327 - specificity: 0.8072 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9025 - val_loss: 0.2948 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 623/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2766 - f1: 0.8897 - auc: 0.9547 - precision_8: 0.8729 - recall_8: 0.9058 - specificity: 0.8348 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8995 - val_loss: 0.3089 - val_f1: 0.9521 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9086 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9086\n",
            "Epoch 624/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2756 - f1: 0.8906 - auc: 0.9548 - precision_8: 0.8755 - recall_8: 0.9085 - specificity: 0.8289 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9019 - val_loss: 0.2303 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 625/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2793 - f1: 0.8981 - auc: 0.9538 - precision_8: 0.8573 - recall_8: 0.9381 - specificity: 0.8127 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9031 - val_loss: 0.3065 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 626/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2757 - f1: 0.8901 - auc: 0.9548 - precision_8: 0.8742 - recall_8: 0.9071 - specificity: 0.8329 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9007 - val_loss: 0.2593 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 627/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2756 - f1: 0.8968 - auc: 0.9551 - precision_8: 0.8637 - recall_8: 0.9300 - specificity: 0.8185 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9037 - val_loss: 0.2918 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 628/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2762 - f1: 0.8882 - auc: 0.9547 - precision_8: 0.8731 - recall_8: 0.9071 - specificity: 0.8333 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9001 - val_loss: 0.2515 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 629/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2765 - f1: 0.8966 - auc: 0.9553 - precision_8: 0.8605 - recall_8: 0.9381 - specificity: 0.8039 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9049 - val_loss: 0.2970 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 630/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2755 - f1: 0.8936 - auc: 0.9547 - precision_8: 0.8763 - recall_8: 0.9058 - specificity: 0.8403 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9013 - val_loss: 0.2842 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 631/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2789 - f1: 0.8915 - auc: 0.9534 - precision_8: 0.8602 - recall_8: 0.9273 - specificity: 0.8061 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9007 - val_loss: 0.2919 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 632/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2770 - f1: 0.8816 - auc: 0.9551 - precision_8: 0.8831 - recall_8: 0.8950 - specificity: 0.8396 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9007 - val_loss: 0.2826 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 633/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2761 - f1: 0.8915 - auc: 0.9542 - precision_8: 0.8596 - recall_8: 0.9314 - specificity: 0.8046 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9019 - val_loss: 0.2571 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 634/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2738 - f1: 0.8907 - auc: 0.9555 - precision_8: 0.8655 - recall_8: 0.9179 - specificity: 0.8301 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9001 - val_loss: 0.3419 - val_f1: 0.9462 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8978 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8978\n",
            "Epoch 635/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2787 - f1: 0.8917 - auc: 0.9536 - precision_8: 0.8731 - recall_8: 0.9071 - specificity: 0.8270 - specificity_at_sensitivity_8: 0.9699 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9001 - val_loss: 0.2467 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 636/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2740 - f1: 0.8919 - auc: 0.9556 - precision_8: 0.8657 - recall_8: 0.9192 - specificity: 0.8231 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9007 - val_loss: 0.3222 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 637/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2736 - f1: 0.8913 - auc: 0.9556 - precision_8: 0.8745 - recall_8: 0.9098 - specificity: 0.8337 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9019 - val_loss: 0.2393 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 638/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2744 - f1: 0.8955 - auc: 0.9556 - precision_8: 0.8586 - recall_8: 0.9314 - specificity: 0.8198 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9013 - val_loss: 0.2942 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 639/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2721 - f1: 0.8920 - auc: 0.9560 - precision_8: 0.8728 - recall_8: 0.9139 - specificity: 0.8325 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9025 - val_loss: 0.2594 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 640/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2721 - f1: 0.8995 - auc: 0.9561 - precision_8: 0.8679 - recall_8: 0.9287 - specificity: 0.8220 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9055 - val_loss: 0.2778 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 641/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2713 - f1: 0.8940 - auc: 0.9562 - precision_8: 0.8713 - recall_8: 0.9206 - specificity: 0.8274 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9043 - val_loss: 0.2823 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 642/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2725 - f1: 0.8947 - auc: 0.9558 - precision_8: 0.8671 - recall_8: 0.9219 - specificity: 0.8185 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9025 - val_loss: 0.3051 - val_f1: 0.9521 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9086 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9086\n",
            "Epoch 643/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2719 - f1: 0.8908 - auc: 0.9560 - precision_8: 0.8745 - recall_8: 0.9098 - specificity: 0.8349 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9019 - val_loss: 0.2888 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 644/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2706 - f1: 0.8910 - auc: 0.9564 - precision_8: 0.8729 - recall_8: 0.9152 - specificity: 0.8314 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9031 - val_loss: 0.2430 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 645/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2719 - f1: 0.8986 - auc: 0.9563 - precision_8: 0.8639 - recall_8: 0.9314 - specificity: 0.8214 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9043 - val_loss: 0.2803 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 646/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2703 - f1: 0.8958 - auc: 0.9565 - precision_8: 0.8753 - recall_8: 0.9166 - specificity: 0.8352 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9049 - val_loss: 0.2569 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 647/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2708 - f1: 0.8963 - auc: 0.9567 - precision_8: 0.8619 - recall_8: 0.9327 - specificity: 0.8163 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9037 - val_loss: 0.3010 - val_f1: 0.9521 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9086 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9086\n",
            "Epoch 648/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2709 - f1: 0.8891 - auc: 0.9566 - precision_8: 0.8771 - recall_8: 0.9031 - specificity: 0.8431 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9007 - val_loss: 0.2580 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 649/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2702 - f1: 0.8993 - auc: 0.9570 - precision_8: 0.8634 - recall_8: 0.9354 - specificity: 0.8177 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9055 - val_loss: 0.2725 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 650/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2694 - f1: 0.8898 - auc: 0.9568 - precision_8: 0.8755 - recall_8: 0.9085 - specificity: 0.8345 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9019 - val_loss: 0.2871 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 651/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2701 - f1: 0.8939 - auc: 0.9565 - precision_8: 0.8649 - recall_8: 0.9219 - specificity: 0.8219 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9013 - val_loss: 0.2683 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 652/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2687 - f1: 0.8954 - auc: 0.9567 - precision_8: 0.8720 - recall_8: 0.9166 - specificity: 0.8341 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9031 - val_loss: 0.2909 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 653/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2693 - f1: 0.8951 - auc: 0.9565 - precision_8: 0.8724 - recall_8: 0.9206 - specificity: 0.8242 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9049 - val_loss: 0.2614 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 654/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2677 - f1: 0.8979 - auc: 0.9574 - precision_8: 0.8687 - recall_8: 0.9260 - specificity: 0.8273 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9049 - val_loss: 0.3181 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 655/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2690 - f1: 0.8935 - auc: 0.9575 - precision_8: 0.8763 - recall_8: 0.9058 - specificity: 0.8464 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9013 - val_loss: 0.2230 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 656/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2711 - f1: 0.8985 - auc: 0.9578 - precision_8: 0.8545 - recall_8: 0.9489 - specificity: 0.8057 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9055 - val_loss: 0.3371 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 657/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2714 - f1: 0.8886 - auc: 0.9569 - precision_8: 0.8827 - recall_8: 0.8910 - specificity: 0.8467 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8989 - val_loss: 0.2699 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 658/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2670 - f1: 0.8980 - auc: 0.9575 - precision_8: 0.8699 - recall_8: 0.9273 - specificity: 0.8228 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9061 - val_loss: 0.2592 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 659/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2668 - f1: 0.8954 - auc: 0.9576 - precision_8: 0.8706 - recall_8: 0.9233 - specificity: 0.8291 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9049 - val_loss: 0.2893 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 660/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2665 - f1: 0.8993 - auc: 0.9578 - precision_8: 0.8760 - recall_8: 0.9219 - specificity: 0.8357 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9073 - val_loss: 0.2430 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 661/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2670 - f1: 0.9005 - auc: 0.9577 - precision_8: 0.8653 - recall_8: 0.9341 - specificity: 0.8251 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9061 - val_loss: 0.2863 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 662/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2666 - f1: 0.8958 - auc: 0.9579 - precision_8: 0.8764 - recall_8: 0.9166 - specificity: 0.8393 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9055 - val_loss: 0.2323 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 663/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2677 - f1: 0.8974 - auc: 0.9584 - precision_8: 0.8612 - recall_8: 0.9435 - specificity: 0.8103 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9073 - val_loss: 0.2999 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 664/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2666 - f1: 0.8938 - auc: 0.9582 - precision_8: 0.8774 - recall_8: 0.9058 - specificity: 0.8449 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9019 - val_loss: 0.2601 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 665/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2653 - f1: 0.8988 - auc: 0.9585 - precision_8: 0.8683 - recall_8: 0.9314 - specificity: 0.8254 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9067 - val_loss: 0.2590 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 666/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2648 - f1: 0.9014 - auc: 0.9584 - precision_8: 0.8709 - recall_8: 0.9260 - specificity: 0.8301 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9061 - val_loss: 0.3199 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 667/1000\n",
            "2/2 [==============================] - 5s 3s/step - loss: 0.2667 - f1: 0.8901 - auc: 0.9583 - precision_8: 0.8801 - recall_8: 0.8991 - specificity: 0.8483 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9007 - val_loss: 0.2127 - val_f1: 0.9695 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9409 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9409\n",
            "Epoch 668/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2687 - f1: 0.9003 - auc: 0.9581 - precision_8: 0.8545 - recall_8: 0.9489 - specificity: 0.8118 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9055 - val_loss: 0.2868 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 669/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2646 - f1: 0.8960 - auc: 0.9584 - precision_8: 0.8763 - recall_8: 0.9152 - specificity: 0.8407 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9049 - val_loss: 0.2694 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 670/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2638 - f1: 0.9003 - auc: 0.9584 - precision_8: 0.8701 - recall_8: 0.9287 - specificity: 0.8308 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9067 - val_loss: 0.2493 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 671/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2650 - f1: 0.8960 - auc: 0.9580 - precision_8: 0.8684 - recall_8: 0.9233 - specificity: 0.8365 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9037 - val_loss: 0.2615 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 672/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2632 - f1: 0.8999 - auc: 0.9589 - precision_8: 0.8722 - recall_8: 0.9273 - specificity: 0.8308 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9073 - val_loss: 0.2442 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 673/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2635 - f1: 0.8955 - auc: 0.9591 - precision_8: 0.8657 - recall_8: 0.9367 - specificity: 0.8215 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9073 - val_loss: 0.3161 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 674/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2651 - f1: 0.8919 - auc: 0.9588 - precision_8: 0.8827 - recall_8: 0.9017 - specificity: 0.8493 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9031 - val_loss: 0.2213 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 675/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2660 - f1: 0.8969 - auc: 0.9593 - precision_8: 0.8566 - recall_8: 0.9489 - specificity: 0.8101 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9067 - val_loss: 0.3118 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 676/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2664 - f1: 0.8914 - auc: 0.9586 - precision_8: 0.8850 - recall_8: 0.8910 - specificity: 0.8575 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9001 - val_loss: 0.2269 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 677/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2663 - f1: 0.9030 - auc: 0.9594 - precision_8: 0.8611 - recall_8: 0.9515 - specificity: 0.8067 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9103 - val_loss: 0.2829 - val_f1: 0.9521 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9086 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9086\n",
            "Epoch 678/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2667 - f1: 0.8928 - auc: 0.9580 - precision_8: 0.8849 - recall_8: 0.9004 - specificity: 0.8544 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9037 - val_loss: 0.2441 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 679/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2630 - f1: 0.8984 - auc: 0.9595 - precision_8: 0.8600 - recall_8: 0.9421 - specificity: 0.8122 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9061 - val_loss: 0.2350 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 680/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2657 - f1: 0.8992 - auc: 0.9572 - precision_8: 0.8690 - recall_8: 0.9287 - specificity: 0.8392 - specificity_at_sensitivity_8: 0.9709 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9061 - val_loss: 0.2808 - val_f1: 0.9521 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9086 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9086\n",
            "Epoch 681/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2607 - f1: 0.9010 - auc: 0.9594 - precision_8: 0.8712 - recall_8: 0.9287 - specificity: 0.8325 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9073 - val_loss: 0.2153 - val_f1: 0.9695 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9409 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9409\n",
            "Epoch 682/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2633 - f1: 0.8990 - auc: 0.9595 - precision_8: 0.8648 - recall_8: 0.9381 - specificity: 0.8226 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9073 - val_loss: 0.3234 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 683/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2655 - f1: 0.8950 - auc: 0.9581 - precision_8: 0.8801 - recall_8: 0.9085 - specificity: 0.8400 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9043 - val_loss: 0.2478 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 684/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2628 - f1: 0.8945 - auc: 0.9588 - precision_8: 0.8706 - recall_8: 0.9233 - specificity: 0.8401 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9049 - val_loss: 0.2492 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 685/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2606 - f1: 0.8994 - auc: 0.9601 - precision_8: 0.8655 - recall_8: 0.9354 - specificity: 0.8226 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9067 - val_loss: 0.2557 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 686/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2605 - f1: 0.8973 - auc: 0.9591 - precision_8: 0.8747 - recall_8: 0.9206 - specificity: 0.8415 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9061 - val_loss: 0.2695 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 687/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2593 - f1: 0.8984 - auc: 0.9600 - precision_8: 0.8698 - recall_8: 0.9260 - specificity: 0.8339 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9055 - val_loss: 0.2336 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 688/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2603 - f1: 0.8964 - auc: 0.9598 - precision_8: 0.8655 - recall_8: 0.9354 - specificity: 0.8286 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9067 - val_loss: 0.2846 - val_f1: 0.9521 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9086 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9086\n",
            "Epoch 689/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2593 - f1: 0.8955 - auc: 0.9601 - precision_8: 0.8756 - recall_8: 0.9192 - specificity: 0.8420 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9061 - val_loss: 0.2250 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 690/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2604 - f1: 0.8984 - auc: 0.9606 - precision_8: 0.8612 - recall_8: 0.9435 - specificity: 0.8198 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9073 - val_loss: 0.3007 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 691/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2603 - f1: 0.8960 - auc: 0.9598 - precision_8: 0.8795 - recall_8: 0.9139 - specificity: 0.8425 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9061 - val_loss: 0.2637 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 692/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2585 - f1: 0.8951 - auc: 0.9604 - precision_8: 0.8690 - recall_8: 0.9287 - specificity: 0.8306 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9061 - val_loss: 0.2639 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 693/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2588 - f1: 0.9034 - auc: 0.9602 - precision_8: 0.8730 - recall_8: 0.9341 - specificity: 0.8307 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9103 - val_loss: 0.3023 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 694/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2596 - f1: 0.8945 - auc: 0.9601 - precision_8: 0.8809 - recall_8: 0.9058 - specificity: 0.8444 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9037 - val_loss: 0.2504 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 695/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2580 - f1: 0.8988 - auc: 0.9608 - precision_8: 0.8652 - recall_8: 0.9327 - specificity: 0.8265 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9055 - val_loss: 0.2867 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 696/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2582 - f1: 0.8979 - auc: 0.9607 - precision_8: 0.8818 - recall_8: 0.9139 - specificity: 0.8445 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9073 - val_loss: 0.2406 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 697/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2583 - f1: 0.8955 - auc: 0.9610 - precision_8: 0.8610 - recall_8: 0.9421 - specificity: 0.8212 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9067 - val_loss: 0.2850 - val_f1: 0.9521 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9086 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9086\n",
            "Epoch 698/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2584 - f1: 0.8985 - auc: 0.9606 - precision_8: 0.8848 - recall_8: 0.9098 - specificity: 0.8518 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9073 - val_loss: 0.2371 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 699/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2572 - f1: 0.8998 - auc: 0.9611 - precision_8: 0.8628 - recall_8: 0.9394 - specificity: 0.8254 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9067 - val_loss: 0.2447 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 700/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2564 - f1: 0.8982 - auc: 0.9610 - precision_8: 0.8720 - recall_8: 0.9260 - specificity: 0.8355 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9067 - val_loss: 0.2824 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 701/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2573 - f1: 0.8989 - auc: 0.9606 - precision_8: 0.8734 - recall_8: 0.9287 - specificity: 0.8345 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9085 - val_loss: 0.2526 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 702/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2566 - f1: 0.9011 - auc: 0.9607 - precision_8: 0.8742 - recall_8: 0.9260 - specificity: 0.8413 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9079 - val_loss: 0.2501 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 703/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2594 - f1: 0.8963 - auc: 0.9601 - precision_8: 0.8603 - recall_8: 0.9367 - specificity: 0.8188 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9043 - val_loss: 0.3001 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 704/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2575 - f1: 0.8965 - auc: 0.9609 - precision_8: 0.8889 - recall_8: 0.9044 - specificity: 0.8488 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9073 - val_loss: 0.2731 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 705/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2572 - f1: 0.9031 - auc: 0.9604 - precision_8: 0.8756 - recall_8: 0.9287 - specificity: 0.8293 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9097 - val_loss: 0.2514 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 706/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2543 - f1: 0.8996 - auc: 0.9614 - precision_8: 0.8731 - recall_8: 0.9260 - specificity: 0.8389 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9073 - val_loss: 0.3061 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 707/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2569 - f1: 0.8943 - auc: 0.9610 - precision_8: 0.8792 - recall_8: 0.9112 - specificity: 0.8439 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9049 - val_loss: 0.2265 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 708/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2550 - f1: 0.9039 - auc: 0.9617 - precision_8: 0.8672 - recall_8: 0.9408 - specificity: 0.8293 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9097 - val_loss: 0.2945 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 709/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2554 - f1: 0.9009 - auc: 0.9613 - precision_8: 0.8843 - recall_8: 0.9152 - specificity: 0.8462 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9091 - val_loss: 0.2315 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 710/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2543 - f1: 0.9014 - auc: 0.9617 - precision_8: 0.8628 - recall_8: 0.9394 - specificity: 0.8312 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9067 - val_loss: 0.2834 - val_f1: 0.9521 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9086 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9086\n",
            "Epoch 711/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2547 - f1: 0.9005 - auc: 0.9612 - precision_8: 0.8834 - recall_8: 0.9179 - specificity: 0.8430 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9097 - val_loss: 0.2549 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 712/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2540 - f1: 0.8991 - auc: 0.9614 - precision_8: 0.8792 - recall_8: 0.9206 - specificity: 0.8436 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9085 - val_loss: 0.2271 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 713/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2540 - f1: 0.9001 - auc: 0.9620 - precision_8: 0.8639 - recall_8: 0.9394 - specificity: 0.8288 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9073 - val_loss: 0.2609 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 714/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2530 - f1: 0.9077 - auc: 0.9620 - precision_8: 0.8824 - recall_8: 0.9287 - specificity: 0.8454 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9133 - val_loss: 0.2490 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 715/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2538 - f1: 0.9002 - auc: 0.9616 - precision_8: 0.8744 - recall_8: 0.9273 - specificity: 0.8438 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9085 - val_loss: 0.2091 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 716/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2552 - f1: 0.9015 - auc: 0.9625 - precision_8: 0.8591 - recall_8: 0.9515 - specificity: 0.8181 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9091 - val_loss: 0.3086 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 717/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2551 - f1: 0.8992 - auc: 0.9617 - precision_8: 0.8924 - recall_8: 0.9044 - specificity: 0.8551 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9091 - val_loss: 0.2354 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 718/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2522 - f1: 0.9040 - auc: 0.9623 - precision_8: 0.8672 - recall_8: 0.9408 - specificity: 0.8308 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9097 - val_loss: 0.2566 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 719/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2522 - f1: 0.8993 - auc: 0.9621 - precision_8: 0.8816 - recall_8: 0.9219 - specificity: 0.8457 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9103 - val_loss: 0.2385 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 720/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2518 - f1: 0.9006 - auc: 0.9624 - precision_8: 0.8651 - recall_8: 0.9408 - specificity: 0.8310 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9085 - val_loss: 0.2577 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 721/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2511 - f1: 0.9010 - auc: 0.9625 - precision_8: 0.8806 - recall_8: 0.9233 - specificity: 0.8450 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9103 - val_loss: 0.2640 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 722/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2511 - f1: 0.9045 - auc: 0.9624 - precision_8: 0.8821 - recall_8: 0.9260 - specificity: 0.8482 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9121 - val_loss: 0.2054 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 723/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2532 - f1: 0.9001 - auc: 0.9628 - precision_8: 0.8570 - recall_8: 0.9515 - specificity: 0.8235 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9079 - val_loss: 0.2997 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 724/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2528 - f1: 0.8984 - auc: 0.9625 - precision_8: 0.8948 - recall_8: 0.9044 - specificity: 0.8597 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9103 - val_loss: 0.2029 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 725/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2554 - f1: 0.9043 - auc: 0.9628 - precision_8: 0.8571 - recall_8: 0.9610 - specificity: 0.8112 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9115 - val_loss: 0.3267 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 726/1000\n",
            "2/2 [==============================] - 5s 3s/step - loss: 0.2604 - f1: 0.8924 - auc: 0.9619 - precision_8: 0.9034 - recall_8: 0.8816 - specificity: 0.8737 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9055 - val_loss: 0.1677 - val_f1: 0.9808 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9624 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9624\n",
            "Epoch 727/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2658 - f1: 0.8969 - auc: 0.9633 - precision_8: 0.8299 - recall_8: 0.9717 - specificity: 0.7897 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.8989 - val_loss: 0.2969 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 728/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2567 - f1: 0.8963 - auc: 0.9622 - precision_8: 0.9042 - recall_8: 0.8896 - specificity: 0.8698 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9091 - val_loss: 0.2438 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 729/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2543 - f1: 0.8982 - auc: 0.9615 - precision_8: 0.8582 - recall_8: 0.9448 - specificity: 0.8196 - specificity_at_sensitivity_8: 0.9731 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9061 - val_loss: 0.2177 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 730/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2525 - f1: 0.9059 - auc: 0.9612 - precision_8: 0.8782 - recall_8: 0.9314 - specificity: 0.8492 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9121 - val_loss: 0.3078 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 731/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2524 - f1: 0.9019 - auc: 0.9616 - precision_8: 0.8837 - recall_8: 0.9206 - specificity: 0.8430 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9109 - val_loss: 0.1914 - val_f1: 0.9695 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9409 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9409\n",
            "Epoch 732/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2514 - f1: 0.9074 - auc: 0.9632 - precision_8: 0.8655 - recall_8: 0.9529 - specificity: 0.8255 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9133 - val_loss: 0.3575 - val_f1: 0.9371 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8817 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8817\n",
            "Epoch 733/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2596 - f1: 0.8930 - auc: 0.9602 - precision_8: 0.8925 - recall_8: 0.8937 - specificity: 0.8583 - specificity_at_sensitivity_8: 0.9742 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9049 - val_loss: 0.2054 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 734/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2509 - f1: 0.9061 - auc: 0.9625 - precision_8: 0.8677 - recall_8: 0.9448 - specificity: 0.8336 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9115 - val_loss: 0.2778 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 735/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2486 - f1: 0.9044 - auc: 0.9630 - precision_8: 0.8873 - recall_8: 0.9219 - specificity: 0.8494 - specificity_at_sensitivity_8: 0.9785 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9133 - val_loss: 0.2231 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 736/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2485 - f1: 0.9032 - auc: 0.9633 - precision_8: 0.8683 - recall_8: 0.9408 - specificity: 0.8311 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9103 - val_loss: 0.2665 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 737/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2500 - f1: 0.9040 - auc: 0.9628 - precision_8: 0.8903 - recall_8: 0.9179 - specificity: 0.8571 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9133 - val_loss: 0.2048 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 738/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2507 - f1: 0.9077 - auc: 0.9636 - precision_8: 0.8590 - recall_8: 0.9596 - specificity: 0.8206 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9121 - val_loss: 0.2592 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 739/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2509 - f1: 0.9079 - auc: 0.9623 - precision_8: 0.8934 - recall_8: 0.9139 - specificity: 0.8617 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9133 - val_loss: 0.2233 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 740/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2485 - f1: 0.9044 - auc: 0.9638 - precision_8: 0.8591 - recall_8: 0.9515 - specificity: 0.8243 - specificity_at_sensitivity_8: 0.9785 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9091 - val_loss: 0.2301 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 741/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2486 - f1: 0.9038 - auc: 0.9624 - precision_8: 0.8842 - recall_8: 0.9246 - specificity: 0.8498 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9127 - val_loss: 0.2567 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 742/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2494 - f1: 0.9062 - auc: 0.9623 - precision_8: 0.8747 - recall_8: 0.9394 - specificity: 0.8327 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9133 - val_loss: 0.2358 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 743/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2489 - f1: 0.8992 - auc: 0.9627 - precision_8: 0.8796 - recall_8: 0.9246 - specificity: 0.8533 - specificity_at_sensitivity_8: 0.9785 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9103 - val_loss: 0.2527 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 744/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2500 - f1: 0.9039 - auc: 0.9622 - precision_8: 0.8728 - recall_8: 0.9421 - specificity: 0.8261 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9133 - val_loss: 0.2440 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 745/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2471 - f1: 0.9024 - auc: 0.9637 - precision_8: 0.8887 - recall_8: 0.9246 - specificity: 0.8543 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9151 - val_loss: 0.2655 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 746/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2454 - f1: 0.9067 - auc: 0.9638 - precision_8: 0.8799 - recall_8: 0.9367 - specificity: 0.8411 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9151 - val_loss: 0.2013 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 747/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2473 - f1: 0.9098 - auc: 0.9638 - precision_8: 0.8675 - recall_8: 0.9515 - specificity: 0.8363 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9139 - val_loss: 0.2983 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 748/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2464 - f1: 0.9070 - auc: 0.9642 - precision_8: 0.8993 - recall_8: 0.9139 - specificity: 0.8594 - specificity_at_sensitivity_8: 0.9785 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9163 - val_loss: 0.1854 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 749/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2498 - f1: 0.9065 - auc: 0.9644 - precision_8: 0.8590 - recall_8: 0.9596 - specificity: 0.8196 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9121 - val_loss: 0.2909 - val_f1: 0.9521 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9086 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9086\n",
            "Epoch 750/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2467 - f1: 0.9002 - auc: 0.9639 - precision_8: 0.8933 - recall_8: 0.9125 - specificity: 0.8563 - specificity_at_sensitivity_8: 0.9785 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9127 - val_loss: 0.2387 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 751/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2440 - f1: 0.9063 - auc: 0.9644 - precision_8: 0.8736 - recall_8: 0.9394 - specificity: 0.8407 - specificity_at_sensitivity_8: 0.9785 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9127 - val_loss: 0.2274 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 752/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2439 - f1: 0.9035 - auc: 0.9645 - precision_8: 0.8716 - recall_8: 0.9408 - specificity: 0.8393 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9121 - val_loss: 0.2806 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 753/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2449 - f1: 0.9094 - auc: 0.9643 - precision_8: 0.8962 - recall_8: 0.9179 - specificity: 0.8568 - specificity_at_sensitivity_8: 0.9785 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9163 - val_loss: 0.2190 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 754/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2450 - f1: 0.9028 - auc: 0.9643 - precision_8: 0.8640 - recall_8: 0.9489 - specificity: 0.8264 - specificity_at_sensitivity_8: 0.9785 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9109 - val_loss: 0.2885 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 755/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2456 - f1: 0.9077 - auc: 0.9639 - precision_8: 0.9008 - recall_8: 0.9166 - specificity: 0.8558 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9181 - val_loss: 0.2409 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 756/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2430 - f1: 0.9096 - auc: 0.9645 - precision_8: 0.8809 - recall_8: 0.9354 - specificity: 0.8480 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9151 - val_loss: 0.2128 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 757/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2437 - f1: 0.9070 - auc: 0.9651 - precision_8: 0.8684 - recall_8: 0.9502 - specificity: 0.8330 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9139 - val_loss: 0.2762 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 758/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2437 - f1: 0.9107 - auc: 0.9648 - precision_8: 0.8988 - recall_8: 0.9206 - specificity: 0.8579 - specificity_at_sensitivity_8: 0.9785 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9187 - val_loss: 0.2158 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 759/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2482 - f1: 0.9006 - auc: 0.9643 - precision_8: 0.8577 - recall_8: 0.9569 - specificity: 0.8193 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9103 - val_loss: 0.3222 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 760/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2520 - f1: 0.9028 - auc: 0.9642 - precision_8: 0.9120 - recall_8: 0.8923 - specificity: 0.8785 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9139 - val_loss: 0.1808 - val_f1: 0.9752 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9516 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9516\n",
            "Epoch 761/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2518 - f1: 0.9028 - auc: 0.9655 - precision_8: 0.8445 - recall_8: 0.9650 - specificity: 0.8070 - specificity_at_sensitivity_8: 0.9785 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9055 - val_loss: 0.2551 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 762/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2504 - f1: 0.8931 - auc: 0.9628 - precision_8: 0.8950 - recall_8: 0.8950 - specificity: 0.8696 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9067 - val_loss: 0.2258 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 763/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2490 - f1: 0.8996 - auc: 0.9635 - precision_8: 0.8540 - recall_8: 0.9529 - specificity: 0.8169 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9067 - val_loss: 0.2113 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 764/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2460 - f1: 0.9027 - auc: 0.9631 - precision_8: 0.8811 - recall_8: 0.9273 - specificity: 0.8563 - specificity_at_sensitivity_8: 0.9720 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9121 - val_loss: 0.2993 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 765/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2493 - f1: 0.9066 - auc: 0.9623 - precision_8: 0.8858 - recall_8: 0.9287 - specificity: 0.8379 - specificity_at_sensitivity_8: 0.9763 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9151 - val_loss: 0.1939 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 766/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2444 - f1: 0.9030 - auc: 0.9642 - precision_8: 0.8747 - recall_8: 0.9394 - specificity: 0.8428 - specificity_at_sensitivity_8: 0.9785 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9133 - val_loss: 0.3129 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 767/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2474 - f1: 0.9020 - auc: 0.9627 - precision_8: 0.8849 - recall_8: 0.9206 - specificity: 0.8469 - specificity_at_sensitivity_8: 0.9752 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9115 - val_loss: 0.1982 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 768/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2426 - f1: 0.9068 - auc: 0.9652 - precision_8: 0.8724 - recall_8: 0.9475 - specificity: 0.8369 - specificity_at_sensitivity_8: 0.9785 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9151 - val_loss: 0.2904 - val_f1: 0.9521 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9086 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9086\n",
            "Epoch 769/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2426 - f1: 0.9047 - auc: 0.9650 - precision_8: 0.8916 - recall_8: 0.9192 - specificity: 0.8568 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9145 - val_loss: 0.2085 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 770/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2413 - f1: 0.9112 - auc: 0.9654 - precision_8: 0.8695 - recall_8: 0.9502 - specificity: 0.8394 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9145 - val_loss: 0.2558 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 771/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2400 - f1: 0.9090 - auc: 0.9654 - precision_8: 0.8926 - recall_8: 0.9287 - specificity: 0.8508 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9187 - val_loss: 0.2353 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 772/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2395 - f1: 0.9079 - auc: 0.9656 - precision_8: 0.8824 - recall_8: 0.9394 - specificity: 0.8463 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9175 - val_loss: 0.2399 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 773/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2394 - f1: 0.9097 - auc: 0.9657 - precision_8: 0.8897 - recall_8: 0.9341 - specificity: 0.8496 - specificity_at_sensitivity_8: 0.9785 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9193 - val_loss: 0.2288 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 774/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2399 - f1: 0.9098 - auc: 0.9655 - precision_8: 0.8810 - recall_8: 0.9367 - specificity: 0.8502 - specificity_at_sensitivity_8: 0.9785 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9157 - val_loss: 0.2110 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 775/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2398 - f1: 0.9055 - auc: 0.9660 - precision_8: 0.8654 - recall_8: 0.9515 - specificity: 0.8360 - specificity_at_sensitivity_8: 0.9785 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9127 - val_loss: 0.2558 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 776/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2391 - f1: 0.9142 - auc: 0.9658 - precision_8: 0.8995 - recall_8: 0.9273 - specificity: 0.8547 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9217 - val_loss: 0.2313 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 777/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2384 - f1: 0.9075 - auc: 0.9660 - precision_8: 0.8781 - recall_8: 0.9408 - specificity: 0.8434 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9157 - val_loss: 0.2248 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 778/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2391 - f1: 0.9135 - auc: 0.9659 - precision_8: 0.8878 - recall_8: 0.9367 - specificity: 0.8493 - specificity_at_sensitivity_8: 0.9785 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9193 - val_loss: 0.2316 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 779/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2379 - f1: 0.9057 - auc: 0.9661 - precision_8: 0.8781 - recall_8: 0.9408 - specificity: 0.8423 - specificity_at_sensitivity_8: 0.9785 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9157 - val_loss: 0.2252 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 780/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2378 - f1: 0.9073 - auc: 0.9663 - precision_8: 0.8820 - recall_8: 0.9354 - specificity: 0.8460 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9157 - val_loss: 0.2532 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 781/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2376 - f1: 0.9139 - auc: 0.9664 - precision_8: 0.8961 - recall_8: 0.9287 - specificity: 0.8551 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9205 - val_loss: 0.2110 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 782/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2380 - f1: 0.9096 - auc: 0.9665 - precision_8: 0.8684 - recall_8: 0.9502 - specificity: 0.8393 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9139 - val_loss: 0.2570 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 783/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2376 - f1: 0.9167 - auc: 0.9663 - precision_8: 0.9018 - recall_8: 0.9273 - specificity: 0.8591 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9228 - val_loss: 0.2197 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 784/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2380 - f1: 0.9037 - auc: 0.9666 - precision_8: 0.8710 - recall_8: 0.9448 - specificity: 0.8370 - specificity_at_sensitivity_8: 0.9785 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9133 - val_loss: 0.2593 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 785/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2378 - f1: 0.9123 - auc: 0.9663 - precision_8: 0.9036 - recall_8: 0.9206 - specificity: 0.8599 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9211 - val_loss: 0.2246 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 786/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2389 - f1: 0.9054 - auc: 0.9661 - precision_8: 0.8710 - recall_8: 0.9448 - specificity: 0.8301 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9133 - val_loss: 0.2569 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 787/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2374 - f1: 0.9102 - auc: 0.9666 - precision_8: 0.9014 - recall_8: 0.9233 - specificity: 0.8627 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9211 - val_loss: 0.2394 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 788/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2386 - f1: 0.9051 - auc: 0.9659 - precision_8: 0.8748 - recall_8: 0.9408 - specificity: 0.8341 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9139 - val_loss: 0.2372 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 789/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2356 - f1: 0.9140 - auc: 0.9668 - precision_8: 0.8973 - recall_8: 0.9287 - specificity: 0.8566 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9211 - val_loss: 0.2694 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 790/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2366 - f1: 0.9155 - auc: 0.9667 - precision_8: 0.8953 - recall_8: 0.9327 - specificity: 0.8534 - specificity_at_sensitivity_8: 0.9785 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9217 - val_loss: 0.1974 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 791/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2363 - f1: 0.9071 - auc: 0.9672 - precision_8: 0.8710 - recall_8: 0.9542 - specificity: 0.8356 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9169 - val_loss: 0.3080 - val_f1: 0.9521 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9086 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9086\n",
            "Epoch 792/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2396 - f1: 0.9090 - auc: 0.9667 - precision_8: 0.9081 - recall_8: 0.9044 - specificity: 0.8717 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9169 - val_loss: 0.1776 - val_f1: 0.9752 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9516 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9516\n",
            "Epoch 793/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2405 - f1: 0.9075 - auc: 0.9662 - precision_8: 0.8577 - recall_8: 0.9569 - specificity: 0.8331 - specificity_at_sensitivity_8: 0.9785 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9103 - val_loss: 0.2388 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 794/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2352 - f1: 0.9139 - auc: 0.9667 - precision_8: 0.8938 - recall_8: 0.9287 - specificity: 0.8589 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9193 - val_loss: 0.2244 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 795/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2344 - f1: 0.9126 - auc: 0.9670 - precision_8: 0.8837 - recall_8: 0.9408 - specificity: 0.8486 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9187 - val_loss: 0.2089 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 796/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2368 - f1: 0.9020 - auc: 0.9669 - precision_8: 0.8671 - recall_8: 0.9569 - specificity: 0.8320 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9157 - val_loss: 0.3188 - val_f1: 0.9432 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.8925 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.8925\n",
            "Epoch 797/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2412 - f1: 0.9086 - auc: 0.9665 - precision_8: 0.9081 - recall_8: 0.9044 - specificity: 0.8713 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9169 - val_loss: 0.2004 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 798/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2359 - f1: 0.9115 - auc: 0.9675 - precision_8: 0.8655 - recall_8: 0.9610 - specificity: 0.8323 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9163 - val_loss: 0.2471 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 799/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2338 - f1: 0.9145 - auc: 0.9674 - precision_8: 0.9013 - recall_8: 0.9341 - specificity: 0.8549 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9252 - val_loss: 0.2650 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 800/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2338 - f1: 0.9166 - auc: 0.9673 - precision_8: 0.9043 - recall_8: 0.9287 - specificity: 0.8603 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9246 - val_loss: 0.1819 - val_f1: 0.9695 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9409 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9409\n",
            "Epoch 801/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2374 - f1: 0.9087 - auc: 0.9670 - precision_8: 0.8654 - recall_8: 0.9515 - specificity: 0.8380 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9127 - val_loss: 0.2463 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 802/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2336 - f1: 0.9173 - auc: 0.9676 - precision_8: 0.9022 - recall_8: 0.9314 - specificity: 0.8585 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9246 - val_loss: 0.2014 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 803/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2359 - f1: 0.9099 - auc: 0.9677 - precision_8: 0.8674 - recall_8: 0.9596 - specificity: 0.8307 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9169 - val_loss: 0.2799 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 804/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2363 - f1: 0.9011 - auc: 0.9676 - precision_8: 0.9050 - recall_8: 0.9098 - specificity: 0.8705 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9175 - val_loss: 0.2043 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 805/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2385 - f1: 0.9075 - auc: 0.9672 - precision_8: 0.8604 - recall_8: 0.9623 - specificity: 0.8260 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9139 - val_loss: 0.2640 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 806/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2351 - f1: 0.9111 - auc: 0.9678 - precision_8: 0.9044 - recall_8: 0.9166 - specificity: 0.8712 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9199 - val_loss: 0.2374 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 807/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2362 - f1: 0.9105 - auc: 0.9672 - precision_8: 0.8805 - recall_8: 0.9421 - specificity: 0.8380 - specificity_at_sensitivity_8: 0.9817 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9175 - val_loss: 0.2154 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 808/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2368 - f1: 0.9116 - auc: 0.9663 - precision_8: 0.8936 - recall_8: 0.9273 - specificity: 0.8646 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9187 - val_loss: 0.2351 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 809/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2328 - f1: 0.9186 - auc: 0.9677 - precision_8: 0.8889 - recall_8: 0.9475 - specificity: 0.8385 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9240 - val_loss: 0.1993 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 810/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2350 - f1: 0.9076 - auc: 0.9672 - precision_8: 0.8805 - recall_8: 0.9421 - specificity: 0.8549 - specificity_at_sensitivity_8: 0.9817 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9175 - val_loss: 0.2513 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 811/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2314 - f1: 0.9160 - auc: 0.9676 - precision_8: 0.8935 - recall_8: 0.9367 - specificity: 0.8499 - specificity_at_sensitivity_8: 0.9774 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9222 - val_loss: 0.1895 - val_f1: 0.9695 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9409 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9409\n",
            "Epoch 812/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2346 - f1: 0.9099 - auc: 0.9666 - precision_8: 0.8763 - recall_8: 0.9435 - specificity: 0.8512 - specificity_at_sensitivity_8: 0.9785 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9157 - val_loss: 0.2488 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 813/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2316 - f1: 0.9138 - auc: 0.9678 - precision_8: 0.8916 - recall_8: 0.9408 - specificity: 0.8507 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9228 - val_loss: 0.2120 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 814/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2310 - f1: 0.9223 - auc: 0.9680 - precision_8: 0.8933 - recall_8: 0.9462 - specificity: 0.8550 - specificity_at_sensitivity_8: 0.9806 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9258 - val_loss: 0.2447 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 815/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2301 - f1: 0.9194 - auc: 0.9686 - precision_8: 0.9013 - recall_8: 0.9341 - specificity: 0.8599 - specificity_at_sensitivity_8: 0.9806 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9252 - val_loss: 0.1820 - val_f1: 0.9695 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9409 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9409\n",
            "Epoch 816/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2334 - f1: 0.9070 - auc: 0.9691 - precision_8: 0.8595 - recall_8: 0.9637 - specificity: 0.8304 - specificity_at_sensitivity_8: 0.9806 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9139 - val_loss: 0.2941 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 817/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2354 - f1: 0.9087 - auc: 0.9683 - precision_8: 0.9096 - recall_8: 0.9071 - specificity: 0.8745 - specificity_at_sensitivity_8: 0.9806 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9187 - val_loss: 0.1783 - val_f1: 0.9695 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9409 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9409\n",
            "Epoch 818/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2343 - f1: 0.9092 - auc: 0.9690 - precision_8: 0.8595 - recall_8: 0.9637 - specificity: 0.8288 - specificity_at_sensitivity_8: 0.9806 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9139 - val_loss: 0.2291 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 819/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2298 - f1: 0.9131 - auc: 0.9687 - precision_8: 0.8986 - recall_8: 0.9300 - specificity: 0.8607 - specificity_at_sensitivity_8: 0.9806 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9222 - val_loss: 0.2463 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 820/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2299 - f1: 0.9134 - auc: 0.9682 - precision_8: 0.8846 - recall_8: 0.9489 - specificity: 0.8489 - specificity_at_sensitivity_8: 0.9806 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9222 - val_loss: 0.2019 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 821/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2325 - f1: 0.9085 - auc: 0.9674 - precision_8: 0.8854 - recall_8: 0.9354 - specificity: 0.8574 - specificity_at_sensitivity_8: 0.9785 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9175 - val_loss: 0.2166 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 822/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2291 - f1: 0.9156 - auc: 0.9690 - precision_8: 0.8836 - recall_8: 0.9502 - specificity: 0.8453 - specificity_at_sensitivity_8: 0.9806 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9222 - val_loss: 0.2150 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 823/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2284 - f1: 0.9203 - auc: 0.9689 - precision_8: 0.8954 - recall_8: 0.9448 - specificity: 0.8537 - specificity_at_sensitivity_8: 0.9817 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9264 - val_loss: 0.2536 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 824/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2291 - f1: 0.9205 - auc: 0.9686 - precision_8: 0.8986 - recall_8: 0.9421 - specificity: 0.8564 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9270 - val_loss: 0.2060 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 825/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2291 - f1: 0.9118 - auc: 0.9690 - precision_8: 0.8843 - recall_8: 0.9462 - specificity: 0.8517 - specificity_at_sensitivity_8: 0.9817 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9211 - val_loss: 0.2290 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 826/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2279 - f1: 0.9184 - auc: 0.9692 - precision_8: 0.8913 - recall_8: 0.9489 - specificity: 0.8503 - specificity_at_sensitivity_8: 0.9806 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9258 - val_loss: 0.2206 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 827/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2274 - f1: 0.9117 - auc: 0.9695 - precision_8: 0.8968 - recall_8: 0.9354 - specificity: 0.8547 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9234 - val_loss: 0.2381 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 828/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2272 - f1: 0.9167 - auc: 0.9696 - precision_8: 0.9014 - recall_8: 0.9354 - specificity: 0.8558 - specificity_at_sensitivity_8: 0.9817 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9258 - val_loss: 0.2024 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 829/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2277 - f1: 0.9154 - auc: 0.9695 - precision_8: 0.8794 - recall_8: 0.9515 - specificity: 0.8485 - specificity_at_sensitivity_8: 0.9817 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9205 - val_loss: 0.2423 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 830/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2272 - f1: 0.9210 - auc: 0.9696 - precision_8: 0.9048 - recall_8: 0.9341 - specificity: 0.8632 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9270 - val_loss: 0.1972 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 831/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2280 - f1: 0.9175 - auc: 0.9697 - precision_8: 0.8821 - recall_8: 0.9569 - specificity: 0.8474 - specificity_at_sensitivity_8: 0.9795 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9240 - val_loss: 0.2206 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 832/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2264 - f1: 0.9187 - auc: 0.9698 - precision_8: 0.8973 - recall_8: 0.9408 - specificity: 0.8558 - specificity_at_sensitivity_8: 0.9817 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9258 - val_loss: 0.2199 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 833/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2272 - f1: 0.9209 - auc: 0.9693 - precision_8: 0.8929 - recall_8: 0.9421 - specificity: 0.8615 - specificity_at_sensitivity_8: 0.9806 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9240 - val_loss: 0.1893 - val_f1: 0.9695 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9409 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9409\n",
            "Epoch 834/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2278 - f1: 0.9175 - auc: 0.9700 - precision_8: 0.8741 - recall_8: 0.9623 - specificity: 0.8383 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9217 - val_loss: 0.2519 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 835/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2270 - f1: 0.9152 - auc: 0.9694 - precision_8: 0.9022 - recall_8: 0.9314 - specificity: 0.8610 - specificity_at_sensitivity_8: 0.9817 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9246 - val_loss: 0.2304 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 836/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2255 - f1: 0.9160 - auc: 0.9700 - precision_8: 0.8972 - recall_8: 0.9394 - specificity: 0.8558 - specificity_at_sensitivity_8: 0.9817 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9252 - val_loss: 0.2143 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 837/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2253 - f1: 0.9172 - auc: 0.9701 - precision_8: 0.8921 - recall_8: 0.9462 - specificity: 0.8515 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9252 - val_loss: 0.2379 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 838/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2261 - f1: 0.9165 - auc: 0.9697 - precision_8: 0.8919 - recall_8: 0.9435 - specificity: 0.8523 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9240 - val_loss: 0.2377 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 839/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2251 - f1: 0.9198 - auc: 0.9703 - precision_8: 0.9048 - recall_8: 0.9341 - specificity: 0.8611 - specificity_at_sensitivity_8: 0.9817 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9270 - val_loss: 0.2207 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 840/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2266 - f1: 0.9168 - auc: 0.9698 - precision_8: 0.8850 - recall_8: 0.9529 - specificity: 0.8447 - specificity_at_sensitivity_8: 0.9806 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9240 - val_loss: 0.2607 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 841/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2267 - f1: 0.9136 - auc: 0.9696 - precision_8: 0.9051 - recall_8: 0.9246 - specificity: 0.8633 - specificity_at_sensitivity_8: 0.9817 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9234 - val_loss: 0.2359 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 842/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2242 - f1: 0.9236 - auc: 0.9703 - precision_8: 0.9052 - recall_8: 0.9381 - specificity: 0.8624 - specificity_at_sensitivity_8: 0.9817 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9288 - val_loss: 0.1964 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 843/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2247 - f1: 0.9198 - auc: 0.9705 - precision_8: 0.8846 - recall_8: 0.9596 - specificity: 0.8489 - specificity_at_sensitivity_8: 0.9806 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9264 - val_loss: 0.2389 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 844/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2245 - f1: 0.9196 - auc: 0.9703 - precision_8: 0.9048 - recall_8: 0.9341 - specificity: 0.8663 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9270 - val_loss: 0.1880 - val_f1: 0.9695 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9409 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9409\n",
            "Epoch 845/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2258 - f1: 0.9158 - auc: 0.9702 - precision_8: 0.8804 - recall_8: 0.9515 - specificity: 0.8495 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9211 - val_loss: 0.2157 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 846/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2233 - f1: 0.9223 - auc: 0.9706 - precision_8: 0.9001 - recall_8: 0.9462 - specificity: 0.8549 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9294 - val_loss: 0.2334 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 847/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2239 - f1: 0.9181 - auc: 0.9703 - precision_8: 0.8964 - recall_8: 0.9435 - specificity: 0.8527 - specificity_at_sensitivity_8: 0.9817 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9264 - val_loss: 0.2359 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 848/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2236 - f1: 0.9191 - auc: 0.9704 - precision_8: 0.9060 - recall_8: 0.9341 - specificity: 0.8651 - specificity_at_sensitivity_8: 0.9817 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9276 - val_loss: 0.1921 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 849/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2243 - f1: 0.9165 - auc: 0.9711 - precision_8: 0.8753 - recall_8: 0.9637 - specificity: 0.8422 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9228 - val_loss: 0.2492 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 850/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2238 - f1: 0.9175 - auc: 0.9706 - precision_8: 0.9060 - recall_8: 0.9341 - specificity: 0.8632 - specificity_at_sensitivity_8: 0.9817 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9276 - val_loss: 0.2333 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 851/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2222 - f1: 0.9208 - auc: 0.9710 - precision_8: 0.9032 - recall_8: 0.9421 - specificity: 0.8563 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9294 - val_loss: 0.2046 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 852/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2225 - f1: 0.9167 - auc: 0.9712 - precision_8: 0.8878 - recall_8: 0.9475 - specificity: 0.8545 - specificity_at_sensitivity_8: 0.9806 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9234 - val_loss: 0.2201 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 853/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2217 - f1: 0.9228 - auc: 0.9714 - precision_8: 0.9022 - recall_8: 0.9435 - specificity: 0.8570 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9294 - val_loss: 0.2043 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 854/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2217 - f1: 0.9193 - auc: 0.9714 - precision_8: 0.8885 - recall_8: 0.9542 - specificity: 0.8534 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9264 - val_loss: 0.2385 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 855/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2216 - f1: 0.9186 - auc: 0.9712 - precision_8: 0.9027 - recall_8: 0.9367 - specificity: 0.8617 - specificity_at_sensitivity_8: 0.9817 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9270 - val_loss: 0.2058 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 856/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2218 - f1: 0.9244 - auc: 0.9711 - precision_8: 0.8971 - recall_8: 0.9502 - specificity: 0.8598 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9294 - val_loss: 0.2038 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 857/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2220 - f1: 0.9149 - auc: 0.9716 - precision_8: 0.8824 - recall_8: 0.9596 - specificity: 0.8453 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9252 - val_loss: 0.2626 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 858/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2229 - f1: 0.9151 - auc: 0.9714 - precision_8: 0.9074 - recall_8: 0.9233 - specificity: 0.8732 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9240 - val_loss: 0.1825 - val_f1: 0.9695 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9409 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9409\n",
            "Epoch 859/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2245 - f1: 0.9144 - auc: 0.9717 - precision_8: 0.8682 - recall_8: 0.9664 - specificity: 0.8356 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9199 - val_loss: 0.2905 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 860/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2269 - f1: 0.9096 - auc: 0.9715 - precision_8: 0.9146 - recall_8: 0.9085 - specificity: 0.8813 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9217 - val_loss: 0.1585 - val_f1: 0.9836 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9677 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9677\n",
            "Epoch 861/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2300 - f1: 0.9072 - auc: 0.9718 - precision_8: 0.8496 - recall_8: 0.9731 - specificity: 0.8229 - specificity_at_sensitivity_8: 0.9817 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9115 - val_loss: 0.2897 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 862/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2275 - f1: 0.9119 - auc: 0.9715 - precision_8: 0.9156 - recall_8: 0.9058 - specificity: 0.8850 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9211 - val_loss: 0.1832 - val_f1: 0.9695 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9409 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9409\n",
            "Epoch 863/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2286 - f1: 0.9027 - auc: 0.9710 - precision_8: 0.8548 - recall_8: 0.9664 - specificity: 0.8277 - specificity_at_sensitivity_8: 0.9817 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9121 - val_loss: 0.2590 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 864/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2243 - f1: 0.9152 - auc: 0.9712 - precision_8: 0.9116 - recall_8: 0.9166 - specificity: 0.8827 - specificity_at_sensitivity_8: 0.9806 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9234 - val_loss: 0.2120 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 865/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2231 - f1: 0.9117 - auc: 0.9711 - precision_8: 0.8765 - recall_8: 0.9556 - specificity: 0.8399 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9205 - val_loss: 0.2044 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 866/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2220 - f1: 0.9125 - auc: 0.9713 - precision_8: 0.8977 - recall_8: 0.9327 - specificity: 0.8682 - specificity_at_sensitivity_8: 0.9806 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9228 - val_loss: 0.2340 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 867/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2210 - f1: 0.9205 - auc: 0.9712 - precision_8: 0.8933 - recall_8: 0.9462 - specificity: 0.8539 - specificity_at_sensitivity_8: 0.9806 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9258 - val_loss: 0.1846 - val_f1: 0.9695 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9409 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9409\n",
            "Epoch 868/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2309 - f1: 0.9074 - auc: 0.9675 - precision_8: 0.8838 - recall_8: 0.9314 - specificity: 0.8699 - specificity_at_sensitivity_8: 0.9785 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9151 - val_loss: 0.1842 - val_f1: 0.9695 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9409 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9409\n",
            "Epoch 869/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2237 - f1: 0.9135 - auc: 0.9723 - precision_8: 0.8659 - recall_8: 0.9650 - specificity: 0.8328 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9181 - val_loss: 0.2165 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 870/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2229 - f1: 0.9195 - auc: 0.9712 - precision_8: 0.9060 - recall_8: 0.9341 - specificity: 0.8759 - specificity_at_sensitivity_8: 0.9806 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9276 - val_loss: 0.2241 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 871/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2189 - f1: 0.9205 - auc: 0.9724 - precision_8: 0.8902 - recall_8: 0.9489 - specificity: 0.8525 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9252 - val_loss: 0.1725 - val_f1: 0.9780 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9570 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9570\n",
            "Epoch 872/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2236 - f1: 0.9149 - auc: 0.9698 - precision_8: 0.8850 - recall_8: 0.9421 - specificity: 0.8607 - specificity_at_sensitivity_8: 0.9785 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9199 - val_loss: 0.2419 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 873/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2180 - f1: 0.9219 - auc: 0.9721 - precision_8: 0.9000 - recall_8: 0.9448 - specificity: 0.8608 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9288 - val_loss: 0.1754 - val_f1: 0.9752 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9516 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9516\n",
            "Epoch 874/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2200 - f1: 0.9131 - auc: 0.9719 - precision_8: 0.8813 - recall_8: 0.9489 - specificity: 0.8525 - specificity_at_sensitivity_8: 0.9817 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9205 - val_loss: 0.2465 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 875/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2176 - f1: 0.9218 - auc: 0.9724 - precision_8: 0.9074 - recall_8: 0.9367 - specificity: 0.8687 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9294 - val_loss: 0.1719 - val_f1: 0.9780 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9570 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9570\n",
            "Epoch 876/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2195 - f1: 0.9254 - auc: 0.9726 - precision_8: 0.8761 - recall_8: 0.9704 - specificity: 0.8452 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9258 - val_loss: 0.2552 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 877/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2186 - f1: 0.9186 - auc: 0.9726 - precision_8: 0.9067 - recall_8: 0.9287 - specificity: 0.8732 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9258 - val_loss: 0.1807 - val_f1: 0.9695 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9409 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9409\n",
            "Epoch 878/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2190 - f1: 0.9171 - auc: 0.9730 - precision_8: 0.8715 - recall_8: 0.9677 - specificity: 0.8409 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9222 - val_loss: 0.2530 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 879/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2187 - f1: 0.9184 - auc: 0.9727 - precision_8: 0.9098 - recall_8: 0.9233 - specificity: 0.8767 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9252 - val_loss: 0.1915 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 880/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2170 - f1: 0.9230 - auc: 0.9730 - precision_8: 0.8816 - recall_8: 0.9623 - specificity: 0.8525 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9258 - val_loss: 0.2091 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 881/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2159 - f1: 0.9227 - auc: 0.9729 - precision_8: 0.9003 - recall_8: 0.9475 - specificity: 0.8625 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9300 - val_loss: 0.2265 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 882/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2197 - f1: 0.9134 - auc: 0.9717 - precision_8: 0.8843 - recall_8: 0.9462 - specificity: 0.8503 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9211 - val_loss: 0.2386 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 883/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2217 - f1: 0.9142 - auc: 0.9718 - precision_8: 0.9097 - recall_8: 0.9219 - specificity: 0.8829 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9246 - val_loss: 0.1612 - val_f1: 0.9808 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9624 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9624\n",
            "Epoch 884/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2241 - f1: 0.9153 - auc: 0.9731 - precision_8: 0.8629 - recall_8: 0.9744 - specificity: 0.8273 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9199 - val_loss: 0.2405 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 885/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2206 - f1: 0.9117 - auc: 0.9726 - precision_8: 0.9081 - recall_8: 0.9179 - specificity: 0.8836 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9222 - val_loss: 0.1979 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 886/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2206 - f1: 0.9146 - auc: 0.9725 - precision_8: 0.8742 - recall_8: 0.9637 - specificity: 0.8361 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9222 - val_loss: 0.2070 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 887/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2166 - f1: 0.9208 - auc: 0.9722 - precision_8: 0.9052 - recall_8: 0.9381 - specificity: 0.8757 - specificity_at_sensitivity_8: 0.9817 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9288 - val_loss: 0.2495 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 888/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2180 - f1: 0.9136 - auc: 0.9720 - precision_8: 0.8893 - recall_8: 0.9408 - specificity: 0.8553 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9217 - val_loss: 0.1759 - val_f1: 0.9752 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9516 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9516\n",
            "Epoch 889/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2190 - f1: 0.9166 - auc: 0.9717 - precision_8: 0.8889 - recall_8: 0.9475 - specificity: 0.8629 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9240 - val_loss: 0.2298 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 890/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2154 - f1: 0.9201 - auc: 0.9726 - precision_8: 0.8907 - recall_8: 0.9542 - specificity: 0.8554 - specificity_at_sensitivity_8: 0.9817 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9276 - val_loss: 0.1876 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 891/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2188 - f1: 0.9186 - auc: 0.9714 - precision_8: 0.8943 - recall_8: 0.9448 - specificity: 0.8692 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9258 - val_loss: 0.2025 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 892/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2135 - f1: 0.9288 - auc: 0.9739 - precision_8: 0.8980 - recall_8: 0.9596 - specificity: 0.8586 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9336 - val_loss: 0.1754 - val_f1: 0.9752 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9516 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9516\n",
            "Epoch 893/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2151 - f1: 0.9276 - auc: 0.9739 - precision_8: 0.8853 - recall_8: 0.9664 - specificity: 0.8557 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9294 - val_loss: 0.2576 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 894/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2149 - f1: 0.9212 - auc: 0.9737 - precision_8: 0.9096 - recall_8: 0.9341 - specificity: 0.8717 - specificity_at_sensitivity_8: 0.9849 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9294 - val_loss: 0.1719 - val_f1: 0.9780 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9570 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9570\n",
            "Epoch 895/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2151 - f1: 0.9186 - auc: 0.9740 - precision_8: 0.8745 - recall_8: 0.9664 - specificity: 0.8474 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9234 - val_loss: 0.2598 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 896/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2156 - f1: 0.9180 - auc: 0.9737 - precision_8: 0.9124 - recall_8: 0.9246 - specificity: 0.8788 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9270 - val_loss: 0.1681 - val_f1: 0.9780 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9570 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9570\n",
            "Epoch 897/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2170 - f1: 0.9173 - auc: 0.9742 - precision_8: 0.8678 - recall_8: 0.9717 - specificity: 0.8403 - specificity_at_sensitivity_8: 0.9849 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9217 - val_loss: 0.2613 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 898/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2171 - f1: 0.9158 - auc: 0.9737 - precision_8: 0.9166 - recall_8: 0.9166 - specificity: 0.8836 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9258 - val_loss: 0.1742 - val_f1: 0.9780 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9570 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9570\n",
            "Epoch 899/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2178 - f1: 0.9143 - auc: 0.9739 - precision_8: 0.8655 - recall_8: 0.9704 - specificity: 0.8345 - specificity_at_sensitivity_8: 0.9849 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9199 - val_loss: 0.2395 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 900/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2165 - f1: 0.9131 - auc: 0.9738 - precision_8: 0.9130 - recall_8: 0.9179 - specificity: 0.8828 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9246 - val_loss: 0.1906 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 901/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2149 - f1: 0.9231 - auc: 0.9740 - precision_8: 0.8824 - recall_8: 0.9690 - specificity: 0.8455 - specificity_at_sensitivity_8: 0.9817 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9288 - val_loss: 0.1987 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 902/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2113 - f1: 0.9309 - auc: 0.9745 - precision_8: 0.9055 - recall_8: 0.9542 - specificity: 0.8695 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9354 - val_loss: 0.2707 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 903/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2150 - f1: 0.9221 - auc: 0.9735 - precision_8: 0.9053 - recall_8: 0.9394 - specificity: 0.8689 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9294 - val_loss: 0.1628 - val_f1: 0.9808 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9624 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9624\n",
            "Epoch 904/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2166 - f1: 0.9171 - auc: 0.9725 - precision_8: 0.8758 - recall_8: 0.9583 - specificity: 0.8590 - specificity_at_sensitivity_8: 0.9817 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9211 - val_loss: 0.2309 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 905/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2116 - f1: 0.9170 - auc: 0.9740 - precision_8: 0.8954 - recall_8: 0.9448 - specificity: 0.8633 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9264 - val_loss: 0.1854 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 906/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2114 - f1: 0.9248 - auc: 0.9741 - precision_8: 0.8955 - recall_8: 0.9569 - specificity: 0.8610 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9312 - val_loss: 0.2337 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 907/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2112 - f1: 0.9215 - auc: 0.9744 - precision_8: 0.9032 - recall_8: 0.9421 - specificity: 0.8665 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9294 - val_loss: 0.1895 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 908/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2110 - f1: 0.9266 - auc: 0.9746 - precision_8: 0.8977 - recall_8: 0.9569 - specificity: 0.8614 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9324 - val_loss: 0.2182 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 909/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2106 - f1: 0.9256 - auc: 0.9746 - precision_8: 0.9018 - recall_8: 0.9515 - specificity: 0.8635 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9324 - val_loss: 0.2107 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 910/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2108 - f1: 0.9192 - auc: 0.9746 - precision_8: 0.9052 - recall_8: 0.9381 - specificity: 0.8721 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9288 - val_loss: 0.1861 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 911/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2115 - f1: 0.9233 - auc: 0.9749 - precision_8: 0.8853 - recall_8: 0.9664 - specificity: 0.8517 - specificity_at_sensitivity_8: 0.9849 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9294 - val_loss: 0.2339 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 912/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2105 - f1: 0.9203 - auc: 0.9750 - precision_8: 0.9073 - recall_8: 0.9354 - specificity: 0.8736 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9288 - val_loss: 0.2141 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 913/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2092 - f1: 0.9284 - auc: 0.9751 - precision_8: 0.9072 - recall_8: 0.9475 - specificity: 0.8698 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9336 - val_loss: 0.1685 - val_f1: 0.9780 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9570 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9570\n",
            "Epoch 914/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2112 - f1: 0.9216 - auc: 0.9751 - precision_8: 0.8799 - recall_8: 0.9664 - specificity: 0.8534 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9264 - val_loss: 0.2411 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 915/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2104 - f1: 0.9225 - auc: 0.9750 - precision_8: 0.9106 - recall_8: 0.9327 - specificity: 0.8778 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9294 - val_loss: 0.1642 - val_f1: 0.9808 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9624 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9624\n",
            "Epoch 916/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2119 - f1: 0.9225 - auc: 0.9749 - precision_8: 0.8739 - recall_8: 0.9704 - specificity: 0.8496 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9246 - val_loss: 0.2333 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 917/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2095 - f1: 0.9212 - auc: 0.9753 - precision_8: 0.9109 - recall_8: 0.9354 - specificity: 0.8759 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9306 - val_loss: 0.1955 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 918/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2099 - f1: 0.9225 - auc: 0.9750 - precision_8: 0.8892 - recall_8: 0.9610 - specificity: 0.8534 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9294 - val_loss: 0.2309 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 919/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2106 - f1: 0.9188 - auc: 0.9750 - precision_8: 0.9104 - recall_8: 0.9300 - specificity: 0.8808 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9282 - val_loss: 0.1738 - val_f1: 0.9752 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9516 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9516\n",
            "Epoch 920/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2101 - f1: 0.9239 - auc: 0.9755 - precision_8: 0.8771 - recall_8: 0.9704 - specificity: 0.8499 - specificity_at_sensitivity_8: 0.9849 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9264 - val_loss: 0.2104 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 921/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2114 - f1: 0.9182 - auc: 0.9746 - precision_8: 0.9115 - recall_8: 0.9287 - specificity: 0.8800 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9282 - val_loss: 0.1726 - val_f1: 0.9752 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9516 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9516\n",
            "Epoch 922/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2113 - f1: 0.9184 - auc: 0.9756 - precision_8: 0.8711 - recall_8: 0.9731 - specificity: 0.8423 - specificity_at_sensitivity_8: 0.9849 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9240 - val_loss: 0.2236 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 923/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2101 - f1: 0.9217 - auc: 0.9751 - precision_8: 0.9149 - recall_8: 0.9260 - specificity: 0.8818 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9288 - val_loss: 0.1894 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 924/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2102 - f1: 0.9214 - auc: 0.9751 - precision_8: 0.8841 - recall_8: 0.9650 - specificity: 0.8447 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9282 - val_loss: 0.2138 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 925/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2085 - f1: 0.9245 - auc: 0.9753 - precision_8: 0.9110 - recall_8: 0.9367 - specificity: 0.8800 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9312 - val_loss: 0.2074 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 926/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2082 - f1: 0.9270 - auc: 0.9752 - precision_8: 0.8969 - recall_8: 0.9596 - specificity: 0.8560 - specificity_at_sensitivity_8: 0.9849 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9330 - val_loss: 0.1896 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 927/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2098 - f1: 0.9200 - auc: 0.9747 - precision_8: 0.8999 - recall_8: 0.9435 - specificity: 0.8747 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9282 - val_loss: 0.2021 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 928/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2075 - f1: 0.9223 - auc: 0.9758 - precision_8: 0.8881 - recall_8: 0.9610 - specificity: 0.8583 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9288 - val_loss: 0.1884 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 929/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2061 - f1: 0.9228 - auc: 0.9761 - precision_8: 0.8994 - recall_8: 0.9502 - specificity: 0.8692 - specificity_at_sensitivity_8: 0.9849 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9306 - val_loss: 0.2438 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 930/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2068 - f1: 0.9262 - auc: 0.9761 - precision_8: 0.9070 - recall_8: 0.9448 - specificity: 0.8714 - specificity_at_sensitivity_8: 0.9849 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9324 - val_loss: 0.1599 - val_f1: 0.9808 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9624 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9624\n",
            "Epoch 931/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2102 - f1: 0.9206 - auc: 0.9747 - precision_8: 0.8832 - recall_8: 0.9569 - specificity: 0.8610 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9246 - val_loss: 0.2175 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 932/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2057 - f1: 0.9252 - auc: 0.9762 - precision_8: 0.8994 - recall_8: 0.9502 - specificity: 0.8673 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9306 - val_loss: 0.1878 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 933/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2055 - f1: 0.9232 - auc: 0.9764 - precision_8: 0.8935 - recall_8: 0.9596 - specificity: 0.8589 - specificity_at_sensitivity_8: 0.9849 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9312 - val_loss: 0.2624 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 934/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2107 - f1: 0.9150 - auc: 0.9747 - precision_8: 0.9054 - recall_8: 0.9273 - specificity: 0.8736 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9246 - val_loss: 0.1931 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 935/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2048 - f1: 0.9288 - auc: 0.9765 - precision_8: 0.8991 - recall_8: 0.9596 - specificity: 0.8640 - specificity_at_sensitivity_8: 0.9849 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9342 - val_loss: 0.2470 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 936/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2066 - f1: 0.9197 - auc: 0.9764 - precision_8: 0.9128 - recall_8: 0.9300 - specificity: 0.8821 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9294 - val_loss: 0.1541 - val_f1: 0.9836 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9677 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9677\n",
            "Epoch 937/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2089 - f1: 0.9197 - auc: 0.9762 - precision_8: 0.8765 - recall_8: 0.9650 - specificity: 0.8522 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9240 - val_loss: 0.2173 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 938/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2045 - f1: 0.9254 - auc: 0.9765 - precision_8: 0.9090 - recall_8: 0.9408 - specificity: 0.8750 - specificity_at_sensitivity_8: 0.9849 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9318 - val_loss: 0.1904 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 939/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2050 - f1: 0.9232 - auc: 0.9765 - precision_8: 0.8918 - recall_8: 0.9650 - specificity: 0.8567 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9324 - val_loss: 0.2287 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 940/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2051 - f1: 0.9236 - auc: 0.9767 - precision_8: 0.9134 - recall_8: 0.9367 - specificity: 0.8801 - specificity_at_sensitivity_8: 0.9849 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9324 - val_loss: 0.1814 - val_f1: 0.9724 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9462 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9462\n",
            "Epoch 941/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2048 - f1: 0.9272 - auc: 0.9770 - precision_8: 0.8889 - recall_8: 0.9690 - specificity: 0.8550 - specificity_at_sensitivity_8: 0.9849 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9324 - val_loss: 0.2187 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 942/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2051 - f1: 0.9240 - auc: 0.9767 - precision_8: 0.9109 - recall_8: 0.9354 - specificity: 0.8821 - specificity_at_sensitivity_8: 0.9849 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9306 - val_loss: 0.1746 - val_f1: 0.9752 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9516 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9516\n",
            "Epoch 943/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2057 - f1: 0.9227 - auc: 0.9772 - precision_8: 0.8805 - recall_8: 0.9717 - specificity: 0.8502 - specificity_at_sensitivity_8: 0.9849 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9288 - val_loss: 0.2291 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 944/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2041 - f1: 0.9247 - auc: 0.9769 - precision_8: 0.9134 - recall_8: 0.9367 - specificity: 0.8783 - specificity_at_sensitivity_8: 0.9849 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9324 - val_loss: 0.2138 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 945/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2025 - f1: 0.9262 - auc: 0.9769 - precision_8: 0.9009 - recall_8: 0.9542 - specificity: 0.8703 - specificity_at_sensitivity_8: 0.9849 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9330 - val_loss: 0.1731 - val_f1: 0.9752 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9516 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9516\n",
            "Epoch 946/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2036 - f1: 0.9299 - auc: 0.9767 - precision_8: 0.8960 - recall_8: 0.9623 - specificity: 0.8657 - specificity_at_sensitivity_8: 0.9828 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9336 - val_loss: 0.2204 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 947/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2024 - f1: 0.9298 - auc: 0.9769 - precision_8: 0.9097 - recall_8: 0.9489 - specificity: 0.8726 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9354 - val_loss: 0.1752 - val_f1: 0.9752 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9516 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9516\n",
            "Epoch 948/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2027 - f1: 0.9302 - auc: 0.9773 - precision_8: 0.8945 - recall_8: 0.9704 - specificity: 0.8623 - specificity_at_sensitivity_8: 0.9839 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9360 - val_loss: 0.2253 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 949/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2023 - f1: 0.9257 - auc: 0.9772 - precision_8: 0.9104 - recall_8: 0.9435 - specificity: 0.8765 - specificity_at_sensitivity_8: 0.9849 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9336 - val_loss: 0.1688 - val_f1: 0.9780 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9570 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9570\n",
            "Epoch 950/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2032 - f1: 0.9246 - auc: 0.9776 - precision_8: 0.8868 - recall_8: 0.9704 - specificity: 0.8536 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9318 - val_loss: 0.2537 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 951/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2050 - f1: 0.9186 - auc: 0.9773 - precision_8: 0.9184 - recall_8: 0.9246 - specificity: 0.8837 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9300 - val_loss: 0.1440 - val_f1: 0.9836 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9677 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9677\n",
            "Epoch 952/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2075 - f1: 0.9241 - auc: 0.9772 - precision_8: 0.8767 - recall_8: 0.9758 - specificity: 0.8467 - specificity_at_sensitivity_8: 0.9849 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9282 - val_loss: 0.2241 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 953/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2027 - f1: 0.9293 - auc: 0.9777 - precision_8: 0.9160 - recall_8: 0.9394 - specificity: 0.8832 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9348 - val_loss: 0.1676 - val_f1: 0.9780 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9570 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9570\n",
            "Epoch 954/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2028 - f1: 0.9278 - auc: 0.9778 - precision_8: 0.8857 - recall_8: 0.9704 - specificity: 0.8574 - specificity_at_sensitivity_8: 0.9849 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9312 - val_loss: 0.2104 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 955/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2007 - f1: 0.9301 - auc: 0.9778 - precision_8: 0.9089 - recall_8: 0.9529 - specificity: 0.8741 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9366 - val_loss: 0.2094 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 956/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2003 - f1: 0.9285 - auc: 0.9777 - precision_8: 0.9065 - recall_8: 0.9529 - specificity: 0.8735 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9354 - val_loss: 0.1682 - val_f1: 0.9780 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9570 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9570\n",
            "Epoch 957/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2025 - f1: 0.9252 - auc: 0.9776 - precision_8: 0.8860 - recall_8: 0.9731 - specificity: 0.8524 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9324 - val_loss: 0.2797 - val_f1: 0.9521 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9086 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9086\n",
            "Epoch 958/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2063 - f1: 0.9226 - auc: 0.9776 - precision_8: 0.9252 - recall_8: 0.9152 - specificity: 0.8930 - specificity_at_sensitivity_8: 0.9849 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9294 - val_loss: 0.1415 - val_f1: 0.9836 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9677 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9677\n",
            "Epoch 959/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2059 - f1: 0.9224 - auc: 0.9781 - precision_8: 0.8686 - recall_8: 0.9785 - specificity: 0.8450 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9246 - val_loss: 0.2603 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 960/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2039 - f1: 0.9205 - auc: 0.9779 - precision_8: 0.9195 - recall_8: 0.9219 - specificity: 0.8871 - specificity_at_sensitivity_8: 0.9849 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9294 - val_loss: 0.1676 - val_f1: 0.9780 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9570 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9570\n",
            "Epoch 961/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2014 - f1: 0.9286 - auc: 0.9782 - precision_8: 0.8859 - recall_8: 0.9717 - specificity: 0.8532 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9318 - val_loss: 0.2389 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 962/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2021 - f1: 0.9228 - auc: 0.9781 - precision_8: 0.9187 - recall_8: 0.9273 - specificity: 0.8888 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9312 - val_loss: 0.1541 - val_f1: 0.9836 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9677 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9677\n",
            "Epoch 963/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2027 - f1: 0.9232 - auc: 0.9782 - precision_8: 0.8805 - recall_8: 0.9717 - specificity: 0.8541 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9288 - val_loss: 0.2105 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 964/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1992 - f1: 0.9301 - auc: 0.9782 - precision_8: 0.9119 - recall_8: 0.9475 - specificity: 0.8783 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9360 - val_loss: 0.1959 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 965/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2005 - f1: 0.9266 - auc: 0.9776 - precision_8: 0.8947 - recall_8: 0.9610 - specificity: 0.8602 - specificity_at_sensitivity_8: 0.9849 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9324 - val_loss: 0.2198 - val_f1: 0.9609 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9247 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9247\n",
            "Epoch 966/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1992 - f1: 0.9263 - auc: 0.9782 - precision_8: 0.9124 - recall_8: 0.9394 - specificity: 0.8795 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9330 - val_loss: 0.2065 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 967/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1981 - f1: 0.9310 - auc: 0.9784 - precision_8: 0.9079 - recall_8: 0.9556 - specificity: 0.8754 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9372 - val_loss: 0.1548 - val_f1: 0.9836 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9677 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9677\n",
            "Epoch 968/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2013 - f1: 0.9252 - auc: 0.9780 - precision_8: 0.8853 - recall_8: 0.9664 - specificity: 0.8592 - specificity_at_sensitivity_8: 0.9882 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9294 - val_loss: 0.2128 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 969/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1979 - f1: 0.9289 - auc: 0.9787 - precision_8: 0.9076 - recall_8: 0.9515 - specificity: 0.8752 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9354 - val_loss: 0.1734 - val_f1: 0.9752 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9516 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9516\n",
            "Epoch 970/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1984 - f1: 0.9301 - auc: 0.9784 - precision_8: 0.8963 - recall_8: 0.9650 - specificity: 0.8676 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9348 - val_loss: 0.2025 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 971/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1984 - f1: 0.9299 - auc: 0.9786 - precision_8: 0.9132 - recall_8: 0.9489 - specificity: 0.8792 - specificity_at_sensitivity_8: 0.9871 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9372 - val_loss: 0.1521 - val_f1: 0.9836 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9677 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9677\n",
            "Epoch 972/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2008 - f1: 0.9230 - auc: 0.9791 - precision_8: 0.8796 - recall_8: 0.9731 - specificity: 0.8489 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9288 - val_loss: 0.2533 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 973/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2013 - f1: 0.9233 - auc: 0.9789 - precision_8: 0.9233 - recall_8: 0.9233 - specificity: 0.8902 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9318 - val_loss: 0.1400 - val_f1: 0.9836 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9677 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9677\n",
            "Epoch 974/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2033 - f1: 0.9206 - auc: 0.9792 - precision_8: 0.8665 - recall_8: 0.9785 - specificity: 0.8468 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9234 - val_loss: 0.2490 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 975/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2004 - f1: 0.9238 - auc: 0.9787 - precision_8: 0.9233 - recall_8: 0.9233 - specificity: 0.8904 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9318 - val_loss: 0.1607 - val_f1: 0.9808 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9624 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9624\n",
            "Epoch 976/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1997 - f1: 0.9211 - auc: 0.9789 - precision_8: 0.8764 - recall_8: 0.9731 - specificity: 0.8506 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9270 - val_loss: 0.2370 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 977/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1989 - f1: 0.9253 - auc: 0.9788 - precision_8: 0.9213 - recall_8: 0.9300 - specificity: 0.8878 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9336 - val_loss: 0.1734 - val_f1: 0.9752 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9516 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9516\n",
            "Epoch 978/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1973 - f1: 0.9279 - auc: 0.9791 - precision_8: 0.8878 - recall_8: 0.9690 - specificity: 0.8611 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9318 - val_loss: 0.2100 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 979/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1981 - f1: 0.9271 - auc: 0.9787 - precision_8: 0.9149 - recall_8: 0.9408 - specificity: 0.8879 - specificity_at_sensitivity_8: 0.9882 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9348 - val_loss: 0.1646 - val_f1: 0.9780 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9570 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9570\n",
            "Epoch 980/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1975 - f1: 0.9284 - auc: 0.9794 - precision_8: 0.8860 - recall_8: 0.9731 - specificity: 0.8580 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9324 - val_loss: 0.2005 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 981/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1955 - f1: 0.9294 - auc: 0.9793 - precision_8: 0.9060 - recall_8: 0.9596 - specificity: 0.8713 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9378 - val_loss: 0.2469 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 982/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1974 - f1: 0.9273 - auc: 0.9795 - precision_8: 0.9181 - recall_8: 0.9354 - specificity: 0.8847 - specificity_at_sensitivity_8: 0.9914 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9342 - val_loss: 0.1487 - val_f1: 0.9836 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9677 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9677\n",
            "Epoch 983/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1985 - f1: 0.9202 - auc: 0.9797 - precision_8: 0.8788 - recall_8: 0.9758 - specificity: 0.8517 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9294 - val_loss: 0.2983 - val_f1: 0.9492 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9032 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9032\n",
            "Epoch 984/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2039 - f1: 0.9168 - auc: 0.9787 - precision_8: 0.9307 - recall_8: 0.9044 - specificity: 0.8944 - specificity_at_sensitivity_8: 0.9871 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9276 - val_loss: 0.1227 - val_f1: 0.9919 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9839 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9839\n",
            "Epoch 985/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2052 - f1: 0.9183 - auc: 0.9795 - precision_8: 0.8629 - recall_8: 0.9825 - specificity: 0.8391 - specificity_at_sensitivity_8: 0.9882 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9228 - val_loss: 0.2795 - val_f1: 0.9521 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9086 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9086\n",
            "Epoch 986/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.2022 - f1: 0.9204 - auc: 0.9789 - precision_8: 0.9277 - recall_8: 0.9152 - specificity: 0.8897 - specificity_at_sensitivity_8: 0.9914 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9306 - val_loss: 0.1574 - val_f1: 0.9808 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9624 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9624\n",
            "Epoch 987/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1968 - f1: 0.9258 - auc: 0.9798 - precision_8: 0.8849 - recall_8: 0.9731 - specificity: 0.8537 - specificity_at_sensitivity_8: 0.9871 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9318 - val_loss: 0.2261 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 988/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1956 - f1: 0.9269 - auc: 0.9796 - precision_8: 0.9195 - recall_8: 0.9381 - specificity: 0.8830 - specificity_at_sensitivity_8: 0.9860 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9360 - val_loss: 0.1822 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 989/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1940 - f1: 0.9318 - auc: 0.9797 - precision_8: 0.9008 - recall_8: 0.9650 - specificity: 0.8677 - specificity_at_sensitivity_8: 0.9871 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9372 - val_loss: 0.1898 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 990/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1934 - f1: 0.9294 - auc: 0.9798 - precision_8: 0.9068 - recall_8: 0.9556 - specificity: 0.8761 - specificity_at_sensitivity_8: 0.9871 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9366 - val_loss: 0.2040 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 991/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1933 - f1: 0.9303 - auc: 0.9799 - precision_8: 0.9103 - recall_8: 0.9556 - specificity: 0.8748 - specificity_at_sensitivity_8: 0.9871 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9384 - val_loss: 0.1634 - val_f1: 0.9780 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9570 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9570\n",
            "Epoch 992/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1945 - f1: 0.9308 - auc: 0.9800 - precision_8: 0.8933 - recall_8: 0.9690 - specificity: 0.8681 - specificity_at_sensitivity_8: 0.9871 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9348 - val_loss: 0.2041 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 993/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1931 - f1: 0.9328 - auc: 0.9801 - precision_8: 0.9092 - recall_8: 0.9569 - specificity: 0.8766 - specificity_at_sensitivity_8: 0.9871 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9384 - val_loss: 0.1901 - val_f1: 0.9667 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9355 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9355\n",
            "Epoch 994/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1925 - f1: 0.9361 - auc: 0.9805 - precision_8: 0.9051 - recall_8: 0.9623 - specificity: 0.8747 - specificity_at_sensitivity_8: 0.9882 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9384 - val_loss: 0.2084 - val_f1: 0.9638 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9301 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9301\n",
            "Epoch 995/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1926 - f1: 0.9366 - auc: 0.9804 - precision_8: 0.9140 - recall_8: 0.9583 - specificity: 0.8795 - specificity_at_sensitivity_8: 0.9871 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9414 - val_loss: 0.1711 - val_f1: 0.9780 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9570 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9570\n",
            "Epoch 996/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1935 - f1: 0.9299 - auc: 0.9798 - precision_8: 0.8971 - recall_8: 0.9623 - specificity: 0.8738 - specificity_at_sensitivity_8: 0.9871 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9342 - val_loss: 0.1819 - val_f1: 0.9724 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9462 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9462\n",
            "Epoch 997/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1937 - f1: 0.9296 - auc: 0.9799 - precision_8: 0.8954 - recall_8: 0.9677 - specificity: 0.8631 - specificity_at_sensitivity_8: 0.9882 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9354 - val_loss: 0.2361 - val_f1: 0.9580 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9194 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9194\n",
            "Epoch 998/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1946 - f1: 0.9302 - auc: 0.9803 - precision_8: 0.9230 - recall_8: 0.9354 - specificity: 0.8888 - specificity_at_sensitivity_8: 0.9903 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9366 - val_loss: 0.1514 - val_f1: 0.9836 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9677 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9677\n",
            "Epoch 999/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1953 - f1: 0.9284 - auc: 0.9806 - precision_8: 0.8831 - recall_8: 0.9758 - specificity: 0.8546 - specificity_at_sensitivity_8: 0.9882 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9318 - val_loss: 0.2475 - val_f1: 0.9551 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9140 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9140\n",
            "Epoch 1000/1000\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.1961 - f1: 0.9265 - auc: 0.9805 - precision_8: 0.9245 - recall_8: 0.9233 - specificity: 0.8969 - specificity_at_sensitivity_8: 0.9871 - sensitivity_at_specificity_8: 1.0000 - accuracy: 0.9324 - val_loss: 0.1350 - val_f1: 0.9836 - val_auc: 0.0000e+00 - val_precision_8: 1.0000 - val_recall_8: 0.9677 - val_specificity: 0.0000e+00 - val_specificity_at_sensitivity_8: 0.0000e+00 - val_sensitivity_at_specificity_8: 0.0000e+00 - val_accuracy: 0.9677\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[04/03/22 16:41:09]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m root\u001b[1m[\u001b[0m\u001b[1;36m1376\u001b[0m\u001b[1m]\u001b[0m                     \u001b]8;id=586140;file://<ipython-input-20-70c0cdcc1dbf>\u001b\\\u001b[2m<ipython-input-20-70c0cdcc1dbf>\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=387851;file://<ipython-input-20-70c0cdcc1dbf>#7\u001b\\\u001b[2m7\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;35mMainProcess\u001b[0m\u001b[1m(\u001b[0mMainThread\u001b[1m)\u001b[0m \u001b[1m<\u001b[0m\u001b[1;95mipyth\u001b[0m \u001b[2m                                 \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;95mon-input-\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;95m-70c0cdcc1dbf\u001b[0m\u001b[1m>\u001b[0m:\u001b[1;36m7\u001b[0m    \u001b[2m                                 \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1m[\u001b[0mTraining\u001b[1m]\u001b[0m done.               \u001b[2m                                 \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/03/22 16:41:09] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> root<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1376</span><span style=\"font-weight: bold\">]</span>                     <a href=\"file://<ipython-input-20-70c0cdcc1dbf>\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-20-70c0cdcc1dbf&gt;</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://<ipython-input-20-70c0cdcc1dbf>#7\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">7</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MainProcess</span><span style=\"font-weight: bold\">(</span>MainThread<span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">ipyth</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                 </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">on-input-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">-70c0cdcc1dbf</span><span style=\"font-weight: bold\">&gt;</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                 </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"font-weight: bold\">[</span>Training<span style=\"font-weight: bold\">]</span> done.               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                 </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m root\u001b[1m[\u001b[0m\u001b[1;36m1376\u001b[0m\u001b[1m]\u001b[0m                     \u001b]8;id=24131;file://<ipython-input-20-70c0cdcc1dbf>\u001b\\\u001b[2m<ipython-input-20-70c0cdcc1dbf>\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=525606;file://<ipython-input-20-70c0cdcc1dbf>#9\u001b\\\u001b[2m9\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;35mMainProcess\u001b[0m\u001b[1m(\u001b[0mMainThread\u001b[1m)\u001b[0m \u001b[1m<\u001b[0m\u001b[1;95mipyth\u001b[0m \u001b[2m                                 \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;95mon-input-\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;95m-70c0cdcc1dbf\u001b[0m\u001b[1m>\u001b[0m:\u001b[1;36m9\u001b[0m    \u001b[2m                                 \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1m[\u001b[0mTesting\u001b[1m]\u001b[0m Start \u001b[33m...\u001b[0m            \u001b[2m                                 \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> root<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1376</span><span style=\"font-weight: bold\">]</span>                     <a href=\"file://<ipython-input-20-70c0cdcc1dbf>\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-20-70c0cdcc1dbf&gt;</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://<ipython-input-20-70c0cdcc1dbf>#9\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">9</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MainProcess</span><span style=\"font-weight: bold\">(</span>MainThread<span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">ipyth</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                 </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">on-input-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">-70c0cdcc1dbf</span><span style=\"font-weight: bold\">&gt;</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                 </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"font-weight: bold\">[</span>Testing<span style=\"font-weight: bold\">]</span> Start <span style=\"color: #808000; text-decoration-color: #808000\">...</span>            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                 </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m tensorflow\u001b[1m[\u001b[0m\u001b[1;36m1376\u001b[0m\u001b[1m]\u001b[0m \u001b[1;35mMainProcess\u001b[0m\u001b[1m(\u001b[0mMainThread\u001b[1m)\u001b[0m    \u001b]8;id=755347;file:///usr/local/lib/python3.7/dist-packages/keras/layers/recurrent_v2.py\u001b\\\u001b[2mrecurrent_v2.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=708322;file:///usr/local/lib/python3.7/dist-packages/keras/layers/recurrent_v2.py#1130\u001b\\\u001b[2m1130\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         recurrent_v\u001b[1;92m2:1130\u001b[0m  Layer lstm_1 will not    \u001b[2m                    \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         use cuDNN kernels since it doesn't meet the \u001b[2m                    \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         criteria. It will use a generic GPU kernel  \u001b[2m                    \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         as fallback when running on GPU.            \u001b[2m                    \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> tensorflow<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1376</span><span style=\"font-weight: bold\">]</span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MainProcess</span><span style=\"font-weight: bold\">(</span>MainThread<span style=\"font-weight: bold\">)</span>    <a href=\"file:///usr/local/lib/python3.7/dist-packages/keras/layers/recurrent_v2.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">recurrent_v2.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.7/dist-packages/keras/layers/recurrent_v2.py#1130\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1130</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         recurrent_v<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">2:1130</span>  Layer lstm_1 will not    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         use cuDNN kernels since it doesn't meet the <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         criteria. It will use a generic GPU kernel  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         as fallback when running on GPU.            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m tensorflow\u001b[1m[\u001b[0m\u001b[1;36m1376\u001b[0m\u001b[1m]\u001b[0m \u001b[1;35mMainProcess\u001b[0m\u001b[1m(\u001b[0mMainThread\u001b[1m)\u001b[0m    \u001b]8;id=831759;file:///usr/local/lib/python3.7/dist-packages/keras/layers/recurrent_v2.py\u001b\\\u001b[2mrecurrent_v2.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=722092;file:///usr/local/lib/python3.7/dist-packages/keras/layers/recurrent_v2.py#1130\u001b\\\u001b[2m1130\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         recurrent_v\u001b[1;92m2:1130\u001b[0m  Layer lstm_1 will not    \u001b[2m                    \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         use cuDNN kernels since it doesn't meet the \u001b[2m                    \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         criteria. It will use a generic GPU kernel  \u001b[2m                    \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         as fallback when running on GPU.            \u001b[2m                    \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> tensorflow<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1376</span><span style=\"font-weight: bold\">]</span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MainProcess</span><span style=\"font-weight: bold\">(</span>MainThread<span style=\"font-weight: bold\">)</span>    <a href=\"file:///usr/local/lib/python3.7/dist-packages/keras/layers/recurrent_v2.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">recurrent_v2.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.7/dist-packages/keras/layers/recurrent_v2.py#1130\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1130</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         recurrent_v<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">2:1130</span>  Layer lstm_1 will not    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         use cuDNN kernels since it doesn't meet the <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         criteria. It will use a generic GPU kernel  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         as fallback when running on GPU.            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m tensorflow\u001b[1m[\u001b[0m\u001b[1;36m1376\u001b[0m\u001b[1m]\u001b[0m \u001b[1;35mMainProcess\u001b[0m\u001b[1m(\u001b[0mMainThread\u001b[1m)\u001b[0m    \u001b]8;id=894347;file:///usr/local/lib/python3.7/dist-packages/keras/layers/recurrent_v2.py\u001b\\\u001b[2mrecurrent_v2.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=274406;file:///usr/local/lib/python3.7/dist-packages/keras/layers/recurrent_v2.py#1130\u001b\\\u001b[2m1130\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         recurrent_v\u001b[1;92m2:1130\u001b[0m  Layer lstm_1 will not    \u001b[2m                    \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         use cuDNN kernels since it doesn't meet the \u001b[2m                    \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         criteria. It will use a generic GPU kernel  \u001b[2m                    \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         as fallback when running on GPU.            \u001b[2m                    \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> tensorflow<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1376</span><span style=\"font-weight: bold\">]</span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MainProcess</span><span style=\"font-weight: bold\">(</span>MainThread<span style=\"font-weight: bold\">)</span>    <a href=\"file:///usr/local/lib/python3.7/dist-packages/keras/layers/recurrent_v2.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">recurrent_v2.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.7/dist-packages/keras/layers/recurrent_v2.py#1130\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1130</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         recurrent_v<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">2:1130</span>  Layer lstm_1 will not    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         use cuDNN kernels since it doesn't meet the <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         criteria. It will use a generic GPU kernel  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         as fallback when running on GPU.            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[04/03/22 16:41:13]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m root\u001b[1m[\u001b[0m\u001b[1;36m1376\u001b[0m\u001b[1m]\u001b[0m                   \u001b]8;id=54076;file://<ipython-input-17-3212bc838956>\u001b\\\u001b[2m<ipython-input-17-3212bc838956>\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=159586;file://<ipython-input-17-3212bc838956>#183\u001b\\\u001b[2m183\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;35mMainProcess\u001b[0m\u001b[1m(\u001b[0mMainThread\u001b[1m)\u001b[0m \u001b[1m<\u001b[0m\u001b[1;95mipy\u001b[0m \u001b[2m                                   \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;95mthon-input-\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;95m-3212bc838956\u001b[0m\u001b[1m>\u001b[0m: \u001b[2m                                   \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;36m183\u001b[0m  Max f1: \u001b[1;36m0.4000\u001b[0m, at      \u001b[2m                                   \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         thres = \u001b[1;36m0.5620\u001b[0m               \u001b[2m                                   \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/03/22 16:41:13] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> root<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1376</span><span style=\"font-weight: bold\">]</span>                   <a href=\"file://<ipython-input-17-3212bc838956>\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-17-3212bc838956&gt;</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://<ipython-input-17-3212bc838956>#183\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">183</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MainProcess</span><span style=\"font-weight: bold\">(</span>MainThread<span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">ipy</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                   </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">thon-input-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">-3212bc838956</span><span style=\"font-weight: bold\">&gt;</span>: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                   </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">183</span>  Max f1: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4000</span>, at      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                   </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         thres = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5620</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                   </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[296  22]\n",
            " [  5   9]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m root\u001b[1m[\u001b[0m\u001b[1;36m1376\u001b[0m\u001b[1m]\u001b[0m                    \u001b]8;id=675681;file://<ipython-input-20-70c0cdcc1dbf>\u001b\\\u001b[2m<ipython-input-20-70c0cdcc1dbf>\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=967456;file://<ipython-input-20-70c0cdcc1dbf>#11\u001b\\\u001b[2m11\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;35mMainProcess\u001b[0m\u001b[1m(\u001b[0mMainThread\u001b[1m)\u001b[0m \u001b[1m<\u001b[0m\u001b[1;95mipyt\u001b[0m \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[1;95mhon-input-\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;95m-70c0cdcc1dbf\u001b[0m\u001b[1m>\u001b[0m:\u001b[1;36m11\u001b[0m \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m           \u001b[1m[\u001b[0mTesting\u001b[1m]\u001b[0m done.             \u001b[2m                                  \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> root<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1376</span><span style=\"font-weight: bold\">]</span>                    <a href=\"file://<ipython-input-20-70c0cdcc1dbf>\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-20-70c0cdcc1dbf&gt;</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://<ipython-input-20-70c0cdcc1dbf>#11\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MainProcess</span><span style=\"font-weight: bold\">(</span>MainThread<span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">ipyt</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">hon-input-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">-70c0cdcc1dbf</span><span style=\"font-weight: bold\">&gt;</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>           <span style=\"font-weight: bold\">[</span>Testing<span style=\"font-weight: bold\">]</span> done.             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nuse gpu:\\nhttps://www.tensorflow.org/guide/gpu\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4haYpT2RpCuN",
        "outputId": "5a50795c-4438-4d49-e25d-619faf14276f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    }
  ]
}