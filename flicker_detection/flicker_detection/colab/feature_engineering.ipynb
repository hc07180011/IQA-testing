{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feature_engineering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1WN8QD5vJ7iOSo7KS9X1knOkijwENB19C",
      "authorship_tag": "ABX9TyNjoZxDIEULWk+7vgQ51rTs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hc07180011/testing-cv/blob/AdaptivePooling/feature_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UPaK4EYi8mc",
        "outputId": "73c82351-8144-4488-e223-b66d04cab230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For loading .ipynb files to colab"
      ],
      "metadata": {
        "id": "kRRJWp-LzZ9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp ROC-Keras.ipynb /content/drive/My\\ Drive/"
      ],
      "metadata": {
        "id": "Wa_45yNpjSsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -qq xattr -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQPbhHRKjyMB",
        "outputId": "ddd9fe88-21ea-4d67-beb9-3eeb4efc89cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package python3-cffi-backend.\n",
            "(Reading database ... 156210 files and directories currently installed.)\n",
            "Preparing to unpack .../0-python3-cffi-backend_1.11.5-1_amd64.deb ...\n",
            "Unpacking python3-cffi-backend (1.11.5-1) ...\n",
            "Selecting previously unselected package python3-ply.\n",
            "Preparing to unpack .../1-python3-ply_3.11-1_all.deb ...\n",
            "Unpacking python3-ply (3.11-1) ...\n",
            "Selecting previously unselected package python3-pycparser.\n",
            "Preparing to unpack .../2-python3-pycparser_2.18-2_all.deb ...\n",
            "Unpacking python3-pycparser (2.18-2) ...\n",
            "Selecting previously unselected package python3-cffi.\n",
            "Preparing to unpack .../3-python3-cffi_1.11.5-1_all.deb ...\n",
            "Unpacking python3-cffi (1.11.5-1) ...\n",
            "Selecting previously unselected package python3-pkg-resources.\n",
            "Preparing to unpack .../4-python3-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python3-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python3-xattr.\n",
            "Preparing to unpack .../5-python3-xattr_0.9.2-0ubuntu1_amd64.deb ...\n",
            "Unpacking python3-xattr (0.9.2-0ubuntu1) ...\n",
            "Selecting previously unselected package xattr.\n",
            "Preparing to unpack .../6-xattr_0.9.2-0ubuntu1_amd64.deb ...\n",
            "Unpacking xattr (0.9.2-0ubuntu1) ...\n",
            "Setting up python3-cffi-backend (1.11.5-1) ...\n",
            "Setting up python3-pkg-resources (39.0.1-2) ...\n",
            "Setting up python3-ply (3.11-1) ...\n",
            "Setting up python3-pycparser (2.18-2) ...\n",
            "Setting up python3-cffi (1.11.5-1) ...\n",
            "Setting up python3-xattr (0.9.2-0ubuntu1) ...\n",
            "Setting up xattr (0.9.2-0ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!xattr -p 'user.drive.id' /content/drive/My\\ Drive/ROC-Keras.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0rksfswjgGe",
        "outputId": "39714128-2aff-4eb1-da60-da359267aee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1v0OZIC6gR0xg55GmI1vdmjtBx8QF0lqx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Asymmetric chunks padding"
      ],
      "metadata": {
        "id": "9s4or1n6zgW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "chunks = 30\n",
        "x = np.zeros((680,1,6,6,256))\n",
        "\n",
        "def _get_chunk_array(input_arr: np.array, chunk_size: int) -> list:\n",
        "    i_pad = np.pad(input_arr,(0,chunk_size-len(input_arr)%chunk_size),'constant')\n",
        "    asymmetric_chunks = np.split(\n",
        "        i_pad,\n",
        "        list(range(\n",
        "            chunk_size,\n",
        "            input_arr.shape[0] + 1,\n",
        "            chunk_size\n",
        "        ))\n",
        "    )\n",
        "    print(i_pad.shape)\n",
        "    print(len(i_pad)/chunk_size)\n",
        "    # TODO: should we take the last chunk?\n",
        "    return np.array(asymmetric_chunks).tolist()\n",
        "\n",
        "batch = _get_chunk_array(x,chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgSV9AZXxuWv",
        "outputId": "bc3dd588-8681-4378-9c36-af85b094df57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(690, 11, 16, 16, 266)\n",
            "23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LabelBox test"
      ],
      "metadata": {
        "id": "eZYXqohONFrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade tensorflow-hub \\\n",
        "                 scikit-learn \\\n",
        "                 seaborn \\\n",
        "                 \"labelbox[data]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcKzx81D_XEb",
        "outputId": "bae41b37-62c5-41b9-817c-7d9098ff731d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 162 kB 14.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 43.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 45.9 MB/s \n",
            "\u001b[?25h  Building wheel for pygeotile (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import labelbox\n",
        "import random\n",
        "import numpy as np\n",
        "from labelbox import Client\n",
        "from labelbox.schema.data_row_metadata import (\n",
        "    DataRowMetadata,\n",
        "    DataRowMetadataField,\n",
        "    DeleteDataRowMetadata,\n",
        ")\n",
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import tensorflow_hub as hub\n",
        "from datetime import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "import requests\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "hPlOd4l8_W2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your api key\n",
        "API_KEY = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VySWQiOiJja2Rma214eGgwN285MDc1NWMxZ2RoenV0Iiwib3JnYW5pemF0aW9uSWQiOiJja2Rma214dXIwODEwMDc2MHNyeGs4Z3BuIiwiYXBpS2V5SWQiOiJjbDE4MnkyZjA2bmgzMHo5djQyaHJmbG9nIiwic2VjcmV0IjoiMjhmZmM5YTllZjkzMzBjZjY4NTIzZWI3NjVlODM0NDQiLCJpYXQiOjE2NDgzMTI5MDYsImV4cCI6MjI3OTQ2NDkwNn0.WJSiHRdxcN_lVYm6nlDU9bA-mgFolBXRhKIR4hv1Rbk'\n",
        "client = Client(api_key=API_KEY)"
      ],
      "metadata": {
        "id": "liA8ZUPDNSmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import labelbox\n",
        "\n",
        "# Enter your Labelbox API key here\n",
        "LB_API_KEY = API_KEY\n",
        "\n",
        "# Create Labelbox client\n",
        "lb = labelbox.Client(api_key=LB_API_KEY)\n",
        "\n",
        "# Create a new dataset\n",
        "dataset = lb.create_dataset(name=\"embeddings\")\n",
        "\n",
        "# Create data payload\n",
        "# External ID is recommended to identify your data_row via unique reference throughout Labelbox workflow.\n",
        "my_data_rows = [\n",
        "  {\n",
        "    \"row_data\": \"https://picsum.photos/200/300\",\n",
        "    \"external_id\": \"uid_01\"},\n",
        "  {\n",
        "    \"row_data\": \"https://picsum.photos/200/400\",\n",
        "    \"external_id\": \"uid_02\"\n",
        "  }\n",
        "]\n",
        "\n",
        "# Bulk add data rows to the dataset\n",
        "task = dataset.create_data_rows(my_data_rows)\n",
        "\n",
        "task.wait_till_done()\n",
        "print(task.status)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcKiAhekNqro",
        "outputId": "0b21eb7a-4b9a-43d9-f957-27c5a35a97ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMPLETE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TFRecord encoding"
      ],
      "metadata": {
        "id": "2d1lP7on1aoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "import imageio\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# TFRecord helpers \n",
        "def _int64_feature(value):\n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
        "def _bytes_feature(value):\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def mp4_2_numpy(filename):\n",
        "    \"\"\"Reads a video and returns its contents in matrix form.\n",
        "    Args:\n",
        "        filename (str): a path to a video\n",
        "    Returns:\n",
        "        np.array(): matrix contents of the video \n",
        "    \"\"\"\n",
        "    vid = imageio.get_reader(filename, 'ffmpeg')\n",
        "    # read all of video frames resulting in a (T, H, W, C) matrix \n",
        "    data = np.stack(list(vid.iter_data()))\n",
        "    return data \n",
        "\n",
        "def line2example(line):\n",
        "    \"\"\"Reads a line from the datafile and returns an\n",
        "    associated TFRecords example containing the encoded data. \n",
        "    Args:\n",
        "        line (str): a line from the datafile \n",
        "            (formatted as {filepath} {label})\n",
        "    Returns:\n",
        "        tf.train.SequenceExample: resulting TFRecords example \n",
        "    \"\"\"\n",
        "    # extract information on dataexample \n",
        "    fn, label = line.split(' ')\n",
        "    label = int(label)\n",
        "\n",
        "    # read matrix data and save its shape \n",
        "    video_data = mp4file_2_numpy(fn)\n",
        "    t, h, w, c = video_data.shape\n",
        "\n",
        "    # save video as list of encoded frames using tensorflow's operation \n",
        "    img_bytes = [tf.image.encode_jpeg(frame, format='rgb') for frame in video_data]\n",
        "    with tf.Session() as sess: \n",
        "        img_bytes = sess.run(img_bytes)\n",
        "        \n",
        "    sequence_dict = {}\n",
        "    # create a feature for each encoded frame\n",
        "    img_feats = [tf.train.Feature(bytes_list=tf.train.BytesList(value=[imgb])) for imgb in img_bytes]\n",
        "    # save video frames as a FeatureList\n",
        "    sequence_dict['video_frames'] = tf.train.FeatureList(feature=img_feats)\n",
        "\n",
        "    # also store associated metadata\n",
        "    context_dict = {}\n",
        "    context_dict['filename'] = _bytes_feature(fn.encode('utf-8'))\n",
        "    context_dict['label'] = _int64_feature(label)\n",
        "    context_dict['temporal'] = _int64_feature(t)\n",
        "    context_dict['height'] = _int64_feature(h)\n",
        "    context_dict['width'] = _int64_feature(w)\n",
        "    context_dict['depth'] = _int64_feature(c)\n",
        "\n",
        "    # combine list + context to create TFRecords example \n",
        "    sequence_context = tf.train.Features(feature=context_dict)\n",
        "    sequence_list = tf.train.FeatureLists(feature_list=sequence_dict)\n",
        "    example = tf.train.SequenceExample(context=sequence_context, feature_lists=sequence_list)\n",
        "\n",
        "    return example\n",
        "\n",
        "def create_tfrecords(datafile_path, save_path):\n",
        "    \"\"\"Creates a TFRecords dataset from video files.\n",
        "    Args:\n",
        "        datafile_path (str): a path to the formatted datafiles (includes train.txt, etc.)\n",
        "        save_path (str): where to save the .tfrecord files \n",
        "    \"\"\"\n",
        "    save_path = pathlib.Path(save_path)\n",
        "    save_path.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    # create a TFRecord for each datasplit \n",
        "    for dset_name in ['train.txt', 'test.txt', 'val.txt']:\n",
        "        # read the lines of the datafile\n",
        "        with open(datafile_path + dset_name, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        # write each example to a {split}.tfrecord (train.tfrecord, etc.) \n",
        "        record_file = str(save_path/'{}.tfrecord'.format(dset_name[:-4]))\n",
        "        with tf.python_io.TFRecordWriter(record_file) as writer: \n",
        "            for line in tqdm(lines): \n",
        "                example = line2example(line)\n",
        "                writer.write(example.SerializeToString())"
      ],
      "metadata": {
        "id": "OBmuxc0zNqg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE_H,IMAGE_SIZE_W = 200,200\n",
        "\n",
        "# define the features to decode \n",
        "sequence_features = {\n",
        "  'video_frames': tf.io.FixedLenSequenceFeature([], dtype=tf.string)\n",
        "}\n",
        "\n",
        "context_features = {\n",
        "  'filename': tf.io.FixedLenFeature([], tf.string),\n",
        "  'height': tf.io.FixedLenFeature([], tf.int64),\n",
        "  'width': tf.io.FixedLenFeature([], tf.int64),\n",
        "  'depth': tf.io.FixedLenFeature([], tf.int64),\n",
        "  'temporal': tf.io.FixedLenFeature([], tf.int64),\n",
        "  'label': tf.io.FixedLenFeature([], tf.int64),\n",
        "}\n",
        "\n",
        "@tf.function\n",
        "def resize(img):\n",
        "  return tf.image.resize(img, [IMAGE_SIZE_H, IMAGE_SIZE_W])\n",
        "\n",
        "def _parse_example(example_proto):\n",
        "    # Parse the input tf.train.Example using the dictionary above.\n",
        "    context, sequence = tf.parse_single_sequence_example(example_proto,\n",
        "                                                        context_features=context_features, \n",
        "                                                        sequence_features=sequence_features)\n",
        "    # extract the expected shape \n",
        "    shape = (context['temporal'], context['height'], context['width'], context['depth'])\n",
        "\n",
        "    ## the golden while loop ## \n",
        "    # loop through the feature lists and decode each image seperately \n",
        "\n",
        "    # decoding the first video \n",
        "    video_data = tf.image.decode_image(tf.gather(sequence['video_frames'], [0])[0])\n",
        "    video_data = tf.expand_dims(video_data, 0)\n",
        "\n",
        "    i = tf.constant(1, dtype=tf.int32)\n",
        "    # condition of when to stop / loop through every frame\n",
        "    cond = lambda i, _: tf.less(i, tf.cast(context['temporal'], tf.int32))\n",
        "\n",
        "    # reading + decoding the i-th image frame \n",
        "    def body(i, video_data):\n",
        "        # get the i-th index \n",
        "        encoded_img = tf.gather(sequence['video_frames'], [i])\n",
        "        # decode the image \n",
        "        img_data = tf.image.decode_image(encoded_img[0]) \n",
        "        # append to list using tf operations \n",
        "        video_data = tf.concat([video_data, [img_data]], 0)\n",
        "        # update counter & new video_data \n",
        "        return (tf.add(i, 1), video_data)\n",
        "\n",
        "    # run the loop (use shape∈variants since video_data changes size)\n",
        "    _, video_data = tf.while_loop(cond, body, [i, video_data], \n",
        "            shape_invariants=[i.get_shape(), tf.TensorShape([None])])\n",
        "    # use this to set the shape + dtype\n",
        "    video_data = tf.reshape(video_data, shape)\n",
        "    video_data = tf.cast(video_data, tf.float32)\n",
        "\n",
        "    # resize each frame in video -- can apply different augmentations etc. like this \n",
        "    video_data = tf.map_fn(resize, video_data, back_prop=False, parallel_iterations=10)\n",
        "\n",
        "    label = context['label']\n",
        "    # return the data example and its corresponding label \n",
        "    return video_data, label\n",
        "\n",
        "# create the dataset \n",
        "dataset = tf.data.TFRecordDataset('train.tfrecord')\\\n",
        "        .map(sequence_features)\\\n",
        "        .batch(2)\n",
        "iterator = dataset.make_one_shot_iterator()\n",
        "next_element = iterator.get_next()\n",
        "\n",
        "# use standard tf training setup \n",
        "with tf.Session() as sess: \n",
        "  batch_vid, batch_label = sess.run(next_element)\n",
        "  print(batch_vid.shape, batch_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "NqsKsmm01nIY",
        "outputId": "609065ce-f920-4451-cf9f-ad4c2c6fe2dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ff2d606610c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# create the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.tfrecord'\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_features\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_one_shot_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mnext_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2014\u001b[0m         warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[1;32m   2015\u001b[0m                       \"`num_parallel_calls` argument is specified.\")\n\u001b[0;32m-> 2016\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2017\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2018\u001b[0m       return ParallelMapDataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m   5193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5194\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5195\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   5196\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_metadata_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     func_name = \"_\".join(\n\u001b[1;32m    162\u001b[0m         [readable_transformation_name,\n\u001b[0;32m--> 163\u001b[0;31m          function_utils.get_func_name(func)])\n\u001b[0m\u001b[1;32m    164\u001b[0m     \u001b[0;31m# Sanitize function name to remove symbols that interfere with graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;31m# construction.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/function_utils.py\u001b[0m in \u001b[0;36mget_func_name\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     raise ValueError(\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;34m'Argument `func` must be a callable. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         f'Received func={func} (of type {type(func)})')\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Argument `func` must be a callable. Received func={'video_frames': FixedLenSequenceFeature(shape=[], dtype=tf.string, allow_missing=False, default_value=None)} (of type <class 'dict'>)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import psutil\n",
        "import imageio \n",
        "\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf \n",
        "import matplotlib.pyplot as plt\n",
        "import multiprocessing as mp\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "class TFRecords4Video():\n",
        "    def __init__(self, tfrecords_save_path, datafile_path, datafile_prefix,\\\n",
        "        fn2video):\n",
        "        \"\"\"Writes video TFRecords for a given dataset.\n",
        "        Args:\n",
        "            tfrecords_save_path (str): Path to save TFRecords to.\n",
        "            datafile_path (str): Path to find train.txt, val.txt and test.txt\n",
        "                {train, test, val}.txt file lines are formatted as: \n",
        "                    file label\n",
        "            datafile_prefix (str): Prefix path for files in train.txt. Paths  \n",
        "            will be given to fn2video function as 'datafile_prefix/file'\n",
        "            fn2video (str or function): function which takes path and \n",
        "            returns the video matrix with size (T, H, W, C). \n",
        "            \n",
        "            Already implemented use cases can use a string: \n",
        "            - 'video' for paths which point to a video file (calls vid2numpy)\n",
        "            - 'images' for paths which point to a folder of images \n",
        "            (calls images2numpy)\n",
        "        \"\"\"\n",
        "        self.tfrecords_save_path = pathlib.Path(tfrecords_save_path)\n",
        "        self.datafile_path = pathlib.Path(datafile_path)\n",
        "        self.datafile_prefix = pathlib.Path(datafile_prefix)\n",
        "\n",
        "        # create folders for TFRecord shards \n",
        "        self.tfrecords_save_path.mkdir(parents=True, exist_ok=True)\n",
        "        for split in ['train', 'val', 'test']:\n",
        "            (self.tfrecords_save_path/split).mkdir(exist_ok=True)\n",
        "\n",
        "        # function for fn -> video data (T, H, W, C)\n",
        "        if fn2video == 'video':\n",
        "            self.fn2video = vid2numpy\n",
        "        elif fn2video == 'images':\n",
        "            self.fn2video = images2numpy\n",
        "        else: \n",
        "            # allow custom parsing of video matrix \n",
        "            self.fn2video = fn2video\n",
        "        \n",
        "    def extract_pathlabels(self, split):\n",
        "        \"\"\"Extracts absolute paths and labels from datafiles \n",
        "        ({train, val, test}.txt) using \n",
        "        self.datafile_path and self.datafile_prefix\n",
        "        Args:\n",
        "            split (str): split to get paths from \n",
        "            must be a value in {'train', 'test', 'val'}\n",
        "        Returns:\n",
        "            tuple(list[pathlib.Path], list[int]): paths and labels from split's\n",
        "            datafile\n",
        "        \"\"\"\n",
        "        assert split in ['train', 'val', 'test'], \"Invalid Split\"\n",
        "\n",
        "        splitfile_path = self.datafile_path/'{}.txt'.format(split)\n",
        "        assert splitfile_path.exists(), \"{} should exist.\".format(splitfile_path)\n",
        "\n",
        "        with open(splitfile_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        skip_counter = 0 \n",
        "        example_paths, example_labels = [], []\n",
        "        for line in tqdm(lines): \n",
        "            fn, label = line.split(' ')\n",
        "            fn, label = self.datafile_prefix/fn, int(label)\n",
        "            if pathlib.Path(fn).exists(): \n",
        "                example_paths.append(fn)\n",
        "                example_labels.append(label)\n",
        "            else: \n",
        "                skip_counter += 1 \n",
        "        print('\\nNumber of files not found: {} / {}'.format(skip_counter, len(lines)))\n",
        "        \n",
        "        if skip_counter > 0: \n",
        "            print('Warning: Some frames were not found, here is an example path \\\n",
        "                to debug: {}'.format(fn))\n",
        "\n",
        "        return example_paths, example_labels\n",
        "\n",
        "    def get_example(self, filename, label):\n",
        "        \"\"\"Returns a TFRecords example for the given video located at filename \n",
        "        with the label label.\n",
        "        Args:\n",
        "            filename (pathlib.Path): path to create example from\n",
        "            label (int): class label for video \n",
        "        Returns:\n",
        "            tf.train.SequenceExample: encoded tfrecord example \n",
        "        \"\"\"\n",
        "        # read matrix data and save its shape \n",
        "        data = self.fn2video(filename)\n",
        "        t, h, w, c = data.shape\n",
        "        \n",
        "        # save video as list of encoded frames using tensorflow's operation \n",
        "        img_bytes = [tf.image.encode_jpeg(frame, format='rgb') for frame in data]\n",
        "        with tf.Session() as sess: \n",
        "            img_bytes = sess.run(img_bytes)\n",
        "            \n",
        "        sequence_dict = {}\n",
        "        # create a feature for each encoded frame\n",
        "        img_feats = [tf.train.Feature(bytes_list=\\\n",
        "            tf.train.BytesList(value=[imgb])) for imgb in img_bytes]\n",
        "        # save video frames as a FeatureList\n",
        "        sequence_dict['video_frames'] = tf.train.FeatureList(feature=img_feats)\n",
        "\n",
        "        # also store associated meta-data\n",
        "        context_dict = {}\n",
        "        context_dict['filename'] = _bytes_feature(str(filename).encode('utf-8'))\n",
        "        context_dict['label'] = _int64_feature(label)\n",
        "        context_dict['temporal'] = _int64_feature(t)\n",
        "        context_dict['height'] = _int64_feature(h)\n",
        "        context_dict['width'] = _int64_feature(w)\n",
        "        context_dict['depth'] = _int64_feature(c)\n",
        "\n",
        "        # combine list + context to create TFRecords example \n",
        "        sequence_context = tf.train.Features(feature=context_dict)\n",
        "        sequence_list = tf.train.FeatureLists(feature_list=sequence_dict)\n",
        "        example = tf.train.SequenceExample(context=sequence_context, \\\n",
        "            feature_lists=sequence_list)\n",
        "\n",
        "        return example\n",
        "\n",
        "    def pathlabels2records(self, paths, labels, split, max_bytes=1e9):\n",
        "        \"\"\"Creates TFRecord files in shards from the given path and labels\n",
        "        Args:\n",
        "            paths (list[pathlib.Path]): paths of videos to write to TFRecords\n",
        "            labels (list[int]): labels associated videos\n",
        "            split (str): datasplit to write to, one of: ('train', 'test', 'val')\n",
        "            max_bytes (int, optional): approx max size of each shard in bytes.\n",
        "            Defaults to 1e9.\n",
        "        \"\"\"\n",
        "        assert split in ['train', 'val', 'test'], \"Invalid Split\"\n",
        "\n",
        "        n_examples = len(paths)\n",
        "        print('Splitting {} examples into {:.2f} GB shards'.format(\\\n",
        "            n_examples, max_bytes / 1e9))\n",
        "\n",
        "        # number of shutdowns + restarts to maintain ~1sec/iteration of encoding\n",
        "        # if factor = 1 it can go up to ~11sec/iteration (really slow)\n",
        "        # larger value = faster single processes but more shutdown/startup time\n",
        "        # smaller value = slower single process but less shutdown/startup time\n",
        "        factor = 90\n",
        "        n_processes = psutil.cpu_count() \n",
        "        print('Using {} processes...'.format(n_processes))\n",
        "        \n",
        "        paths_split = np.array_split(paths, factor)\n",
        "        labels_split = np.array_split(labels, factor)\n",
        "\n",
        "        process_id = 0 \n",
        "        pbar = tqdm(total=factor)\n",
        "        for (m_paths, m_labels) in zip(paths_split, labels_split):\n",
        "            # split data into equal sized chunks for each process\n",
        "            paths_further_split = np.array_split(m_paths, n_processes)\n",
        "            labels_further_split = np.array_split(m_labels, n_processes)\n",
        "\n",
        "            # multiprocess the writing \n",
        "            pool = mp.Pool(n_processes)\n",
        "            returns = []\n",
        "            for paths, labels in zip(paths_further_split, labels_further_split):\n",
        "                r = pool.apply_async(process_write, args=(paths, labels, split, \\\n",
        "                        max_bytes, process_id, self))\n",
        "                returns.append(r)\n",
        "                process_id += 1 \n",
        "            pool.close()\n",
        "            # use this to view errors in children (if any)\n",
        "            for r in returns: r.get()\n",
        "            pool.join()\n",
        "            pbar.update(1)\n",
        "        pbar.close()\n",
        "\n",
        "    def split2records(self, split, max_bytes=1e9):\n",
        "        \"\"\"Creates TFRecords for a given data split\n",
        "        Args:\n",
        "            split (str): split to create for, in ['train', 'test', 'val']\n",
        "            max_bytes (int, optional): approx max size of each shard in bytes.\n",
        "            Defaults to 1e9.\n",
        "        \"\"\"\n",
        "        print('Starting processing split {}.'.format(split))\n",
        "\n",
        "        print('Extracting paths and labels...')\n",
        "        paths, labels = self.extract_pathlabels(split)\n",
        "        \n",
        "        print('Writing to TFRecords...')\n",
        "        self.pathlabels2records(paths, labels, split, max_bytes)\n",
        "        \n",
        "        print('Finished processing split {}.'.format(split))\n",
        "\n",
        "    def create_tfrecords(self, max_bytes=1e9):\n",
        "        \"\"\"Creates TFRecords for all splits ('train', 'test', 'val')\n",
        "        Args:\n",
        "            max_bytes (int, optional): approx max size of each shard in bytes.\n",
        "            Defaults to 1e9.\n",
        "        \"\"\"\n",
        "        for split in ['train', 'test', 'val']:\n",
        "            self.split2records(split, max_bytes)\n",
        "\n",
        "# multiprocessing function \n",
        "def process_write(paths, labels, split, max_bytes, process_id, tf4v):\n",
        "    \"\"\"Writes a list of video examples as a TFRecord. \n",
        "    Args:\n",
        "        paths (list[pathlib.Path]): paths to videos\n",
        "        labels (list[int]): associative labels for the videos \n",
        "        split (str): one of ['train', 'test', 'val']\n",
        "        max_bytes (int): Number of bytes per shard \n",
        "        process_id (int): id of processes \n",
        "        tf4v (TFRecords4Video): video processing class \n",
        "    Returns:\n",
        "        int: 1 for success\n",
        "    \"\"\"\n",
        "    shard_count, i = 0, 0\n",
        "    n_examples = len(paths)\n",
        "    while i != n_examples:\n",
        "        # tf record file to write to \n",
        "        tf_record_name = ('{}/{}-shard{}.tfrecord').format(split, \\\n",
        "            process_id, shard_count)\n",
        "\n",
        "        record_file = tf4v.tfrecords_save_path/tf_record_name\n",
        "        with tf.python_io.TFRecordWriter(str(record_file)) as writer: \n",
        "            # split into approx. equal sized shards \n",
        "            while record_file.stat().st_size < max_bytes and i != n_examples: \n",
        "                # write each example to tfrecord \n",
        "                example_i = tf4v.get_example(paths[i], labels[i])\n",
        "                writer.write(example_i.SerializeToString())\n",
        "                # process next example \n",
        "                i += 1 \n",
        "\n",
        "        # process a new shard \n",
        "        shard_count += 1\n",
        "    return 1 \n",
        "\n",
        "# TFRecords helpers \n",
        "def _int64_feature(value):\n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
        "def _bytes_feature(value):\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "# file -> video data functions \n",
        "def vid2numpy(filename):\n",
        "    \"\"\"Reads a video and returns its contents in matrix form.\n",
        "    Args:\n",
        "        filename (pathlib.Path): a path to a video\n",
        "    Returns:\n",
        "        np.array(): matrix contents of the video \n",
        "    \"\"\"\n",
        "    vid = imageio.get_reader(str(filename), 'ffmpeg')\n",
        "    # read all of video frames resulting in a (T, H, W, C) matrix \n",
        "    data = np.stack(list(vid.iter_data()))\n",
        "    return data \n",
        "\n",
        "def images2numpy(filename):\n",
        "    \"\"\"Reads a fold of images and returns its contents in matrix form.\n",
        "    Args:\n",
        "        filename (pathlib.Path): a path to a folder of frames\n",
        "        which make up a video. \n",
        "    Returns:\n",
        "        np.array(): matrix contents of the video \n",
        "    \"\"\"\n",
        "    data = np.stack([plt.imread(frame_path) \\\n",
        "        for frame_path in filename.iterdir()])\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "RnG6ZZTL4KNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = TFRecords4Video(\n",
        "    \"/content/drive/MyDrive/google_cv/sample_augmentations\",\n",
        "    \"/content/drive/MyDrive/google_cv/flicker-detection\",\n",
        "    \"_encoded\",\n",
        "    \"video\")"
      ],
      "metadata": {
        "id": "SvizUSne4hKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1MF_yBc5l45",
        "outputId": "677026c3-b4c3-425f-d464-e384ca7a883a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Path.mkdir of PosixPath('/content/drive/MyDrive/google_cv/sample_augmentations')>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}